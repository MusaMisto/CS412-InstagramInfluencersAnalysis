{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instagram Influencers Analysis\n",
    "# Authors: Musa Misto & Neda Mohamed\n",
    "\n",
    "# Purpose:\n",
    "# This notebook serves as the main workspace for the CS412 course project on Instagram influencer analysis.\n",
    "# The project aims to develop machine learning models for two primary tasks:\n",
    "# 1. Multi-class classification to predict influencer categories based on profile meta-data and recent posts.\n",
    "# 2. Regression analysis to estimate content popularity (e.g., like_count) using relevant features.\n",
    "# The goal is to explore the dataset, preprocess the data, build and evaluate models, and document findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\itsmm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1) Download stopwords if they aren't already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 2) Load Turkish stopwords\n",
    "turkish_stopwords = stopwords.words('turkish')\n",
    "\n",
    "# 3) Dynamically locate training data paths (no prints)\n",
    "current_notebook_dir = os.getcwd()  # e.g. \"repo/notebooks\"\n",
    "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))  # \"repo\"\n",
    "data_dir = os.path.join(repo_dir, 'data')                             # \"repo/data\"\n",
    "training_dir = os.path.join(data_dir, 'training')                     # \"repo/data/training\"\n",
    "\n",
    "train_csv_path = os.path.join(training_dir, 'train-classification.csv')\n",
    "train_jsonl_path = os.path.join(training_dir, 'training-dataset.jsonl.gz')\n",
    "\n",
    "# 4) Read the CSV into a DataFrame\n",
    "df_classification = pd.read_csv(train_csv_path)\n",
    "\n",
    "# 5) Read the JSONL.GZ lines into Python objects\n",
    "records = []\n",
    "with gzip.open(train_jsonl_path, 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        records.append(record)\n",
    "\n",
    "# 6) Optionally, create two DataFrames if you want to split profile/post data\n",
    "df_profiles_list = []\n",
    "df_posts_list = []\n",
    "\n",
    "for rec in records:\n",
    "    profile = rec.get('profile', {})\n",
    "    posts = rec.get('posts', [])\n",
    "    \n",
    "    df_profiles_list.append(profile)\n",
    "    \n",
    "    # For each post, attach the username to keep track\n",
    "    for p in posts:\n",
    "        post_entry = p.copy()  # shallow copy\n",
    "        post_entry['username'] = profile.get('username')\n",
    "        df_posts_list.append(post_entry)\n",
    "\n",
    "df_profiles = pd.DataFrame(df_profiles_list)\n",
    "df_posts = pd.DataFrame(df_posts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING & FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification DataFrame: (2742, 2)\n",
      "Profiles DataFrame: (5415, 44)\n",
      "Posts DataFrame: (187302, 8)\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows and columns in each DataFrame\n",
    "print(f\"Classification DataFrame: {df_classification.shape}\")\n",
    "print(f\"Profiles DataFrame: {df_profiles.shape}\")\n",
    "print(f\"Posts DataFrame: {df_posts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2742 entries, 0 to 2741\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  2742 non-null   object\n",
      " 1   label       2742 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 43.0+ KB\n",
      "None\n",
      "\n",
      "Profiles DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5415 entries, 0 to 5414\n",
      "Data columns (total 44 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   username                           5415 non-null   object \n",
      " 1   id                                 5415 non-null   object \n",
      " 2   full_name                          5359 non-null   object \n",
      " 3   biography                          5067 non-null   object \n",
      " 4   category_name                      4395 non-null   object \n",
      " 5   post_count                         579 non-null    float64\n",
      " 6   follower_count                     5415 non-null   int64  \n",
      " 7   following_count                    5415 non-null   int64  \n",
      " 8   is_business_account                5415 non-null   bool   \n",
      " 9   is_private                         5415 non-null   bool   \n",
      " 10  is_verified                        5415 non-null   bool   \n",
      " 11  highlight_reel_count               5415 non-null   int64  \n",
      " 12  bio_links                          5415 non-null   object \n",
      " 13  entities                           5067 non-null   object \n",
      " 14  ai_agent_type                      0 non-null      object \n",
      " 15  fb_profile_biolink                 0 non-null      object \n",
      " 16  restricted_by_viewer               0 non-null      object \n",
      " 17  country_block                      5415 non-null   bool   \n",
      " 18  eimu_id                            5415 non-null   object \n",
      " 19  external_url                       4490 non-null   object \n",
      " 20  fbid                               5415 non-null   object \n",
      " 21  has_clips                          5415 non-null   bool   \n",
      " 22  hide_like_and_view_counts          5415 non-null   bool   \n",
      " 23  is_professional_account            5415 non-null   bool   \n",
      " 24  is_supervision_enabled             5415 non-null   bool   \n",
      " 25  is_guardian_of_viewer              5415 non-null   bool   \n",
      " 26  is_supervised_by_viewer            5415 non-null   bool   \n",
      " 27  is_supervised_user                 5415 non-null   bool   \n",
      " 28  is_embeds_disabled                 5415 non-null   bool   \n",
      " 29  is_joined_recently                 5415 non-null   bool   \n",
      " 30  business_address_json              5415 non-null   object \n",
      " 31  business_contact_method            5415 non-null   object \n",
      " 32  business_email                     0 non-null      object \n",
      " 33  business_phone_number              0 non-null      object \n",
      " 34  business_category_name             543 non-null    object \n",
      " 35  overall_category_name              0 non-null      object \n",
      " 36  category_enum                      3191 non-null   object \n",
      " 37  is_verified_by_mv4b                5415 non-null   bool   \n",
      " 38  is_regulated_c18                   5415 non-null   bool   \n",
      " 39  profile_pic_url                    5415 non-null   object \n",
      " 40  should_show_category               5415 non-null   bool   \n",
      " 41  should_show_public_contacts        5415 non-null   bool   \n",
      " 42  show_account_transparency_details  5415 non-null   bool   \n",
      " 43  profile_picture_base64             5414 non-null   object \n",
      "dtypes: bool(18), float64(1), int64(3), object(22)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "\n",
      "Posts DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 187302 entries, 0 to 187301\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   caption         180668 non-null  object \n",
      " 1   comments_count  187302 non-null  int64  \n",
      " 2   id              187302 non-null  object \n",
      " 3   like_count      183083 non-null  float64\n",
      " 4   media_type      187302 non-null  object \n",
      " 5   media_url       175396 non-null  object \n",
      " 6   timestamp       187302 non-null  object \n",
      " 7   username        187302 non-null  object \n",
      "dtypes: float64(1), int64(1), object(6)\n",
      "memory usage: 11.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# View column names and data types for each DataFrame\n",
    "print(\"Classification DataFrame Info:\")\n",
    "print(df_classification.info())\n",
    "print(\"\\nProfiles DataFrame Info:\")\n",
    "print(df_profiles.info())\n",
    "print(\"\\nPosts DataFrame Info:\")\n",
    "print(df_posts.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Classification DataFrame:\n",
      "Unnamed: 0    0\n",
      "label         0\n",
      "dtype: int64\n",
      "\n",
      "Missing Values in Profiles DataFrame:\n",
      "username                                0\n",
      "id                                      0\n",
      "full_name                              56\n",
      "biography                             348\n",
      "category_name                        1020\n",
      "post_count                           4836\n",
      "follower_count                          0\n",
      "following_count                         0\n",
      "is_business_account                     0\n",
      "is_private                              0\n",
      "is_verified                             0\n",
      "highlight_reel_count                    0\n",
      "bio_links                               0\n",
      "entities                              348\n",
      "ai_agent_type                        5415\n",
      "fb_profile_biolink                   5415\n",
      "restricted_by_viewer                 5415\n",
      "country_block                           0\n",
      "eimu_id                                 0\n",
      "external_url                          925\n",
      "fbid                                    0\n",
      "has_clips                               0\n",
      "hide_like_and_view_counts               0\n",
      "is_professional_account                 0\n",
      "is_supervision_enabled                  0\n",
      "is_guardian_of_viewer                   0\n",
      "is_supervised_by_viewer                 0\n",
      "is_supervised_user                      0\n",
      "is_embeds_disabled                      0\n",
      "is_joined_recently                      0\n",
      "business_address_json                   0\n",
      "business_contact_method                 0\n",
      "business_email                       5415\n",
      "business_phone_number                5415\n",
      "business_category_name               4872\n",
      "overall_category_name                5415\n",
      "category_enum                        2224\n",
      "is_verified_by_mv4b                     0\n",
      "is_regulated_c18                        0\n",
      "profile_pic_url                         0\n",
      "should_show_category                    0\n",
      "should_show_public_contacts             0\n",
      "show_account_transparency_details       0\n",
      "profile_picture_base64                  1\n",
      "dtype: int64\n",
      "\n",
      "Missing Values in Posts DataFrame:\n",
      "caption            6634\n",
      "comments_count        0\n",
      "id                    0\n",
      "like_count         4219\n",
      "media_type            0\n",
      "media_url         11906\n",
      "timestamp             0\n",
      "username              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each DataFrame\n",
    "print(\"Missing Values in Classification DataFrame:\")\n",
    "print(df_classification.isnull().sum())\n",
    "print(\"\\nMissing Values in Profiles DataFrame:\")\n",
    "print(df_profiles.isnull().sum())\n",
    "print(\"\\nMissing Values in Posts DataFrame:\")\n",
    "print(df_posts.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0             label\n",
      "0    taskirancemal  Mom and Children\n",
      "1    tam_kararinda              Food\n",
      "2         spart4nn              Food\n",
      "3  sosyalyiyiciler              Food\n",
      "4  sonaydizdarahad  Mom and Children\n",
      "\n",
      "\n",
      "          username             label\n",
      "0    taskirancemal  Mom and Children\n",
      "1    tam_kararinda              Food\n",
      "2         spart4nn              Food\n",
      "3  sosyalyiyiciler              Food\n",
      "4  sonaydizdarahad  Mom and Children\n",
      "\n",
      "\n",
      "Index(['username', 'label'], dtype='object')\n",
      "\n",
      "\n",
      "Duplicate Usernames in Classification DataFrame: 0\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of df_classification\n",
    "print(df_classification.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Rename 'Unnamed: 0' to 'username'\n",
    "df_classification.rename(columns={'Unnamed: 0': 'username'}, inplace=True)\n",
    "\n",
    "# Verify the renaming\n",
    "print(df_classification.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check the updated columns\n",
    "print(df_classification.columns)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check for duplicate usernames in Classification DataFrame\n",
    "duplicate_classification = df_classification['username'].duplicated().sum()\n",
    "print(f\"Duplicate Usernames in Classification DataFrame: {duplicate_classification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
      "Duplicated Columns in Profiles DataFrame: []\n",
      "Columns after removing duplicates: ['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
      "Remaining Duplicated Columns in Profiles DataFrame: []\n"
     ]
    }
   ],
   "source": [
    "# List all column names in df_profiles\n",
    "print(df_profiles.columns.tolist())\n",
    "\n",
    "# Find duplicated column names\n",
    "duplicated_columns = df_profiles.columns[df_profiles.columns.duplicated()]\n",
    "print(f\"Duplicated Columns in Profiles DataFrame: {duplicated_columns.tolist()}\")\n",
    "\n",
    "# Remove duplicated columns, keeping the first occurrence\n",
    "df_profiles_clean = df_profiles.loc[:, ~df_profiles.columns.duplicated()]\n",
    "\n",
    "# Verify the columns after removal\n",
    "print(\"Columns after removing duplicates:\", df_profiles_clean.columns.tolist())\n",
    "\n",
    "# Check for any remaining duplicated columns\n",
    "remaining_duplicates = df_profiles_clean.columns[df_profiles_clean.columns.duplicated()]\n",
    "print(f\"Remaining Duplicated Columns in Profiles DataFrame: {remaining_duplicates.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values after Cleaning in Profiles DataFrame:\n",
      "username                                0\n",
      "id                                      0\n",
      "full_name                               0\n",
      "biography                               0\n",
      "category_name                           0\n",
      "post_count                              0\n",
      "follower_count                          0\n",
      "following_count                         0\n",
      "is_business_account                     0\n",
      "is_private                              0\n",
      "is_verified                             0\n",
      "highlight_reel_count                    0\n",
      "bio_links                               0\n",
      "entities                              348\n",
      "country_block                           0\n",
      "eimu_id                                 0\n",
      "external_url                            0\n",
      "fbid                                    0\n",
      "has_clips                               0\n",
      "hide_like_and_view_counts               0\n",
      "is_professional_account                 0\n",
      "is_supervision_enabled                  0\n",
      "is_guardian_of_viewer                   0\n",
      "is_supervised_by_viewer                 0\n",
      "is_supervised_user                      0\n",
      "is_embeds_disabled                      0\n",
      "is_joined_recently                      0\n",
      "business_address_json                   0\n",
      "business_contact_method                 0\n",
      "business_email                       5415\n",
      "business_phone_number                5415\n",
      "business_category_name               4872\n",
      "overall_category_name                5415\n",
      "category_enum                        2224\n",
      "is_verified_by_mv4b                     0\n",
      "is_regulated_c18                        0\n",
      "profile_pic_url                         0\n",
      "should_show_category                    0\n",
      "should_show_public_contacts             0\n",
      "show_account_transparency_details       0\n",
      "profile_picture_base64                  1\n",
      "dtype: int64\n",
      "Dropped 'profile_picture_base64' and 'entities' columns from Profiles DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# 1. Drop columns that are entirely missing or irrelevant\n",
    "columns_to_drop = ['ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer']\n",
    "df_profiles_clean = df_profiles_clean.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# 2. Fill missing 'full_name' with empty string\n",
    "df_profiles_clean['full_name'] = df_profiles_clean['full_name'].fillna('')\n",
    "\n",
    "# 3. Fill missing 'biography' with empty string\n",
    "df_profiles_clean['biography'] = df_profiles_clean['biography'].fillna('')\n",
    "\n",
    "# 4. Fill missing 'category_name' with 'Unknown'\n",
    "df_profiles_clean['category_name'] = df_profiles_clean['category_name'].fillna('Unknown')\n",
    "\n",
    "# 5. Fill missing 'post_count' with 0 and convert to integer\n",
    "df_profiles_clean['post_count'] = df_profiles_clean['post_count'].fillna(0).astype(int)\n",
    "\n",
    "# 6. Handle 'entities' by filling missing with empty dictionary\n",
    "df_profiles_clean['entities'] = df_profiles_clean['entities'].fillna({})\n",
    "\n",
    "# 7. Fill missing 'external_url' with empty string\n",
    "if 'external_url' in df_profiles_clean.columns:\n",
    "    df_profiles_clean['external_url'] = df_profiles_clean['external_url'].fillna('')\n",
    "\n",
    "# Check for remaining missing values in Profiles DataFrame\n",
    "print(\"Missing Values after Cleaning in Profiles DataFrame:\")\n",
    "print(df_profiles_clean.isnull().sum())\n",
    "\n",
    "# 1. Drop 'profile_picture_base64' column\n",
    "df_profiles_clean = df_profiles_clean.drop(columns=['profile_picture_base64'], errors='ignore')\n",
    "\n",
    "# 2. Drop 'entities' column (if deemed unnecessary)\n",
    "df_profiles_clean = df_profiles_clean.drop(columns=['entities'], errors='ignore')\n",
    "\n",
    "# If you decide to keep 'entities', use the following instead:\n",
    "# df_profiles_clean['entities'] = df_profiles_clean['entities'].fillna({})\n",
    "\n",
    "print(\"Dropped 'profile_picture_base64' and 'entities' columns from Profiles DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values after Cleaning in Profiles DataFrame:\n",
      "username                                0\n",
      "id                                      0\n",
      "full_name                               0\n",
      "biography                               0\n",
      "category_name                           0\n",
      "post_count                              0\n",
      "follower_count                          0\n",
      "following_count                         0\n",
      "is_business_account                     0\n",
      "is_private                              0\n",
      "is_verified                             0\n",
      "highlight_reel_count                    0\n",
      "bio_links                               0\n",
      "country_block                           0\n",
      "eimu_id                                 0\n",
      "external_url                            0\n",
      "fbid                                    0\n",
      "has_clips                               0\n",
      "hide_like_and_view_counts               0\n",
      "is_professional_account                 0\n",
      "is_supervision_enabled                  0\n",
      "is_guardian_of_viewer                   0\n",
      "is_supervised_by_viewer                 0\n",
      "is_supervised_user                      0\n",
      "is_embeds_disabled                      0\n",
      "is_joined_recently                      0\n",
      "business_address_json                   0\n",
      "business_contact_method                 0\n",
      "business_email                       5415\n",
      "business_phone_number                5415\n",
      "business_category_name               4872\n",
      "overall_category_name                5415\n",
      "category_enum                        2224\n",
      "is_verified_by_mv4b                     0\n",
      "is_regulated_c18                        0\n",
      "profile_pic_url                         0\n",
      "should_show_category                    0\n",
      "should_show_public_contacts             0\n",
      "show_account_transparency_details       0\n",
      "dtype: int64\n",
      "Dropped columns with all missing values: ['business_email', 'business_phone_number', 'overall_category_name']\n",
      "Dropped columns with high missingness: ['business_category_name', 'category_enum']\n",
      "Missing Values after Final Cleaning in Profiles DataFrame:\n",
      "username                             0\n",
      "id                                   0\n",
      "full_name                            0\n",
      "biography                            0\n",
      "category_name                        0\n",
      "post_count                           0\n",
      "follower_count                       0\n",
      "following_count                      0\n",
      "is_business_account                  0\n",
      "is_private                           0\n",
      "is_verified                          0\n",
      "highlight_reel_count                 0\n",
      "bio_links                            0\n",
      "country_block                        0\n",
      "eimu_id                              0\n",
      "external_url                         0\n",
      "fbid                                 0\n",
      "has_clips                            0\n",
      "hide_like_and_view_counts            0\n",
      "is_professional_account              0\n",
      "is_supervision_enabled               0\n",
      "is_guardian_of_viewer                0\n",
      "is_supervised_by_viewer              0\n",
      "is_supervised_user                   0\n",
      "is_embeds_disabled                   0\n",
      "is_joined_recently                   0\n",
      "business_address_json                0\n",
      "business_contact_method              0\n",
      "is_verified_by_mv4b                  0\n",
      "is_regulated_c18                     0\n",
      "profile_pic_url                      0\n",
      "should_show_category                 0\n",
      "should_show_public_contacts          0\n",
      "show_account_transparency_details    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for remaining missing values in Profiles DataFrame\n",
    "print(\"Missing Values after Cleaning in Profiles DataFrame:\")\n",
    "print(df_profiles_clean.isnull().sum())\n",
    "\n",
    "# Define columns to drop that have all entries missing\n",
    "columns_to_drop_all_missing = ['business_email', 'business_phone_number', 'overall_category_name']\n",
    "\n",
    "# Drop these columns from df_profiles_clean\n",
    "df_profiles_clean = df_profiles_clean.drop(columns=columns_to_drop_all_missing, errors='ignore')\n",
    "\n",
    "print(\"Dropped columns with all missing values:\", columns_to_drop_all_missing)\n",
    "\n",
    "# Define columns with high missingness\n",
    "columns_to_drop_high_missing = ['business_category_name', 'category_enum']\n",
    "\n",
    "# Drop these columns from df_profiles_clean\n",
    "df_profiles_clean = df_profiles_clean.drop(columns=columns_to_drop_high_missing, errors='ignore')\n",
    "\n",
    "print(\"Dropped columns with high missingness:\", columns_to_drop_high_missing)\n",
    "\n",
    "# Check for remaining missing values in Profiles DataFrame\n",
    "print(\"Missing Values after Final Cleaning in Profiles DataFrame:\")\n",
    "print(df_profiles_clean.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['entities', 'profile_picture_base64']\n",
      "Missing Values after Final Cleaning in Profiles DataFrame:\n",
      "username                             0\n",
      "id                                   0\n",
      "full_name                            0\n",
      "biography                            0\n",
      "category_name                        0\n",
      "post_count                           0\n",
      "follower_count                       0\n",
      "following_count                      0\n",
      "is_business_account                  0\n",
      "is_private                           0\n",
      "is_verified                          0\n",
      "highlight_reel_count                 0\n",
      "bio_links                            0\n",
      "country_block                        0\n",
      "eimu_id                              0\n",
      "external_url                         0\n",
      "fbid                                 0\n",
      "has_clips                            0\n",
      "hide_like_and_view_counts            0\n",
      "is_professional_account              0\n",
      "is_supervision_enabled               0\n",
      "is_guardian_of_viewer                0\n",
      "is_supervised_by_viewer              0\n",
      "is_supervised_user                   0\n",
      "is_embeds_disabled                   0\n",
      "is_joined_recently                   0\n",
      "business_address_json                0\n",
      "business_contact_method              0\n",
      "is_verified_by_mv4b                  0\n",
      "is_regulated_c18                     0\n",
      "profile_pic_url                      0\n",
      "should_show_category                 0\n",
      "should_show_public_contacts          0\n",
      "show_account_transparency_details    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the columns to drop\n",
    "columns_to_drop_remaining = ['entities', 'profile_picture_base64']\n",
    "\n",
    "# 2. Drop the specified columns from df_profiles_clean\n",
    "df_profiles_clean = df_profiles_clean.drop(columns=columns_to_drop_remaining, errors='ignore')\n",
    "\n",
    "print(f\"Dropped columns: {columns_to_drop_remaining}\")\n",
    "\n",
    "# Check for remaining missing values in Profiles DataFrame\n",
    "print(\"Missing Values after Final Cleaning in Profiles DataFrame:\")\n",
    "print(df_profiles_clean.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 standardized usernames in df_classification_clean:\n",
      "0      taskirancemal\n",
      "1      tam_kararinda\n",
      "2           spart4nn\n",
      "3    sosyalyiyiciler\n",
      "4    sonaydizdarahad\n",
      "Name: username, dtype: object\n",
      "\n",
      "First 5 standardized usernames in df_profiles_clean:\n",
      "0                    deparmedya\n",
      "1                beyazyakaliyiz\n",
      "2                    kafesfirin\n",
      "3                      vimerang\n",
      "4    totalenergies_istasyonlari\n",
      "Name: username, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Standardize 'username' in Classification DataFrame\n",
    "df_classification_clean = df_classification.copy()\n",
    "df_classification_clean['username'] = df_classification_clean['username'].str.strip().str.lower()\n",
    "\n",
    "# Standardize 'username' in Profiles DataFrame\n",
    "df_profiles_clean = df_profiles_clean.copy()\n",
    "df_profiles_clean['username'] = df_profiles_clean['username'].str.strip().str.lower()\n",
    "\n",
    "# Inspect standardized usernames in Classification DataFrame\n",
    "print(\"First 5 standardized usernames in df_classification_clean:\")\n",
    "print(df_classification_clean['username'].head())\n",
    "\n",
    "# Inspect standardized usernames in Profiles DataFrame\n",
    "print(\"\\nFirst 5 standardized usernames in df_profiles_clean:\")\n",
    "print(df_profiles_clean['username'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping users: 2741\n",
      "Sample overlapping usernames: ['sumeyyeboyacii', 'safiloturkey', 'sivilsayfalar', 'frozzafood', 'yalcinyasemin', 'hatemoglu1924', 'istanbuldentopia', 'a.y.s.store', 'profdrbariscaynak', 'tick.tock.boom']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_posts is your posts DataFrame and has a 'username' column\n",
    "# and df_classification_clean and df_profiles_clean have been standardized\n",
    "\n",
    "# Create a set of overlapping usernames for efficient lookup\n",
    "overlapping_users = set(df_classification_clean['username']).intersection(set(df_profiles_clean['username']))\n",
    "\n",
    "print(f\"Number of overlapping users: {len(overlapping_users)}\")\n",
    "print(f\"Sample overlapping usernames: {list(overlapping_users)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_posts_overlap: (94824, 8)\n"
     ]
    }
   ],
   "source": [
    "# Filter df_posts to include only overlapping users\n",
    "df_posts_overlap = df_posts[df_posts['username'].isin(overlapping_users)].copy()\n",
    "\n",
    "print(f\"Shape of df_posts_overlap: {df_posts_overlap.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing or invalid 'timestamp' entries: 0\n",
      "Shape after dropping missing timestamps: (94824, 8)\n"
     ]
    }
   ],
   "source": [
    "# Convert 'timestamp' to datetime\n",
    "df_posts_overlap['timestamp'] = pd.to_datetime(df_posts_overlap['timestamp'], errors='coerce')\n",
    "\n",
    "# Check for any missing or invalid timestamps\n",
    "missing_timestamps = df_posts_overlap['timestamp'].isnull().sum()\n",
    "print(f\"Number of missing or invalid 'timestamp' entries: {missing_timestamps}\")\n",
    "\n",
    "# Optionally, drop rows with missing timestamps\n",
    "df_posts_overlap = df_posts_overlap.dropna(subset=['timestamp'])\n",
    "print(f\"Shape after dropping missing timestamps: {df_posts_overlap.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated Post-Level Data (df_post_agg):\n",
      "        username   avg_likes  avg_comments  total_posts           last_post\n",
      "0      1001sanat   20.657143      0.485714           35 2023-10-30 08:38:25\n",
      "1   1924istanbul  571.742857     13.457143           35 2023-06-28 11:15:27\n",
      "2     1dil1insan   16.400000      0.657143           35 2023-10-29 12:51:31\n",
      "3  1kitap.1mekan  842.100000     25.342857           35 2023-10-08 09:25:46\n",
      "4  1kizinonerisi  302.571429      1.571429           35 2023-09-08 19:02:15\n"
     ]
    }
   ],
   "source": [
    "# Aggregating post-level data\n",
    "df_post_agg = df_posts_overlap.groupby('username').agg(\n",
    "    avg_likes=('like_count', 'mean'),\n",
    "    avg_comments=('comments_count', 'mean'),\n",
    "    total_posts=('id', 'count'),       # Assuming 'id' is unique per post\n",
    "    last_post=('timestamp', 'max')     # Most recent post date\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nAggregated Post-Level Data (df_post_agg):\")\n",
    "print(df_post_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged df_user DataFrame:\n",
      "          username             label          id        full_name  \\\n",
      "0    taskirancemal  Mom and Children  1282703608   Cemal TaÅŸkÄ±ran   \n",
      "1    tam_kararinda              Food  2114951482      Kaan Yarman   \n",
      "2         spart4nn              Food   645610128     Cemil Ceylan   \n",
      "3  sosyalyiyiciler              Food  1671187359                    \n",
      "4  sonaydizdarahad  Mom and Children  1635669992  Sonay Demiryeri   \n",
      "\n",
      "                                           biography    category_name  \\\n",
      "0                                     ðŸ“Antalya / KaÅŸ     Entrepreneur   \n",
      "1     Milliyet Pazar\\nKalori Alacaksan Buna DeÄŸecekðŸ“š  Kitchen/cooking   \n",
      "2  KÃ¼Ã§Ã¼k ev\\nKamp ve doÄŸa hayatÄ±ðŸ•\\nMutfaÄŸÄ±mÄ±z doÄŸ...    Video creator   \n",
      "3  Founder @bitte.izmir \\nIZMIR yemek/seyahat foo...          Unknown   \n",
      "4  DÄ°ZDAR ðŸ§¿ Ahad ðŸ§¿ DaÄŸhan \\n@d.a.d.kids \\n@dilanp...    Personal blog   \n",
      "\n",
      "   post_count  follower_count  following_count is_business_account  ...  \\\n",
      "0      1382.0         12145.0           1064.0               False  ...   \n",
      "1       619.0        783223.0            867.0               False  ...   \n",
      "2      2094.0       1127332.0            496.0               False  ...   \n",
      "3       919.0         27002.0           4279.0                True  ...   \n",
      "4      2387.0        781362.0           1877.0                True  ...   \n",
      "\n",
      "  is_embeds_disabled is_joined_recently  \\\n",
      "0              False              False   \n",
      "1              False              False   \n",
      "2              False              False   \n",
      "3              False              False   \n",
      "4              False              False   \n",
      "\n",
      "                               business_address_json business_contact_method  \\\n",
      "0  \"{\\\"city_name\\\": null, \\\"city_id\\\": null, \\\"la...                 UNKNOWN   \n",
      "1  \"{\\\"city_name\\\": null, \\\"city_id\\\": null, \\\"la...                 UNKNOWN   \n",
      "2                                             \"None\"                 UNKNOWN   \n",
      "3  \"{\\\"city_name\\\": null, \\\"city_id\\\": null, \\\"la...                 UNKNOWN   \n",
      "4  \"{\\\"city_name\\\": null, \\\"city_id\\\": null, \\\"la...                 UNKNOWN   \n",
      "\n",
      "  is_verified_by_mv4b is_regulated_c18  \\\n",
      "0               False            False   \n",
      "1               False            False   \n",
      "2               False            False   \n",
      "3               False            False   \n",
      "4               False            False   \n",
      "\n",
      "                                     profile_pic_url should_show_category  \\\n",
      "0  https://z-p42-instagram.fsaw1-14.fna.fbcdn.net...                 True   \n",
      "1  https://instagram.fist1-1.fna.fbcdn.net/v/t51....                 True   \n",
      "2  https://z-p42-instagram.fsaw1-12.fna.fbcdn.net...                False   \n",
      "3  https://instagram.fadb2-2.fna.fbcdn.net/v/t51....                 True   \n",
      "4  https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....                 True   \n",
      "\n",
      "  should_show_public_contacts show_account_transparency_details  \n",
      "0                        True                              True  \n",
      "1                        True                              True  \n",
      "2                        True                              True  \n",
      "3                        True                              True  \n",
      "4                        True                              True  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge classification and profiles data on 'username'\n",
    "df_user = pd.merge(\n",
    "    df_classification_clean,\n",
    "    df_profiles_clean,\n",
    "    on='username',\n",
    "    how='left',            # Keeps all users from classification\n",
    "    suffixes=('_class', '_profile')\n",
    ")\n",
    "\n",
    "print(\"\\nMerged df_user DataFrame:\")\n",
    "print(df_user.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_user after merging post-level aggregated data:\n",
      "          username     avg_likes  avg_comments  total_posts  \\\n",
      "0    taskirancemal    422.971429     12.657143         35.0   \n",
      "1    tam_kararinda  13460.457143    260.685714         35.0   \n",
      "2         spart4nn  93142.085714    586.257143         35.0   \n",
      "3  sosyalyiyiciler    309.285714     17.971429         35.0   \n",
      "4  sonaydizdarahad  45827.342857    625.685714         35.0   \n",
      "\n",
      "            last_post  \n",
      "0 2023-05-15 14:32:32  \n",
      "1 2023-10-17 14:55:11  \n",
      "2 2023-10-14 14:17:30  \n",
      "3 2023-10-08 07:47:03  \n",
      "4 2023-10-07 20:04:25  \n"
     ]
    }
   ],
   "source": [
    "# Merge post-level aggregated data into df_user\n",
    "df_user = pd.merge(\n",
    "    df_user,\n",
    "    df_post_agg,\n",
    "    on='username',\n",
    "    how='left'            # Keeps all users from df_user\n",
    ")\n",
    "\n",
    "print(\"\\ndf_user after merging post-level aggregated data:\")\n",
    "print(df_user[['username', 'avg_likes', 'avg_comments', 'total_posts', 'last_post']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of users with post-level data: 2719\n",
      "Number of users without post-level data: 23\n",
      "\n",
      "Sample users without post-level data:\n",
      "682      belediyesikose\n",
      "1037    khloekardashian\n",
      "1044      karikaturcaps\n",
      "1048         aslangozde\n",
      "1068       sserelyereli\n",
      "1148        meerveebass\n",
      "1270            hakanuc\n",
      "1313         halicizade\n",
      "1407    kumbahcemobilya\n",
      "1481         sebainsaat\n",
      "Name: username, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Number of users with post-level data\n",
    "num_with_posts = df_user['avg_likes'].notnull().sum()\n",
    "num_without_posts = df_user['avg_likes'].isnull().sum()\n",
    "\n",
    "print(f\"\\nNumber of users with post-level data: {num_with_posts}\")\n",
    "print(f\"Number of users without post-level data: {num_without_posts}\")\n",
    "\n",
    "# Display users without post-level data, if any\n",
    "if num_without_posts > 0:\n",
    "    print(\"\\nSample users without post-level data:\")\n",
    "    print(df_user[df_user['avg_likes'].isnull()]['username'].head(10))\n",
    "else:\n",
    "    print(\"\\nAll users have post-level data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical Aggregated Features After Filling Missing Values:\n",
      "                  username     avg_likes  avg_comments  total_posts\n",
      "2717             serefoguz    140.028571      2.771429         35.0\n",
      "2718        tahincioglugmk     72.285714      1.800000         35.0\n",
      "2719       thecraton.hotel     21.828571      0.085714         35.0\n",
      "2720        kaplan_turkiye     20.000000      0.314286         35.0\n",
      "2721            demiirhaan  50490.400000    235.000000         35.0\n",
      "2722              rawsters     32.166667      0.400000         35.0\n",
      "2723      trendy.shoppings    492.333333     32.285714         35.0\n",
      "2724  pastirmasucukkayseri    117.314286      7.028571         35.0\n",
      "2725       villeroyboch_tr     11.714286      0.085714         35.0\n",
      "2726             erkimilac     27.771429      0.028571         35.0\n",
      "2727           kisafilmder    224.428571      8.085714         35.0\n",
      "2728            tiytroadam    106.342857      2.885714         35.0\n",
      "2729            cem.boyner   3080.657143    118.742857         35.0\n",
      "2730            visitsplit    862.457143     13.857143         35.0\n",
      "2731             thehunger    178.954545      6.514286         35.0\n",
      "2732      kadir_albayrak59    522.600000      6.171429         35.0\n",
      "2733           muzegazhane    353.542857      3.285714         35.0\n",
      "2734          seyhaneturla     94.314286      1.600000         35.0\n",
      "2735      sistem.aluminyum     88.085714      0.314286         35.0\n",
      "2736   kocaeligazetesi1975      3.257143      0.142857         35.0\n",
      "2737            rubisaglik     23.371429      0.857143         35.0\n",
      "2738       safamecomercial      5.371429      0.057143         35.0\n",
      "2739          sahrapsoysal   8982.742857    249.200000         35.0\n",
      "2740      woodtechistanbul     14.600000      0.057143         35.0\n",
      "2741         herbisiatolye    100.457143      1.885714         35.0\n"
     ]
    }
   ],
   "source": [
    "# Fill missing numerical aggregated data with 0\n",
    "df_user[['avg_likes', 'avg_comments', 'total_posts']] = df_user[['avg_likes', 'avg_comments', 'total_posts']].fillna(0)\n",
    "\n",
    "print(\"\\nNumerical Aggregated Features After Filling Missing Values:\")\n",
    "print(df_user[['username', 'avg_likes', 'avg_comments', 'total_posts']].tail(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'last_post' Feature After Filling Missing Values:\n",
      "                  username           last_post\n",
      "2717             serefoguz 2023-11-18 03:47:44\n",
      "2718        tahincioglugmk 2023-11-21 08:12:29\n",
      "2719       thecraton.hotel 2023-11-22 16:12:51\n",
      "2720        kaplan_turkiye 2023-10-11 11:30:05\n",
      "2721            demiirhaan 2023-10-24 17:04:31\n",
      "2722              rawsters 2023-11-13 07:50:24\n",
      "2723      trendy.shoppings 2023-11-03 19:26:14\n",
      "2724  pastirmasucukkayseri 2023-11-21 16:58:15\n",
      "2725       villeroyboch_tr 2023-11-29 07:56:54\n",
      "2726             erkimilac 2023-10-27 13:13:05\n",
      "2727           kisafilmder 2023-10-26 16:16:46\n",
      "2728            tiytroadam 2023-11-23 16:12:28\n",
      "2729            cem.boyner 2023-10-22 13:11:23\n",
      "2730            visitsplit 2023-10-20 07:00:03\n",
      "2731             thehunger 2023-11-22 11:26:25\n",
      "2732      kadir_albayrak59 2023-11-04 20:09:05\n",
      "2733           muzegazhane 2023-11-17 10:23:08\n",
      "2734          seyhaneturla 2022-11-17 18:11:55\n",
      "2735      sistem.aluminyum 2023-11-17 14:43:08\n",
      "2736   kocaeligazetesi1975 2023-11-07 12:44:20\n",
      "2737            rubisaglik 2022-12-31 09:54:53\n",
      "2738       safamecomercial 2022-12-03 10:01:42\n",
      "2739          sahrapsoysal 2023-11-16 07:05:42\n",
      "2740      woodtechistanbul 2023-11-10 07:54:12\n",
      "2741         herbisiatolye 2023-11-18 18:46:35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define placeholder date\n",
    "placeholder_date = pd.Timestamp('1900-01-01')\n",
    "\n",
    "# Fill missing 'last_post' with the placeholder date\n",
    "df_user['last_post'] = df_user['last_post'].fillna(placeholder_date)\n",
    "\n",
    "print(\"\\n'last_post' Feature After Filling Missing Values:\")\n",
    "print(df_user[['username', 'last_post']].tail(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'last_post' Feature After Filling Missing Values:\n",
      "                  username           last_post\n",
      "2717             serefoguz 2023-11-18 03:47:44\n",
      "2718        tahincioglugmk 2023-11-21 08:12:29\n",
      "2719       thecraton.hotel 2023-11-22 16:12:51\n",
      "2720        kaplan_turkiye 2023-10-11 11:30:05\n",
      "2721            demiirhaan 2023-10-24 17:04:31\n",
      "2722              rawsters 2023-11-13 07:50:24\n",
      "2723      trendy.shoppings 2023-11-03 19:26:14\n",
      "2724  pastirmasucukkayseri 2023-11-21 16:58:15\n",
      "2725       villeroyboch_tr 2023-11-29 07:56:54\n",
      "2726             erkimilac 2023-10-27 13:13:05\n",
      "2727           kisafilmder 2023-10-26 16:16:46\n",
      "2728            tiytroadam 2023-11-23 16:12:28\n",
      "2729            cem.boyner 2023-10-22 13:11:23\n",
      "2730            visitsplit 2023-10-20 07:00:03\n",
      "2731             thehunger 2023-11-22 11:26:25\n",
      "2732      kadir_albayrak59 2023-11-04 20:09:05\n",
      "2733           muzegazhane 2023-11-17 10:23:08\n",
      "2734          seyhaneturla 2022-11-17 18:11:55\n",
      "2735      sistem.aluminyum 2023-11-17 14:43:08\n",
      "2736   kocaeligazetesi1975 2023-11-07 12:44:20\n",
      "2737            rubisaglik 2022-12-31 09:54:53\n",
      "2738       safamecomercial 2022-12-03 10:01:42\n",
      "2739          sahrapsoysal 2023-11-16 07:05:42\n",
      "2740      woodtechistanbul 2023-11-10 07:54:12\n",
      "2741         herbisiatolye 2023-11-18 18:46:35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define placeholder date\n",
    "placeholder_date = pd.Timestamp('1900-01-01')\n",
    "\n",
    "# Fill missing 'last_post' with the placeholder date\n",
    "df_user['last_post'] = df_user['last_post'].fillna(placeholder_date)\n",
    "\n",
    "print(\"\\n'last_post' Feature After Filling Missing Values:\")\n",
    "print(df_user[['username', 'last_post']].tail(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'days_since_last_post' Feature After Imputation:\n",
      "                  username           last_post  days_since_last_post\n",
      "2717             serefoguz 2023-11-18 03:47:44                   400\n",
      "2718        tahincioglugmk 2023-11-21 08:12:29                   397\n",
      "2719       thecraton.hotel 2023-11-22 16:12:51                   396\n",
      "2720        kaplan_turkiye 2023-10-11 11:30:05                   438\n",
      "2721            demiirhaan 2023-10-24 17:04:31                   425\n",
      "2722              rawsters 2023-11-13 07:50:24                   405\n",
      "2723      trendy.shoppings 2023-11-03 19:26:14                   415\n",
      "2724  pastirmasucukkayseri 2023-11-21 16:58:15                   397\n",
      "2725       villeroyboch_tr 2023-11-29 07:56:54                   389\n",
      "2726             erkimilac 2023-10-27 13:13:05                   422\n",
      "2727           kisafilmder 2023-10-26 16:16:46                   423\n",
      "2728            tiytroadam 2023-11-23 16:12:28                   395\n",
      "2729            cem.boyner 2023-10-22 13:11:23                   427\n",
      "2730            visitsplit 2023-10-20 07:00:03                   429\n",
      "2731             thehunger 2023-11-22 11:26:25                   396\n",
      "2732      kadir_albayrak59 2023-11-04 20:09:05                   414\n",
      "2733           muzegazhane 2023-11-17 10:23:08                   401\n",
      "2734          seyhaneturla 2022-11-17 18:11:55                   766\n",
      "2735      sistem.aluminyum 2023-11-17 14:43:08                   401\n",
      "2736   kocaeligazetesi1975 2023-11-07 12:44:20                   411\n",
      "2737            rubisaglik 2022-12-31 09:54:53                   722\n",
      "2738       safamecomercial 2022-12-03 10:01:42                   750\n",
      "2739          sahrapsoysal 2023-11-16 07:05:42                   402\n",
      "2740      woodtechistanbul 2023-11-10 07:54:12                   408\n",
      "2741         herbisiatolye 2023-11-18 18:46:35                   400\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Current date for reference\n",
    "current_date = pd.Timestamp(datetime.now())\n",
    "\n",
    "# Calculate Days Since Last Post\n",
    "df_user['days_since_last_post'] = (current_date - df_user['last_post']).dt.days\n",
    "\n",
    "# Handle anomalies: negative days if 'last_post' is in the future\n",
    "df_user['days_since_last_post'] = df_user['days_since_last_post'].apply(lambda x: x if x >= 0 else 0)\n",
    "\n",
    "# For users with placeholder 'last_post', set 'days_since_last_post' to -1 to indicate no posts\n",
    "df_user.loc[df_user['last_post'] == placeholder_date, 'days_since_last_post'] = -1\n",
    "\n",
    "print(\"\\n'days_since_last_post' Feature After Imputation:\")\n",
    "print(df_user[['username', 'last_post', 'days_since_last_post']].tail(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of users with no posts (days_since_last_post = -1): 3\n",
      "\n",
      "Users without post-level data:\n",
      "682         belediyesikose\n",
      "1720     touchdownistanbul\n",
      "1973    orhanelibelediyesi\n",
      "Name: username, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the number of users with 'days_since_last_post' as -1\n",
    "num_no_posts = (df_user['days_since_last_post'] == -1).sum()\n",
    "print(f\"\\nNumber of users with no posts (days_since_last_post = -1): {num_no_posts}\")\n",
    "\n",
    "# Display the users without post data\n",
    "print(\"\\nUsers without post-level data:\")\n",
    "print(df_user[df_user['days_since_last_post'] == -1]['username'].head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature  Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Features Added to df_user:\n",
      "          username             label          id        full_name  \\\n",
      "0    taskirancemal  Mom and Children  1282703608   Cemal TaÅŸkÄ±ran   \n",
      "1    tam_kararinda              Food  2114951482      Kaan Yarman   \n",
      "2         spart4nn              Food   645610128     Cemil Ceylan   \n",
      "3  sosyalyiyiciler              Food  1671187359                    \n",
      "4  sonaydizdarahad  Mom and Children  1635669992  Sonay Demiryeri   \n",
      "\n",
      "                                           biography    category_name  \\\n",
      "0                                     ðŸ“Antalya / KaÅŸ     Entrepreneur   \n",
      "1     Milliyet Pazar\\nKalori Alacaksan Buna DeÄŸecekðŸ“š  Kitchen/cooking   \n",
      "2  KÃ¼Ã§Ã¼k ev\\nKamp ve doÄŸa hayatÄ±ðŸ•\\nMutfaÄŸÄ±mÄ±z doÄŸ...    Video creator   \n",
      "3  Founder @bitte.izmir \\nIZMIR yemek/seyahat foo...          Unknown   \n",
      "4  DÄ°ZDAR ðŸ§¿ Ahad ðŸ§¿ DaÄŸhan \\n@d.a.d.kids \\n@dilanp...    Personal blog   \n",
      "\n",
      "   post_count  follower_count  following_count is_business_account  ...  \\\n",
      "0      1382.0         12145.0           1064.0               False  ...   \n",
      "1       619.0        783223.0            867.0               False  ...   \n",
      "2      2094.0       1127332.0            496.0               False  ...   \n",
      "3       919.0         27002.0           4279.0                True  ...   \n",
      "4      2387.0        781362.0           1877.0                True  ...   \n",
      "\n",
      "  Ã¼reticisi Ã¼retim  Ã¼rÃ¼n Ã¼rÃ¼nler Ã¼rÃ¼nleri Ã¼yesi   Ä±n  ÅŸey ÅŸimdi ÅŸube  \n",
      "0       0.0    0.0   0.0     0.0      0.0   0.0  0.0  0.0   0.0  0.0  \n",
      "1       0.0    0.0   0.0     0.0      0.0   0.0  0.0  0.0   0.0  0.0  \n",
      "2       0.0    0.0   0.0     0.0      0.0   0.0  0.0  0.0   0.0  0.0  \n",
      "3       0.0    0.0   0.0     0.0      0.0   0.0  0.0  0.0   0.0  0.0  \n",
      "4       0.0    0.0   0.0     0.0      0.0   0.0  0.0  0.0   0.0  0.0  \n",
      "\n",
      "[5 rows x 540 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ensure 'biography' is string and handle missing values\n",
    "df_user['biography'] = df_user['biography'].fillna('').astype(str)\n",
    "\n",
    "# Define a list of Turkish stopwords (ensure it's comprehensive)\n",
    "turkish_stopwords = [\n",
    "    've', 'bir', 'bu', 'da', 'de', 'ile', 'Ã§ok', 'Ã§okÃ§a', 'da', 'ne', 'mi', 'bu', \n",
    "    'ben', 'siz', 'o', 'biz', 'sizler'\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=500,               # Adjust based on your dataset and computational resources\n",
    "    stop_words=turkish_stopwords,   # Use Turkish stopwords\n",
    "    lowercase=True,                 # Convert all text to lowercase\n",
    "    ngram_range=(1, 2)              # Consider unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit and transform the 'biography' column\n",
    "tfidf_bio = tfidf_vectorizer.fit_transform(df_user['biography'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_bio_df = pd.DataFrame(tfidf_bio.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the TF-IDF features with the original DataFrame\n",
    "df_user = pd.concat([df_user.reset_index(drop=True), tfidf_bio_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"\\nTF-IDF Features Added to df_user:\")\n",
    "print(df_user.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical Variables Encoded Using Label Encoding:\n",
      "     category_name  category_encoded\n",
      "0     Entrepreneur               116\n",
      "1  Kitchen/cooking               180\n",
      "2    Video creator               323\n",
      "3          Unknown               319\n",
      "4    Personal blog               229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode 'category_name' if not already encoded\n",
    "df_user['category_encoded'] = label_encoder.fit_transform(df_user['category_name'].astype(str))\n",
    "\n",
    "print(\"\\nCategorical Variables Encoded Using Label Encoding:\")\n",
    "print(df_user[['category_name', 'category_encoded']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'engagement_rate' Feature Added:\n",
      "          username     avg_likes  avg_comments  total_posts  engagement_rate\n",
      "0    taskirancemal    422.971429     12.657143         35.0        12.446531\n",
      "1    tam_kararinda  13460.457143    260.685714         35.0       392.032653\n",
      "2         spart4nn  93142.085714    586.257143         35.0      2677.952653\n",
      "3  sosyalyiyiciler    309.285714     17.971429         35.0         9.350204\n",
      "4  sonaydizdarahad  45827.342857    625.685714         35.0      1327.229388\n"
     ]
    }
   ],
   "source": [
    "# Avoid division by zero by replacing zero total_posts with 1 temporarily\n",
    "df_user['total_posts_nonzero'] = df_user['total_posts'].replace(0, 1)\n",
    "\n",
    "# Calculate engagement_rate\n",
    "df_user['engagement_rate'] = (df_user['avg_likes'] + df_user['avg_comments']) / df_user['total_posts_nonzero']\n",
    "\n",
    "# Drop the temporary column\n",
    "df_user = df_user.drop(columns=['total_posts_nonzero'])\n",
    "\n",
    "# Verify the new column\n",
    "print(\"\\n'engagement_rate' Feature Added:\")\n",
    "print(df_user[['username', 'avg_likes', 'avg_comments', 'total_posts', 'engagement_rate']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical Features Scaled Using StandardScaler:\n",
      "   follower_count  following_count  highlight_reel_count  avg_likes  \\\n",
      "0       -0.050865         0.616689             -0.064921  -0.160447   \n",
      "1        0.075421         0.409158              2.652599   0.216413   \n",
      "2        0.131779         0.018326              7.081150   2.519677   \n",
      "3       -0.048431         4.003552              1.679660  -0.163733   \n",
      "4        0.075116         1.473150              0.371225   1.152005   \n",
      "\n",
      "   avg_comments  total_posts  days_since_last_post  engagement_rate  \n",
      "0     -0.061870     0.134432              0.682042        -0.150072  \n",
      "1      0.077402     0.134432             -0.177230         0.172871  \n",
      "2      0.260215     0.134432             -0.160599         2.117676  \n",
      "3     -0.058886     0.134432             -0.127337        -0.152706  \n",
      "4      0.282355     0.134432             -0.121793         0.968514  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define numerical features to scale\n",
    "numerical_features = [\n",
    "    'follower_count',\n",
    "    'following_count',\n",
    "    'highlight_reel_count',\n",
    "    'avg_likes',\n",
    "    'avg_comments',\n",
    "    'total_posts',\n",
    "    'days_since_last_post',\n",
    "    'engagement_rate'\n",
    "]\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical features\n",
    "df_user[numerical_features] = scaler.fit_transform(df_user[numerical_features])\n",
    "\n",
    "print(\"\\nNumerical Features Scaled Using StandardScaler:\")\n",
    "print(df_user[numerical_features].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN / VALIDATION SPLIT, MODEL SELECTION, & TRAINING (CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'like_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'like_count'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[218], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train-Test Split for Regression\u001b[39;00m\n\u001b[0;32m     21\u001b[0m X_reg \u001b[38;5;241m=\u001b[39m df_user[classification_features]\n\u001b[1;32m---> 22\u001b[0m y_reg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog1p(\u001b[43mdf_user\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlike_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Apply log1p transformation for regression target\u001b[39;00m\n\u001b[0;32m     23\u001b[0m X_train_reg, X_test_reg, y_train_reg, y_test_reg \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     24\u001b[0m     X_reg, y_reg, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Initialize StandardScaler\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'like_count'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Ensure dataset is loaded into df_user (from previous steps)\n",
    "# Define features for classification and regression\n",
    "classification_features = ['follower_count', 'following_count', 'highlight_reel_count', 'avg_likes', \n",
    "                           'avg_comments', 'total_posts', 'days_since_last_post', 'engagement_rate']\n",
    "\n",
    "# Train-Test Split for Classification\n",
    "X_class = df_user[classification_features]\n",
    "y_class = df_user['category_encoded']  # Assuming category_encoded exists\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train-Test Split for Regression\n",
    "X_reg = df_user[classification_features]\n",
    "y_reg = np.log1p(df_user['like_count'])  # Apply log1p transformation for regression target\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale Classification Features\n",
    "X_train_class = scaler.fit_transform(X_train_class)\n",
    "X_test_class = scaler.transform(X_test_class)\n",
    "\n",
    "# Scale Regression Features\n",
    "X_train_reg = scaler.fit_transform(X_train_reg)\n",
    "X_test_reg = scaler.transform(X_test_reg)\n",
    "\n",
    "# Classification Model: Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Predictions and Evaluation (Classification)\n",
    "y_pred_class = clf.predict(X_test_class)\n",
    "classification_accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "print(f\"Classification Accuracy: {classification_accuracy:.4f}\")\n",
    "\n",
    "# Regression Model: Random Forest\n",
    "reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predictions and Evaluation (Regression)\n",
    "y_pred_reg = reg.predict(X_test_reg)\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "print(f\"Regression Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Print Predictions\n",
    "print(\"\\nClassification Predictions (Actual vs Predicted):\")\n",
    "classification_results = pd.DataFrame({\n",
    "    'Actual': y_test_class,\n",
    "    'Predicted': y_pred_class\n",
    "})\n",
    "print(classification_results.head())\n",
    "\n",
    "print(\"\\nRegression Predictions (Actual vs Predicted):\")\n",
    "regression_results = pd.DataFrame({\n",
    "    'Actual': np.expm1(y_test_reg),  # Reverse log transformation\n",
    "    'Predicted': np.expm1(y_pred_reg)\n",
    "})\n",
    "print(regression_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT ON TEST DATA FOR CLASSIFICATION & SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION FOR LIKE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST REGRESSION PREDICTIONS & SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
