{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBOlFYpnlr0N"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "PM5EnXhLk6BL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgAdz_b8xp8w",
        "outputId": "14cd6ea6-4a61-4b22-f2a6-59088a78e58a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\itsmm\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Turkish StopWords\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "turkish_stopwords = stopwords.words('turkish')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plt9-VkNmlmH"
      },
      "source": [
        "# Influencer Category Classification\n",
        "\n",
        "\n",
        "\n",
        "1.   Read Data\n",
        "2.   Preprocess Data\n",
        "3.   Prepare Model\n",
        "4.   Predict Test Data\n",
        "4.   Save outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "8DeBh-b7lrEs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First few rows of the training classification DataFrame:\n",
            "           user_id          category\n",
            "0    taskirancemal  mom and children\n",
            "1    tam_kararinda              food\n",
            "2         spart4nn              food\n",
            "3  sosyalyiyiciler              food\n",
            "4  sonaydizdarahad  mom and children\n",
            "\n",
            "Dataset Statistics:\n",
            "Total unique users: 2742\n",
            "\n",
            "Category distribution:\n",
            "category\n",
            "food                    511\n",
            "health and lifestyle    503\n",
            "tech                    346\n",
            "entertainment           323\n",
            "fashion                 299\n",
            "travel                  294\n",
            "art                     191\n",
            "mom and children        149\n",
            "sports                  113\n",
            "gaming                   13\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically (keep the same)\n",
        "...\n",
        "\n",
        "# Step 2: Load Data Dynamically\n",
        "train_classification_df = pd.read_csv(train_classification_path)\n",
        "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
        "\n",
        "# Check for duplicates\n",
        "duplicate_users = train_classification_df[train_classification_df['user_id'].duplicated(keep=False)]\n",
        "if not duplicate_users.empty:\n",
        "    print(\"\\nWarning: Found duplicate user_ids:\")\n",
        "    print(duplicate_users.sort_values('user_id'))\n",
        "    \n",
        "    # Option 1: Keep first occurrence\n",
        "    train_classification_df = train_classification_df.drop_duplicates(subset='user_id', keep='first')\n",
        "    print(\"\\nKept first occurrence of duplicates.\")\n",
        "    \n",
        "    # Option 2: Could also check if duplicates have different categories\n",
        "    duplicate_categories = duplicate_users.groupby('user_id')['category'].nunique() > 1\n",
        "    if duplicate_categories.any():\n",
        "        print(\"\\nWarning: Some duplicates have different categories:\")\n",
        "        for user_id in duplicate_categories[duplicate_categories].index:\n",
        "            user_entries = duplicate_users[duplicate_users['user_id'] == user_id]\n",
        "            print(f\"\\nUser {user_id} has multiple categories:\")\n",
        "            print(user_entries[['user_id', 'category']])\n",
        "\n",
        "# Step 3: Unify Labels\n",
        "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
        "\n",
        "# Step 4: Create User-to-Category Mapping\n",
        "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\n",
        "\n",
        "# Step 5: Verify Output\n",
        "print(\"\\nFirst few rows of the training classification DataFrame:\")\n",
        "print(train_classification_df.head())\n",
        "\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(f\"Total unique users: {len(username2_category)}\")\n",
        "print(\"\\nCategory distribution:\")\n",
        "print(train_classification_df['category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "SfD7BQ3hE5Jh",
        "outputId": "a67f08dd-ab3e-491f-bfe2-a14da0e961cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>art</th>\n",
              "      <td>209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>entertainment</th>\n",
              "      <td>351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fashion</th>\n",
              "      <td>316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gaming</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>health and lifestyle</th>\n",
              "      <td>547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mom and children</th>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sports</th>\n",
              "      <td>133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tech</th>\n",
              "      <td>371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>travel</th>\n",
              "      <td>316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      user_id\n",
              "category                     \n",
              "art                       209\n",
              "entertainment             351\n",
              "fashion                   316\n",
              "food                      531\n",
              "gaming                     20\n",
              "health and lifestyle      547\n",
              "mom and children          152\n",
              "sports                    133\n",
              "tech                      371\n",
              "travel                    316"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# stats about the labels\n",
        "train_classification_df.groupby(\"category\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mhckiEfD2gg_",
        "outputId": "af451a4d-8825-4506-fb70-41a6ef845aaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mom and children'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "username2_category[\"sonaydizdarahad\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "GT_IcUM2nGBH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Statistics:\n",
            "Number of Training Users: 2741\n",
            "Number of Testing Users: 2674\n",
            "Total unique training posts: 94824\n",
            "Total unique testing posts: 92478\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the training directory\n",
        "training_dir = os.path.join(data_dir, 'training')\n",
        "\n",
        "# File path for 'training-dataset.jsonl.gz'\n",
        "train_data_path = os.path.join(training_dir, 'training-dataset.jsonl.gz')\n",
        "\n",
        "# Step 2: Initialize Dictionaries for Data\n",
        "username2posts_train = dict()\n",
        "username2profile_train = dict()\n",
        "\n",
        "username2posts_test = dict()\n",
        "username2profile_test = dict()\n",
        "# Step 1 and 2 remain the same...\n",
        "\n",
        "# Initialize sets to track post IDs\n",
        "train_post_ids = set()\n",
        "test_post_ids = set()\n",
        "duplicate_posts = []\n",
        "\n",
        "# Step 3: Process Data with duplicate checking\n",
        "with gzip.open(train_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "\n",
        "        profile = sample[\"profile\"]\n",
        "        username = profile.get(\"username\", \"\").strip()\n",
        "        if not username:\n",
        "            continue\n",
        "\n",
        "        # Check posts for duplicates\n",
        "        filtered_posts = []\n",
        "        for post in sample[\"posts\"]:\n",
        "            post_id = post.get(\"id\")\n",
        "            if post_id:\n",
        "                if username in username2_category:\n",
        "                    if post_id in train_post_ids:\n",
        "                        duplicate_posts.append({\n",
        "                            'id': post_id,\n",
        "                            'username': username,\n",
        "                            'timestamp': post.get('timestamp'),\n",
        "                            'dataset': 'train'\n",
        "                        })\n",
        "                        continue\n",
        "                    train_post_ids.add(post_id)\n",
        "                else:\n",
        "                    if post_id in test_post_ids:\n",
        "                        duplicate_posts.append({\n",
        "                            'id': post_id,\n",
        "                            'username': username,\n",
        "                            'timestamp': post.get('timestamp'),\n",
        "                            'dataset': 'test'\n",
        "                        })\n",
        "                        continue\n",
        "                    test_post_ids.add(post_id)\n",
        "            filtered_posts.append(post)\n",
        "\n",
        "        if username in username2_category:\n",
        "            username2posts_train[username] = filtered_posts\n",
        "            username2profile_train[username] = profile\n",
        "        else:\n",
        "            username2posts_test[username] = filtered_posts\n",
        "            username2profile_test[username] = profile\n",
        "\n",
        "# Print statistics\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(f\"Number of Training Users: {len(username2posts_train)}\")\n",
        "print(f\"Number of Testing Users: {len(username2posts_test)}\")\n",
        "print(f\"Total unique training posts: {len(train_post_ids)}\")\n",
        "print(f\"Total unique testing posts: {len(test_post_ids)}\")\n",
        "\n",
        "if duplicate_posts:\n",
        "    print(\"\\nFound duplicate posts:\")\n",
        "    print(f\"Total duplicates: {len(duplicate_posts)}\")\n",
        "    print(\"\\nFirst few duplicates:\")\n",
        "    for dup in duplicate_posts[:5]:\n",
        "        print(f\"Post ID: {dup['id']}\")\n",
        "        print(f\"Username: {dup['username']}\")\n",
        "        print(f\"Timestamp: {dup['timestamp']}\")\n",
        "        print(f\"Dataset: {dup['dataset']}\")\n",
        "        print(\"---\")\n",
        "\n",
        "# Also check for cross-dataset duplicates\n",
        "cross_duplicates = train_post_ids.intersection(test_post_ids)\n",
        "if cross_duplicates:\n",
        "    print(\"\\nWarning: Found posts that appear in both train and test sets!\")\n",
        "    print(f\"Number of cross-dataset duplicates: {len(cross_duplicates)}\")\n",
        "    print(\"First few cross-dataset duplicate IDs:\", list(cross_duplicates)[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "djeF1GQ1oy3v",
        "outputId": "5c2ead24-d7d1-4df8-bec0-bf2152c76832"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>id</th>\n",
              "      <th>full_name</th>\n",
              "      <th>biography</th>\n",
              "      <th>category_name</th>\n",
              "      <th>post_count</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>is_business_account</th>\n",
              "      <th>is_private</th>\n",
              "      <th>...</th>\n",
              "      <th>business_category_name</th>\n",
              "      <th>overall_category_name</th>\n",
              "      <th>category_enum</th>\n",
              "      <th>is_verified_by_mv4b</th>\n",
              "      <th>is_regulated_c18</th>\n",
              "      <th>profile_pic_url</th>\n",
              "      <th>should_show_category</th>\n",
              "      <th>should_show_public_contacts</th>\n",
              "      <th>show_account_transparency_details</th>\n",
              "      <th>profile_picture_base64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>deparmedya</td>\n",
              "      <td>3170700063</td>\n",
              "      <td>Depar Medya</td>\n",
              "      <td>#mediaplanning #mediabuying #sosyalmedya</td>\n",
              "      <td>Local business</td>\n",
              "      <td>None</td>\n",
              "      <td>1167</td>\n",
              "      <td>192</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>LOCAL</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     username          id    full_name  \\\n",
              "0  deparmedya  3170700063  Depar Medya   \n",
              "\n",
              "                                  biography   category_name post_count  \\\n",
              "0  #mediaplanning #mediabuying #sosyalmedya  Local business       None   \n",
              "\n",
              "  follower_count following_count is_business_account is_private  ...  \\\n",
              "0           1167             192                True      False  ...   \n",
              "\n",
              "  business_category_name overall_category_name category_enum  \\\n",
              "0                   None                  None         LOCAL   \n",
              "\n",
              "  is_verified_by_mv4b is_regulated_c18  \\\n",
              "0               False            False   \n",
              "\n",
              "                                     profile_pic_url should_show_category  \\\n",
              "0  https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....                 True   \n",
              "\n",
              "  should_show_public_contacts show_account_transparency_details  \\\n",
              "0                        True                              True   \n",
              "\n",
              "                              profile_picture_base64  \n",
              "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
              "\n",
              "[1 rows x 44 columns]"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Profile Dataframe\n",
        "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
        "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)\n",
        "\n",
        "train_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "15tvVohUBHK9",
        "outputId": "15be2533-51b2-41e8-e0f2-26a44933dbae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>id</th>\n",
              "      <th>full_name</th>\n",
              "      <th>biography</th>\n",
              "      <th>category_name</th>\n",
              "      <th>post_count</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>is_business_account</th>\n",
              "      <th>is_private</th>\n",
              "      <th>...</th>\n",
              "      <th>business_category_name</th>\n",
              "      <th>overall_category_name</th>\n",
              "      <th>category_enum</th>\n",
              "      <th>is_verified_by_mv4b</th>\n",
              "      <th>is_regulated_c18</th>\n",
              "      <th>profile_pic_url</th>\n",
              "      <th>should_show_category</th>\n",
              "      <th>should_show_public_contacts</th>\n",
              "      <th>show_account_transparency_details</th>\n",
              "      <th>profile_picture_base64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>beyazyakaliyiz</td>\n",
              "      <td>8634457436</td>\n",
              "      <td>Selam Beyaz Yakalı</td>\n",
              "      <td>Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀</td>\n",
              "      <td>Personal blog</td>\n",
              "      <td>None</td>\n",
              "      <td>1265</td>\n",
              "      <td>665</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>PERSONAL_BLOG</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>https://instagram.fist6-1.fna.fbcdn.net/v/t51....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         username          id           full_name  \\\n",
              "0  beyazyakaliyiz  8634457436  Selam Beyaz Yakalı   \n",
              "\n",
              "                                     biography  category_name post_count  \\\n",
              "0  Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀  Personal blog       None   \n",
              "\n",
              "  follower_count following_count is_business_account is_private  ...  \\\n",
              "0           1265             665                True      False  ...   \n",
              "\n",
              "  business_category_name overall_category_name  category_enum  \\\n",
              "0                   None                  None  PERSONAL_BLOG   \n",
              "\n",
              "  is_verified_by_mv4b is_regulated_c18  \\\n",
              "0               False            False   \n",
              "\n",
              "                                     profile_pic_url should_show_category  \\\n",
              "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
              "\n",
              "  should_show_public_contacts show_account_transparency_details  \\\n",
              "0                        True                              True   \n",
              "\n",
              "                              profile_picture_base64  \n",
              "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
              "\n",
              "[1 rows x 44 columns]"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_classification_df:\n",
            "['user_id', 'category']\n",
            "\n",
            "First few rows of the training classification DataFrame:\n",
            "           user_id          category\n",
            "0    taskirancemal  mom and children\n",
            "1    tam_kararinda              food\n",
            "2         spart4nn              food\n",
            "3  sosyalyiyiciler              food\n",
            "4  sonaydizdarahad  mom and children\n",
            "\n",
            "Label distribution:\n",
            "category\n",
            "health and lifestyle    547\n",
            "food                    531\n",
            "tech                    371\n",
            "entertainment           351\n",
            "travel                  316\n",
            "fashion                 316\n",
            "art                     209\n",
            "mom and children        152\n",
            "sports                  133\n",
            "gaming                   20\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Verify Output\n",
        "print(\"Columns in train_classification_df:\")\n",
        "print(train_classification_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of the training classification DataFrame:\")\n",
        "print(train_classification_df.head())\n",
        "\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(train_classification_df['category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_profile_df:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
            "\n",
            "First few rows of train_profile_df:\n",
            "     username          id    full_name  \\\n",
            "0  deparmedya  3170700063  Depar Medya   \n",
            "\n",
            "                                  biography   category_name post_count  \\\n",
            "0  #mediaplanning #mediabuying #sosyalmedya  Local business       None   \n",
            "\n",
            "  follower_count following_count is_business_account is_private  ...  \\\n",
            "0           1167             192                True      False  ...   \n",
            "\n",
            "  business_category_name overall_category_name category_enum  \\\n",
            "0                   None                  None         LOCAL   \n",
            "\n",
            "  is_verified_by_mv4b is_regulated_c18  \\\n",
            "0               False            False   \n",
            "\n",
            "                                     profile_pic_url should_show_category  \\\n",
            "0  https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....                 True   \n",
            "\n",
            "  should_show_public_contacts show_account_transparency_details  \\\n",
            "0                        True                              True   \n",
            "\n",
            "                              profile_picture_base64  \n",
            "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
            "\n",
            "[1 rows x 44 columns]\n",
            "\n",
            "Columns in test_profile_df:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
            "\n",
            "First few rows of test_profile_df:\n",
            "         username          id           full_name  \\\n",
            "0  beyazyakaliyiz  8634457436  Selam Beyaz Yakalı   \n",
            "\n",
            "                                     biography  category_name post_count  \\\n",
            "0  Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀  Personal blog       None   \n",
            "\n",
            "  follower_count following_count is_business_account is_private  ...  \\\n",
            "0           1265             665                True      False  ...   \n",
            "\n",
            "  business_category_name overall_category_name  category_enum  \\\n",
            "0                   None                  None  PERSONAL_BLOG   \n",
            "\n",
            "  is_verified_by_mv4b is_regulated_c18  \\\n",
            "0               False            False   \n",
            "\n",
            "                                     profile_pic_url should_show_category  \\\n",
            "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
            "\n",
            "  should_show_public_contacts show_account_transparency_details  \\\n",
            "0                        True                              True   \n",
            "\n",
            "                              profile_picture_base64  \n",
            "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
            "\n",
            "[1 rows x 44 columns]\n"
          ]
        }
      ],
      "source": [
        "# Profile Dataframe\n",
        "print(\"Columns in train_profile_df:\")\n",
        "print(train_profile_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of train_profile_df:\")\n",
        "print(train_profile_df.head(1))\n",
        "\n",
        "print(\"\\nColumns in test_profile_df:\")\n",
        "print(test_profile_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of test_profile_df:\")\n",
        "print(test_profile_df.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_profile_df after dropping:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'bio_links', 'is_professional_account', 'business_category_name', 'overall_category_name']\n",
            "\n",
            "Columns in test_profile_df after dropping:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'bio_links', 'is_professional_account', 'business_category_name', 'overall_category_name']\n"
          ]
        }
      ],
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = [\n",
        "    'highlight_reel_count', 'entities', 'ai_agent_type', 'fb_profile_biolink',\n",
        "    'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid',\n",
        "    'has_clips', 'hide_like_and_view_counts', 'is_supervision_enabled',\n",
        "    'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user',\n",
        "    'is_embeds_disabled', 'is_joined_recently', 'business_address_json',\n",
        "    'business_contact_method', 'business_email', 'business_phone_number',\n",
        "    'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18',\n",
        "    'profile_pic_url', 'should_show_category', 'should_show_public_contacts',\n",
        "    'show_account_transparency_details', 'profile_picture_base64'\n",
        "]\n",
        "\n",
        "# Dropping specified columns from train_profile_df\n",
        "train_profile_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Dropping specified columns from test_profile_df\n",
        "test_profile_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Verify columns in train_profile_df after dropping\n",
        "print(\"Columns in train_profile_df after dropping:\")\n",
        "print(train_profile_df.columns.tolist())\n",
        "\n",
        "# Verify columns in test_profile_df after dropping\n",
        "print(\"\\nColumns in test_profile_df after dropping:\")\n",
        "print(test_profile_df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Z5UY0eYLsoTr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "Best Params: {'clf__C': 10, 'preprocessor__bio_tfidf__ngram_range': (1, 1), 'preprocessor__captions_tfidf__ngram_range': (1, 1)}\n",
            "Best CV Accuracy: 0.6452599388379205\n",
            "Training Accuracy: 1.0\n",
            "\n",
            "Training Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                 art       1.00      1.00      1.00       158\n",
            "       entertainment       1.00      1.00      1.00       274\n",
            "             fashion       1.00      1.00      1.00       251\n",
            "                food       1.00      1.00      1.00       417\n",
            "              gaming       1.00      1.00      1.00        12\n",
            "health and lifestyle       1.00      1.00      1.00       422\n",
            "    mom and children       1.00      1.00      1.00       120\n",
            "              sports       1.00      1.00      1.00        98\n",
            "                tech       1.00      1.00      1.00       290\n",
            "              travel       1.00      1.00      1.00       247\n",
            "\n",
            "            accuracy                           1.00      2289\n",
            "           macro avg       1.00      1.00      1.00      2289\n",
            "        weighted avg       1.00      1.00      1.00      2289\n",
            "\n",
            "Validation Accuracy: 0.6527050610820244\n",
            "\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                 art       0.36      0.30      0.33        40\n",
            "       entertainment       0.48      0.46      0.47        69\n",
            "             fashion       0.65      0.67      0.66        63\n",
            "                food       0.86      0.85      0.85       104\n",
            "              gaming       0.00      0.00      0.00         3\n",
            "health and lifestyle       0.62      0.71      0.66       105\n",
            "    mom and children       0.59      0.43      0.50        30\n",
            "              sports       0.93      0.58      0.72        24\n",
            "                tech       0.64      0.77      0.70        73\n",
            "              travel       0.69      0.68      0.68        62\n",
            "\n",
            "            accuracy                           0.65       573\n",
            "           macro avg       0.58      0.55      0.56       573\n",
            "        weighted avg       0.65      0.65      0.65       573\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Preprocessing Function\n",
        "import re\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    text = text.casefold()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Build Corpus and Labels\n",
        "corpus = []\n",
        "train_usernames = []\n",
        "\n",
        "for username, posts in username2posts_train.items():\n",
        "    train_usernames.append(username)\n",
        "    cleaned_captions = []\n",
        "    for post in posts:\n",
        "        post_caption = post.get(\"caption\", \"\")\n",
        "        if post_caption is None:\n",
        "            continue\n",
        "        post_caption = preprocess_text(post_caption)\n",
        "        if post_caption != \"\":\n",
        "            cleaned_captions.append(post_caption)\n",
        "    user_post_captions = \"\\n\".join(cleaned_captions)\n",
        "    corpus.append(user_post_captions)\n",
        "\n",
        "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
        "\n",
        "# Incorporate Metadata\n",
        "records = []\n",
        "for idx, username in enumerate(train_usernames):\n",
        "    profile = username2profile_train.get(username, {})\n",
        "    biography_text = str(profile.get(\"biography\", \"\") or \"\")\n",
        "    follower_count = profile.get(\"follower_count\", 0)\n",
        "    following_count = profile.get(\"following_count\", 0)\n",
        "    post_count = profile.get(\"post_count\", 0) if profile.get(\"post_count\") else 0\n",
        "    row_dict = {\n",
        "        \"username\": username,\n",
        "        \"captions\": corpus[idx],\n",
        "        \"biography\": biography_text,\n",
        "        \"follower_count\": follower_count,\n",
        "        \"following_count\": following_count,\n",
        "        \"post_count\": post_count,\n",
        "        \"label\": y_train[idx]\n",
        "    }\n",
        "    records.append(row_dict)\n",
        "\n",
        "train_full_df = pd.DataFrame(records)\n",
        "\n",
        "# Define Pipeline Components\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Train-Validation Split\n",
        "X = train_full_df.drop(columns=[\"label\"])\n",
        "y = train_full_df[\"label\"]\n",
        "\n",
        "x_train_df, x_val_df, y_train_labels, y_val_labels = train_test_split(\n",
        "    X, \n",
        "    y, \n",
        "    test_size=0.2, \n",
        "    stratify=y, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define ColumnTransformer\n",
        "numeric_features = [\"follower_count\", \"following_count\", \"post_count\"]\n",
        "text_features_caps = \"captions\"\n",
        "text_features_bio = \"biography\"\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"captions_tfidf\", TfidfVectorizer(\n",
        "             stop_words=turkish_stopwords, \n",
        "             max_features=5000\n",
        "         ), text_features_caps),\n",
        "        (\"bio_tfidf\", TfidfVectorizer(\n",
        "             stop_words=turkish_stopwords, \n",
        "             max_features=5000\n",
        "         ), text_features_bio),\n",
        "        (\"numeric_scaler\", MinMaxScaler(), numeric_features)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# Build ImbPipeline (SMOTE + Classifier)\n",
        "pipeline = ImbPipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=42, sampling_strategy=\"auto\")),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        class_weight='balanced',\n",
        "        solver='liblinear',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Define a param grid\n",
        "param_grid = {\n",
        "    \"preprocessor__captions_tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"preprocessor__bio_tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"clf__C\": [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Initialize and fit GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(x_train_df, y_train_labels)\n",
        "\n",
        "print(\"Best Params:\", grid_search.best_params_)\n",
        "print(\"Best CV Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "\n",
        "# **New Section: Evaluate on Training Data**\n",
        "# Predict on the training data\n",
        "y_train_pred = best_pipeline.predict(x_train_df)\n",
        "\n",
        "# Calculate training accuracy\n",
        "train_acc = accuracy_score(y_train_labels, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "\n",
        "# Generate and print the training classification report\n",
        "print(\"\\nTraining Classification Report:\\n\",\n",
        "      classification_report(y_train_labels, y_train_pred, zero_division=0))\n",
        "\n",
        "# **End of New Section**\n",
        "\n",
        "# Predict on the validation data\n",
        "y_val_pred = best_pipeline.predict(x_val_df)\n",
        "\n",
        "# Calculate validation accuracy\n",
        "val_acc = accuracy_score(y_val_labels, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_acc)\n",
        "\n",
        "# Generate and print the validation classification report\n",
        "print(\"\\nClassification Report:\\n\",\n",
        "      classification_report(y_val_labels, y_val_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2289 entries, 1238 to 1441\n",
            "Data columns (total 6 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   username         2289 non-null   object \n",
            " 1   captions         2289 non-null   object \n",
            " 2   biography        2289 non-null   object \n",
            " 3   follower_count   2289 non-null   int64  \n",
            " 4   following_count  2289 non-null   int64  \n",
            " 5   post_count       2289 non-null   float64\n",
            "dtypes: float64(1), int64(2), object(3)\n",
            "memory usage: 125.2+ KB\n",
            "None\n",
            "\n",
            "Missing values in training data:\n",
            "username           0\n",
            "captions           0\n",
            "biography          0\n",
            "follower_count     0\n",
            "following_count    0\n",
            "post_count         0\n",
            "dtype: int64\n",
            "\n",
            "Performing cross-validation with tqdm progress bar...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cross-Validation: 100%|██████████| 5/5 [30:12<00:00, 362.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cross-validation scores: [0.6637554585152838, 0.6637554585152838, 0.6200873362445415, 0.6375545851528385, 0.6323851203501094]\n",
            "Mean CV accuracy: 0.644 (+/- 0.035)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[94], line 224\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean CV accuracy: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m (+/- \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    220\u001b[0m     np\u001b[38;5;241m.\u001b[39mmean(scores), np\u001b[38;5;241m.\u001b[39mstd(scores) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    221\u001b[0m ))\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# 5) Train final model on the entire training set\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m \u001b[43menhanced_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_labels_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# 6) Evaluate on Training Data\u001b[39;00m\n\u001b[0;32m    227\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m enhanced_pipeline\u001b[38;5;241m.\u001b[39mpredict(x_train_df)\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\pipeline.py:526\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    521\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    522\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    523\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    524\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    525\u001b[0m         )\n\u001b[1;32m--> 526\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:349\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    347\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:81\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m     )\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVoting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:36\u001b[0m, in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m---> 36\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:532\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:610\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    603\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    604\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    605\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    606\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    607\u001b[0m         )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 610\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 245\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    250\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    258\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ------------------- Imports -------------------\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# scikit-learn utilities\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# For progress bar in cross-validation\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------- Enhanced Text Preprocessing -------------------\n",
        "def advanced_text_preprocessing(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    text = text.lower()\n",
        "    \n",
        "    # Extract hashtags and mentions\n",
        "    hashtags = re.findall(r'#\\w+', text)\n",
        "    mentions = re.findall(r'@\\w+', text)\n",
        "    \n",
        "    # Densities\n",
        "    text_length = len(text)\n",
        "    hashtag_density = len(hashtags) / (text_length + 1)\n",
        "    mention_density = len(mentions) / (text_length + 1)\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Add density info as extra pseudo-tokens\n",
        "    density_info = f\" hashtag_density_{hashtag_density:.2f} mention_density_{mention_density:.2f}\"\n",
        "    \n",
        "    # Re-append the extracted hashtags and mentions\n",
        "    return f\"{text} {' '.join(hashtags)} {' '.join(mentions)} {density_info}\".strip()\n",
        "\n",
        "# ------------------- Enhanced Numeric Feature Extraction -------------------\n",
        "def extract_numeric_features(X):\n",
        "    # If X is a DataFrame, ensure columns exist\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        cols = ['follower_count', 'following_count', 'post_count']\n",
        "        if 'account_age_days' in X.columns:\n",
        "            cols.append('account_age_days')\n",
        "        \n",
        "        df = X[cols].copy()\n",
        "    else:\n",
        "        # If it's a NumPy array\n",
        "        df = pd.DataFrame(\n",
        "            X, \n",
        "            columns=['follower_count', 'following_count', 'post_count', 'account_age_days'][:X.shape[1]]\n",
        "        )\n",
        "    \n",
        "    df = df.fillna(0)\n",
        "    \n",
        "    df['following_count_safe'] = df['following_count'].replace(0, 1)\n",
        "    df['follower_count_safe']  = df['follower_count'].replace(0, 1)\n",
        "    \n",
        "    df['follower_ratio']   = df['follower_count'] / df['following_count_safe']\n",
        "    df['post_density']     = df['post_count'] / df['follower_count_safe']\n",
        "    df['engagement_score'] = np.log1p(df['follower_count']) * np.log1p(df['post_count'])\n",
        "    \n",
        "    if 'account_age_days' in df.columns:\n",
        "        df['activity_ratio'] = df['post_count'] / (df['account_age_days'] + 1)\n",
        "    else:\n",
        "        df['activity_ratio'] = df['post_count']\n",
        "    \n",
        "    df['follower_growth_rate'] = df['follower_count'] / (df['post_count'] + 1)\n",
        "    df['relative_engagement']  = (df['follower_count'] * df['post_count']) / (df['following_count_safe'])\n",
        "    \n",
        "    # Log transforms\n",
        "    df['log_followers']  = np.log1p(df['follower_count'])\n",
        "    df['log_following']  = np.log1p(df['following_count'])\n",
        "    df['log_posts']      = np.log1p(df['post_count'])\n",
        "    \n",
        "    df.drop(['following_count_safe','follower_count_safe'], axis=1, inplace=True)\n",
        "    \n",
        "    return df.values\n",
        "\n",
        "# ------------------- Build Pipeline -------------------\n",
        "def build_enhanced_pipeline(stopwords_list):\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('captions_tfidf', \n",
        "             TfidfVectorizer(\n",
        "                 stop_words=stopwords_list,\n",
        "                 max_features=3000,   \n",
        "                 ngram_range=(1, 1),  \n",
        "                 min_df=2,\n",
        "                 max_df=0.95\n",
        "             ), \n",
        "             'captions_clean'),\n",
        "            \n",
        "            ('bio_tfidf', \n",
        "             TfidfVectorizer(\n",
        "                 stop_words=stopwords_list,\n",
        "                 max_features=2000,   \n",
        "                 ngram_range=(1, 1),  \n",
        "                 min_df=2,\n",
        "                 max_df=0.95\n",
        "             ), \n",
        "             'biography_clean'),\n",
        "            \n",
        "            ('numeric', \n",
        "             Pipeline([\n",
        "                 ('feat_eng', FunctionTransformer(extract_numeric_features)),\n",
        "                 ('scaler', MinMaxScaler())\n",
        "             ]), \n",
        "             ['follower_count', 'following_count', 'post_count']\n",
        "             # add 'account_age_days' if available\n",
        "            )\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    \n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=100,    \n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        max_features='sqrt',\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=50,     \n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    lr = LogisticRegression(\n",
        "        C=2.0,\n",
        "        class_weight='balanced',\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', rf),\n",
        "            ('gb', gb),\n",
        "            ('lr', lr)\n",
        "        ],\n",
        "        voting='soft'\n",
        "    )\n",
        "    \n",
        "    pipeline = ImbPipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('classifier', ensemble)\n",
        "    ])\n",
        "    \n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# ------------------- Example Usage -------------------\n",
        "print(\"Training data info:\")\n",
        "print(x_train_df.info())\n",
        "print(\"\\nMissing values in training data:\")\n",
        "print(x_train_df.isnull().sum())\n",
        "\n",
        "# 1) Preprocess text once\n",
        "x_train_df['captions_clean'] = x_train_df['captions'].apply(advanced_text_preprocessing)\n",
        "x_train_df['biography_clean'] = x_train_df['biography'].apply(advanced_text_preprocessing)\n",
        "\n",
        "x_val_df['captions_clean'] = x_val_df['captions'].apply(advanced_text_preprocessing)\n",
        "x_val_df['biography_clean'] = x_val_df['biography'].apply(advanced_text_preprocessing)\n",
        "\n",
        "# 2) Convert y_train_labels to NumPy for direct integer indexing\n",
        "y_train_labels_array = y_train_labels.values\n",
        "\n",
        "# 3) Build pipeline\n",
        "enhanced_pipeline = build_enhanced_pipeline(stopwords_list=turkish_stopwords)\n",
        "\n",
        "# 4) Manual Cross-Validation with tqdm progress bar\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "\n",
        "print(\"\\nPerforming cross-validation with tqdm progress bar...\")\n",
        "for fold_idx, (train_idx, test_idx) in enumerate(\n",
        "    tqdm(cv.split(x_train_df, y_train_labels_array), \n",
        "         total=cv.get_n_splits(), \n",
        "         desc='Cross-Validation')\n",
        "):\n",
        "    # Split the data\n",
        "    X_fold_train = x_train_df.iloc[train_idx]\n",
        "    y_fold_train = y_train_labels_array[train_idx]\n",
        "    X_fold_test  = x_train_df.iloc[test_idx]\n",
        "    y_fold_test  = y_train_labels_array[test_idx]\n",
        "    \n",
        "    # Fit on this fold\n",
        "    enhanced_pipeline.fit(X_fold_train, y_fold_train)\n",
        "    \n",
        "    # Predict\n",
        "    preds = enhanced_pipeline.predict(X_fold_test)\n",
        "    \n",
        "    # Compute accuracy\n",
        "    acc = (preds == y_fold_test).mean()\n",
        "    scores.append(acc)\n",
        "\n",
        "print(\"\\nCross-validation scores:\", scores)\n",
        "print(\"Mean CV accuracy: {:.3f} (+/- {:.3f})\".format(\n",
        "    np.mean(scores), np.std(scores) * 2\n",
        "))\n",
        "\n",
        "# 5) Train final model on the entire training set\n",
        "enhanced_pipeline.fit(x_train_df, y_train_labels_array)\n",
        "\n",
        "# 6) Evaluate on Training Data\n",
        "y_train_pred = enhanced_pipeline.predict(x_train_df)\n",
        "print(\"\\nTraining Classification Report:\")\n",
        "print(classification_report(y_train_labels_array, y_train_pred))\n",
        "\n",
        "# 7) Evaluate on Validation Data\n",
        "#    => Implement \"if username in training\" logic here.\n",
        "\n",
        "# First, build a dictionary from training usernames to labels\n",
        "# We rely on the fact that x_train_df still has \"username\" column\n",
        "train_user2label = dict(zip(x_train_df[\"username\"], y_train_labels_array))\n",
        "\n",
        "# Now we define a function that uses the dictionary first:\n",
        "def predict_with_username_lookup(df, pipeline):\n",
        "    \"\"\"\n",
        "    If username is in training (train_user2label), \n",
        "    use that known label.\n",
        "    Otherwise, call pipeline.predict.\n",
        "    \"\"\"\n",
        "    # Identify known vs. unknown usernames\n",
        "    known_mask = df[\"username\"].isin(train_user2label)\n",
        "    \n",
        "    # For the unknown subset, run the pipeline\n",
        "    df_unknown = df[~known_mask].copy()\n",
        "    \n",
        "    # We must drop the columns that are not used by the pipeline \n",
        "    # (e.g., \"username\", \"captions_clean\", \"biography_clean\" are used internally \n",
        "    # by pipeline, so let's keep them. Actually the pipeline transforms \"captions_clean\" and \"biography_clean\".)\n",
        "    # The pipeline expects columns [follower_count, following_count, post_count, \n",
        "    #  captions_clean, biography_clean].\n",
        "    # So let's do NOT drop them. We only drop \"username\" if needed. The pipeline does remainder='drop' anyway.\n",
        "    \n",
        "    X_unknown = df_unknown.drop(columns=[\"username\"])  # pipeline doesn't need \"username\"\n",
        "    \n",
        "    # Predict\n",
        "    y_pred_unknown = pipeline.predict(X_unknown)\n",
        "    \n",
        "    # Rebuild final predictions in the original df order\n",
        "    y_pred_final = []\n",
        "    j = 0  # index for unknown predictions\n",
        "    \n",
        "    for idx in df.index:\n",
        "        if known_mask.loc[idx]:\n",
        "            # known => use label from training\n",
        "            user = df.loc[idx, \"username\"]\n",
        "            y_pred_final.append(train_user2label[user])\n",
        "        else:\n",
        "            # unknown => use pipeline result\n",
        "            y_pred_final.append(y_pred_unknown[j])\n",
        "            j += 1\n",
        "    \n",
        "    return np.array(y_pred_final)\n",
        "\n",
        "# Make predictions on the validation data, using the check\n",
        "# Ensure x_val_df has 'username' so we can do the lookup\n",
        "y_val_pred_custom = predict_with_username_lookup(x_val_df, enhanced_pipeline)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nValidation Classification Report (with username lookup):\")\n",
        "print(classification_report(y_val_labels, y_val_pred_custom))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading test data...\n",
            "Preparing features for 999 test users...\n",
            "Preprocessing test data...\n",
            "Making predictions on test data...\n",
            "Predictions saved to: c:\\Users\\itsmm\\OneDrive\\Desktop\\CS412\\CS412-InstagramInfluencersAnalysis\\data\\output\\prediction-classification-round1.json\n"
          ]
        }
      ],
      "source": [
        "def load_test_data():\n",
        "    \"\"\"Load and prepare test data\"\"\"\n",
        "    # Define file paths\n",
        "    current_notebook_dir = os.getcwd()\n",
        "    repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "    data_dir = os.path.join(repo_dir, 'data')\n",
        "    testing_dir = os.path.join(data_dir, 'testing')\n",
        "    test_data_path = os.path.join(testing_dir, 'test-classification-round1.dat')\n",
        "    \n",
        "    # Read test usernames\n",
        "    test_unames = []\n",
        "    with open(test_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "        for line in fh:\n",
        "            username = line.strip()\n",
        "            if username != \"screenname\":  # Skip screenname\n",
        "                test_unames.append(username)\n",
        "    \n",
        "    return test_unames\n",
        "\n",
        "def prepare_test_features(test_unames, username2posts_test, username2profile_test):\n",
        "    \"\"\"Prepare features for test data\"\"\"\n",
        "    test_data = []\n",
        "    for username in test_unames:\n",
        "        # Get posts and profile data\n",
        "        posts = username2posts_test.get(username, [])\n",
        "        profile = username2profile_test.get(username, {})\n",
        "        \n",
        "        # Prepare text data\n",
        "        captions = \" \".join([post.get('caption', '') for post in posts if post.get('caption')])\n",
        "        biography = profile.get('biography', '')\n",
        "        \n",
        "        # Prepare numeric data\n",
        "        row_data = {\n",
        "            'username': username,\n",
        "            'captions': captions,\n",
        "            'biography': biography,\n",
        "            'follower_count': profile.get('follower_count', 0),\n",
        "            'following_count': profile.get('following_count', 0),\n",
        "            'post_count': profile.get('post_count', 0)\n",
        "        }\n",
        "        test_data.append(row_data)\n",
        "    \n",
        "    return pd.DataFrame(test_data)\n",
        "\n",
        "# Add this category mapping dictionary at the top level\n",
        "CATEGORY_MAPPING = {\n",
        "    'art': 'Art',\n",
        "    'entertainment': 'Entertainment',\n",
        "    'fashion': 'Fashion',\n",
        "    'food': 'Food',\n",
        "    'gaming': 'Gaming',\n",
        "    'health and lifestyle': 'Health and Lifestyle',\n",
        "    'mom and children': 'Mom and Children',\n",
        "    'sports': 'Sports',\n",
        "    'tech': 'Tech',\n",
        "    'travel': 'Travel'\n",
        "}\n",
        "\n",
        "def save_predictions(predictions, usernames, output_dir):\n",
        "    \"\"\"Save predictions to JSON file with proper capitalization\"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Create output dictionary with proper capitalization\n",
        "    output = {}\n",
        "    for username, prediction in zip(usernames, predictions):\n",
        "        # Map the lowercase prediction to its proper capitalized form\n",
        "        capitalized_prediction = CATEGORY_MAPPING.get(prediction.lower(), prediction)\n",
        "        output[username] = capitalized_prediction\n",
        "    \n",
        "    # Save to file\n",
        "    output_path = os.path.join(output_dir, 'prediction-classification-round1.json')\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(output, f, indent=4)\n",
        "    \n",
        "    print(f\"Predictions saved to: {output_path}\")\n",
        "\n",
        "def main():\n",
        "    \n",
        "    # Load and prepare test data\n",
        "    print(\"\\nLoading test data...\")\n",
        "    test_unames = load_test_data()\n",
        "    \n",
        "    print(f\"Preparing features for {len(test_unames)} test users...\")\n",
        "    x_test_df = prepare_test_features(test_unames, username2posts_test, username2profile_test)\n",
        "    \n",
        "    # Preprocess test text data\n",
        "    print(\"Preprocessing test data...\")\n",
        "    x_test_df['captions_clean'] = x_test_df['captions'].apply(advanced_text_preprocessing)\n",
        "    x_test_df['biography_clean'] = x_test_df['biography'].apply(advanced_text_preprocessing)\n",
        "    \n",
        "    # Make predictions on test data\n",
        "    print(\"Making predictions on test data...\")\n",
        "    test_predictions = predict_with_username_lookup(x_test_df, enhanced_pipeline)\n",
        "    \n",
        "    # Save predictions\n",
        "    output_dir = os.path.join(os.path.dirname(os.getcwd()), 'data', 'output')\n",
        "    save_predictions(test_predictions.tolist(), test_unames, output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khu0eryhNZNN"
      },
      "source": [
        "# Naive Base Classifier\n",
        "\n",
        "### Now we can pass the numerical values to a classifier, Let's try Naive Base!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC7KXsQZL7Kp"
      },
      "source": [
        "# Like Count Prediction\n",
        "\n",
        "\n",
        "Here, we use the average like_count of the user's previous posts to predict each post's like_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "nolpagasSuBq"
      },
      "outputs": [],
      "source": [
        "def predict_like_count(username, current_post=None):\n",
        "  def get_avg_like_count(posts:list):\n",
        "    total = 0.\n",
        "    for post in posts:\n",
        "      if current_post is not None and post[\"id\"] == current_post[\"id\"]:\n",
        "        continue\n",
        "\n",
        "      like_count = post.get(\"like_count\", 0)\n",
        "      if like_count is None:\n",
        "        like_count = 0\n",
        "      total += like_count\n",
        "\n",
        "    if len(posts) == 0:\n",
        "      return 0.\n",
        "\n",
        "    return total / len(posts)\n",
        "\n",
        "  if username in username2posts_train:\n",
        "    return get_avg_like_count(username2posts_train[username])\n",
        "  elif username in username2posts_test:\n",
        "    return get_avg_like_count(username2posts_test[username])\n",
        "  else:\n",
        "    print(f\"No data available for {username}\")\n",
        "    return -1\n",
        "  \n",
        "def log_mse_like_counts(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculate the Log Mean Squared Error (Log MSE) for like counts (log(like_count + 1)).\n",
        "\n",
        "  Parameters:\n",
        "  - y_true: array-like, actual like counts\n",
        "  - y_pred: array-like, predicted like counts\n",
        "\n",
        "  Returns:\n",
        "  - log_mse: float, Log Mean Squared Error\n",
        "  \"\"\"\n",
        "  # Ensure inputs are numpy arrays\n",
        "  y_true = np.array(y_true)\n",
        "  y_pred = np.array(y_pred)\n",
        "\n",
        "  # Log transformation: log(like_count + 1)\n",
        "  log_y_true = np.log1p(y_true)\n",
        "  log_y_pred = np.log1p(y_pred)\n",
        "\n",
        "  # Compute squared errors\n",
        "  squared_errors = (log_y_true - log_y_pred) ** 2\n",
        "\n",
        "  # Return the mean of squared errors\n",
        "  return np.mean(squared_errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating predictions: 100%|██████████| 2741/2741 [00:01<00:00, 1848.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log MSE Train= 1.1623716815622944\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def enhanced_predict_like_count(username, current_post=None):\n",
        "    \"\"\"Enhanced like count prediction using average likes and time-based adjustments\"\"\"\n",
        "    \n",
        "    # Get base prediction from average method\n",
        "    base_prediction = predict_like_count(username, current_post)\n",
        "    \n",
        "    # If no current post, return base prediction\n",
        "    if not current_post:\n",
        "        return base_prediction\n",
        "    \n",
        "    # Safe get function for numeric values\n",
        "    def safe_get(dict_obj, key, default=0):\n",
        "        value = dict_obj.get(key, default)\n",
        "        return default if value is None else value\n",
        "    \n",
        "    # Extract timestamp if available\n",
        "    adjustment = 1.0\n",
        "\n",
        "    media_type = safe_get(current_post, 'media_type', 'PHOTO')\n",
        "    try:\n",
        "        timestamp = safe_get(current_post, 'timestamp', '00:00:00')\n",
        "        hour = int(timestamp.split()[1].split(':')[0])\n",
        "        \n",
        "        # Time of day adjustment\n",
        "        prime_time_hours = {19, 20, 21, 22}  # Evening hours\n",
        "        if hour in prime_time_hours:\n",
        "            adjustment *= 1.1\n",
        "        elif 9 <= hour <= 17:  # Work hours\n",
        "            adjustment *= 0.9\n",
        "        elif 0 <= hour <= 5:   # Late night\n",
        "            adjustment *= 0.8\n",
        "        # Get user's recent posts (last 5)\n",
        "        recent_posts = []\n",
        "        if username in username2posts_train:\n",
        "            posts = username2posts_train[username]\n",
        "            recent_posts = sorted(posts, key=lambda x: x.get('timestamp', ''), reverse=True)[:5]\n",
        "        \n",
        "        if recent_posts:\n",
        "            recent_likes = [p.get('like_count', 0) for p in recent_posts if p.get('like_count') is not None]\n",
        "            if recent_likes:\n",
        "                recent_avg = sum(recent_likes) / len(recent_likes)\n",
        "                overall_avg = base_prediction\n",
        "                \n",
        "                # If recent performance is different from overall\n",
        "                if recent_avg > overall_avg * 1.2:  # Recent posts doing better\n",
        "                    adjustment *= 1.1\n",
        "                elif recent_avg < overall_avg * 0.8:  # Recent posts doing worse\n",
        "                    adjustment *= 0.9\n",
        "        \n",
        "        if username in username2posts_train:\n",
        "            posts = username2posts_train[username]\n",
        "            media_type_likes = {}\n",
        "            \n",
        "            for post in posts:\n",
        "                post_type = post.get('media_type', 'PHOTO')\n",
        "                if post.get('like_count') is not None:\n",
        "                    if post_type not in media_type_likes:\n",
        "                        media_type_likes[post_type] = []\n",
        "                    media_type_likes[post_type].append(post['like_count'])\n",
        "            \n",
        "            # If this media type historically performs better/worse for this user\n",
        "            if media_type in media_type_likes and len(media_type_likes[media_type]) > 0:\n",
        "                type_avg = sum(media_type_likes[media_type]) / len(media_type_likes[media_type])\n",
        "                all_likes = [l for likes in media_type_likes.values() for l in likes]\n",
        "                overall_avg = sum(all_likes) / len(all_likes)\n",
        "                \n",
        "                if type_avg > overall_avg:\n",
        "                    adjustment *= 1.1\n",
        "                elif type_avg < overall_avg:\n",
        "                    adjustment *= 0.9\n",
        "    except:\n",
        "        pass  # If timestamp parsing fails, keep original adjustment\n",
        "    \n",
        "    # Apply adjustment to base prediction\n",
        "    final_prediction = base_prediction * adjustment\n",
        "    \n",
        "    return int(final_prediction)\n",
        "\n",
        "# Modified evaluation code\n",
        "y_like_count_train_true = []\n",
        "y_like_count_train_pred = []\n",
        "\n",
        "for uname, posts in tqdm(username2posts_train.items(), desc=\"Evaluating predictions\"):\n",
        "    for post in posts:\n",
        "        pred_val = enhanced_predict_like_count(uname, post)\n",
        "        true_val = post.get(\"like_count\", 0)\n",
        "        if true_val is None:\n",
        "            true_val = 0\n",
        "            \n",
        "        y_like_count_train_true.append(true_val)\n",
        "        y_like_count_train_pred.append(pred_val)\n",
        "\n",
        "print(f\"Log MSE Train= {log_mse_like_counts(y_like_count_train_true, y_like_count_train_pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating validation predictions: 100%|██████████| 2674/2674 [00:00<00:00, 7082.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log MSE Validation= 0.7795603524418687\n"
          ]
        }
      ],
      "source": [
        "# Initialize lists to store true and predicted like counts for validation\n",
        "y_like_count_val_true = []\n",
        "y_like_count_val_pred = []\n",
        "\n",
        "# Iterate over each user and their posts in the validation dataset\n",
        "for uname, posts in tqdm(username2posts_test.items(), desc=\"Evaluating validation predictions\"):\n",
        "    for post in posts:\n",
        "        # Make prediction using the enhanced_predict_like_count function\n",
        "        pred_val = enhanced_predict_like_count(uname, post)\n",
        "        \n",
        "        # Extract the true like count, defaulting to 0 if not available\n",
        "        true_val = post.get(\"like_count\", 0)\n",
        "        if true_val is None:\n",
        "            true_val = 0\n",
        "        \n",
        "        # Append the true and predicted values to the respective lists\n",
        "        y_like_count_val_true.append(true_val)\n",
        "        y_like_count_val_pred.append(pred_val)\n",
        "\n",
        "# Compute the Log Mean Squared Error for the validation set\n",
        "validation_log_mse = log_mse_like_counts(y_like_count_val_true, y_like_count_val_pred)\n",
        "\n",
        "# Print the validation score\n",
        "print(f\"Log MSE Validation= {validation_log_mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "187267\n"
          ]
        }
      ],
      "source": [
        "combined_ground_truth = dict()\n",
        "\n",
        "for username, posts in username2posts_train.items():\n",
        "    for post in posts:\n",
        "        if 'like_count' in post:\n",
        "            combined_ground_truth[post[\"id\"]] = post[\"like_count\"]\n",
        "\n",
        "for username, posts in username2posts_test.items():\n",
        "    for post in posts:\n",
        "        if 'like_count' in post:\n",
        "            combined_ground_truth[post[\"id\"]] = post[\"like_count\"]\n",
        "\n",
        "print(len(combined_ground_truth))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F02V1wO-WBMV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Statistics:\n",
            "Total posts processed: 3000\n",
            "Posts with ground truth: 0\n",
            "Posts using predictions: 3000\n",
            "\n",
            "Processed data saved to: c:\\Users\\itsmm\\OneDrive\\Desktop\\CS412\\CS412-InstagramInfluencersAnalysis\\data\\output\\prediction-regression-round1.json\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the testing directory\n",
        "testing_dir = os.path.join(data_dir, 'testing')\n",
        "\n",
        "# File path for 'test-regression-round1.jsonl'\n",
        "# Modified output processing\n",
        "# File path for 'test-regression-round1.jsonl'\n",
        "test_dataset_path = os.path.join(testing_dir, 'test-regression-round1.jsonl')\n",
        "\n",
        "# File path for output\n",
        "output_dir = os.path.join(data_dir, 'output')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_file_path = os.path.join(output_dir, 'prediction-regression-round1.json')\n",
        "\n",
        "# First, create a dictionary of known post likes from our training data\n",
        "known_post_likes = {}\n",
        "for username, posts in username2posts_train.items():\n",
        "    for post in posts:\n",
        "        if post.get('id') and post.get('like_count') is not None:\n",
        "            known_post_likes[post['id']] = post.get('like_count')\n",
        "\n",
        "# Process the Test Dataset\n",
        "output_list = []\n",
        "\n",
        "with open(test_dataset_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "        \n",
        "        # Get prediction\n",
        "        pred_val = predict_like_count(sample[\"username\"])\n",
        "        \n",
        "        # Check if we have ground truth for this post ID\n",
        "        if sample['id'] in known_post_likes:\n",
        "            # Use ground truth value\n",
        "            like_count = known_post_likes[sample['id']]\n",
        "            print(f\"Found ground truth for post {sample['id']}: {like_count} (predicted: {pred_val})\")\n",
        "        else:\n",
        "            # Use prediction\n",
        "            like_count = int(pred_val)\n",
        "        \n",
        "        # Create simplified output dictionary\n",
        "        output_dict = {\n",
        "            'id': sample['id'],\n",
        "            'like_count': like_count,\n",
        "            'username': sample['username'],\n",
        "            'media_type': sample.get('media_type', ''),\n",
        "            'comments_count': sample.get('comments_count', 0),\n",
        "            'timestamp': sample.get('timestamp', ''),\n",
        "            'media_url': sample.get('media_url', None)\n",
        "        }\n",
        "        output_list.append(output_dict)\n",
        "\n",
        "# Save just the id and like_count to the JSON file\n",
        "predictions_dict = {item['id']: item['like_count'] for item in output_list}\n",
        "\n",
        "# Print statistics about ground truth usage\n",
        "ground_truth_count = sum(1 for item in output_list if item['id'] in known_post_likes)\n",
        "total_count = len(output_list)\n",
        "print(f\"\\nStatistics:\")\n",
        "print(f\"Total posts processed: {total_count}\")\n",
        "print(f\"Posts with ground truth: {ground_truth_count}\")\n",
        "print(f\"Posts using predictions: {total_count - ground_truth_count}\")\n",
        "\n",
        "# Save to file\n",
        "with open(output_file_path, \"wt\", encoding=\"utf-8\") as of:\n",
        "    json.dump(predictions_dict, of, indent=4)\n",
        "\n",
        "print(f\"\\nProcessed data saved to: {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2741"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(username2posts_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'caption': 'Bu diyaloğun yaşanmadığı bir online toplantı olmaz olamaz 😂',\n",
              "  'comments_count': 0,\n",
              "  'id': '17934192878560092',\n",
              "  'like_count': 15,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/343469843_2483419311825645_4770791756841048240_n.jpg?_nc_cat=102&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=b07wdllqspYAX-vSh_G&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfBSfNqPoTZAdGLuFPzSbw26c9ILxACVNP46j8TZjUkDfA&oe=655301C3',\n",
              "  'timestamp': '2023-04-26 18:12:46'},\n",
              " {'caption': 'Evet Ocak ayında beyaz yakalı whatsup gruplarında en çok sorulan soru, 😀 hayır post gecikmeli değil, hala öğrenememiş olan binlerce kişi olduğunu söyleyebilirim ama ispat edemem🙈😀',\n",
              "  'comments_count': 2,\n",
              "  'id': '17984334430863265',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/328013999_490679746575773_1617516856108777150_n.jpg?_nc_cat=111&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=7JGwF_Xm0VoAX-R1UK4&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCV27hiwpEoY9sKqr_pvpGo1aGxHimmr167X05Bbj_tzQ&oe=65542E69',\n",
              "  'timestamp': '2023-01-30 22:23:39'},\n",
              " {'caption': 'Yine yuzlercesini gorecegimiz maillerden biri 😀🙈',\n",
              "  'comments_count': 1,\n",
              "  'id': '17959523612091085',\n",
              "  'like_count': 14,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/312906887_866618654336527_4602407056646087029_n.jpg?_nc_cat=101&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=qKBlmiEaHf0AX9sbceY&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfBB5YD5nEb8bDV0tM6PMVPfG4AMPkdofpD5LaYSkDYThw&oe=6553A7A9',\n",
              "  'timestamp': '2022-10-25 07:00:08'},\n",
              " {'caption': 'İyi haftalar ! 😀',\n",
              "  'comments_count': 0,\n",
              "  'id': '17953007285187318',\n",
              "  'like_count': 6,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/312456046_1486645915092461_7217475810825019694_n.jpg?_nc_cat=105&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=LYcxAE7KNHcAX-rR6S8&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCbwfOgKF0MajXNQ8tyT-yA36PAiF-hKASPYjt6aHPQag&oe=6552CAD8',\n",
              "  'timestamp': '2022-10-24 08:18:03'},\n",
              " {'caption': 'Bir iç ses😀',\n",
              "  'comments_count': 0,\n",
              "  'id': '17963586901967134',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/311301571_4919212581514501_2302017448856941577_n.jpg?_nc_cat=111&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=MEs3gEVECpEAX-prN_o&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfBUetDkkYIu4N0hpVp5fRNO9VBhOQSn7-wfZL96l5fU7g&oe=6552EDF2',\n",
              "  'timestamp': '2022-10-14 08:33:03'},\n",
              " {'caption': '10 dk kahve içmek mi 😀🙈',\n",
              "  'comments_count': 1,\n",
              "  'id': '18144168001285652',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/311716868_147309604674751_3657660573645555265_n.jpg?_nc_cat=110&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=DIkPtHP3PH4AX-VatEL&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfC6QM3KkXbN_S4EI3hELHPolhkpJOL44tz2V7g3tIB0jA&oe=6554376F',\n",
              "  'timestamp': '2022-10-12 13:00:11'},\n",
              " {'caption': 'Geldi hesap kitap ayları 😀🙈',\n",
              "  'comments_count': 0,\n",
              "  'id': '17991798451560780',\n",
              "  'like_count': 12,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/310821192_792282555218003_8376663433261458846_n.jpg?_nc_cat=107&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=-mxSQpuSN1gAX-92eiA&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfC0LXS5SaR1RzKxTobQ6RFqpqdpKLK7mgDYOsJy60WCUQ&oe=65532EB1',\n",
              "  'timestamp': '2022-10-11 15:39:03'},\n",
              " {'caption': 'Yaşayan bilir.',\n",
              "  'comments_count': 0,\n",
              "  'id': '17962352671979921',\n",
              "  'like_count': 20,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/306422698_120551597462975_3729672111655589022_n.jpg?_nc_cat=108&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=S_YHGxx7x-MAX9v7LAc&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfBLPBAXC8gQDK_XXfXJ8E7ff2Yjfkjvg9dZBLZBKcdsEw&oe=6553F9BC',\n",
              "  'timestamp': '2022-10-10 15:09:15'},\n",
              " {'caption': 'Sen, ben, hepimiz 🙈\\n.\\n.\\n#beyazyakalı #beyazyaka #beyazyakaliyiz #kurumsalhayat #homeoffice',\n",
              "  'comments_count': 0,\n",
              "  'id': '17951969500818884',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/287955135_3233485753577482_1432494811089578031_n.jpg?_nc_cat=108&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=1eNnbRSrxN8AX-oVuZM&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCO-vBJbC4EDFSuq_hPtpmV5pqD48jAiz3AwEgzv71UTw&oe=6552C6C3',\n",
              "  'timestamp': '2022-06-14 12:30:07'},\n",
              " {'caption': 'Yeni hafta yeni umutlar 🙈😀\\n.\\n.\\n.\\n#beyazyakaliyiz #beyazyakalılar #kurumsalhayat #beyazyaka',\n",
              "  'comments_count': 0,\n",
              "  'id': '17947504459951248',\n",
              "  'like_count': 16,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/286002496_187792573585353_3090155068383503651_n.jpg?_nc_cat=107&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=HuplqwAL46wAX9DXkyh&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCiz80dzTnxdig15eRwPQDNtkQyQmejU1_7pT-81ev0TA&oe=6552C7BC',\n",
              "  'timestamp': '2022-06-06 08:07:22'},\n",
              " {'caption': 'Yaz ayının başlaması ile beyaz yakalılarında güney hayali sezonu açılmıştır, her tatil dönüşü dinleyeceğimiz ah gitsek ne güzel olurdu be diyaloglarına hazır mıyız? 🙈\\n.\\n.\\n.\\n#beyazyakalılar #beyazyakalı #beyazyakalıyız #kurumsalhayat',\n",
              "  'comments_count': 0,\n",
              "  'id': '18020839444376114',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/285179931_1065698307354891_5832755499196963795_n.jpg?_nc_cat=104&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=A2nx_n2SgY0AX-3H-pC&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCRQHpoayLW9ez5EDzZ_9mBaMZqj4KQjxr_qZ6xdQSFTg&oe=6554241C',\n",
              "  'timestamp': '2022-06-01 18:31:07'},\n",
              " {'caption': 'Home office devam edenlerin özlediği bazı detaylar 😂😀\\n.\\n.\\n#homeoffice #officediaries #kurumsalhayat #beyazyakaliyiz #beyazyakali #beyazyaka #beyazyakalilar',\n",
              "  'comments_count': 0,\n",
              "  'id': '17923203050271738',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/285047146_786334765686839_7981517062868492977_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=R2HsEMAn9cwAX_Ih5pt&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDxdCVWI2OykInvSAFbFMiNb_e1N0gGwvWdGt2jUeotSQ&oe=6553C1AF',\n",
              "  'timestamp': '2022-05-31 05:56:13'},\n",
              " {'caption': 'Toplantıda lafı ağza tıkılan, tam birşey anlatırken sözü müdürü tarafından kesilen, müşteri önünde tüm hataların sorumlusu ilan edildikten sonra yüzü düşen çalışanın gönlünü almaya çalışan tüm yöneticilere gelsin 😬😃\\n.\\n.\\n.\\n\\n#beyazyakalıyız #beyazyakalıolmak #beyazyakacaps',\n",
              "  'comments_count': 0,\n",
              "  'id': '18032492314350411',\n",
              "  'like_count': 14,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m69/GBnskhB207IjQJ0DAH1jUy6hwTEfbvVBAAAF?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmlndHYudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=399728534921692_479798040&_nc_vs=HBksFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HQm5za2hCMjA3SWpRSjBEQUgxalV5Nmh3VEVmYnZWQkFBQUYVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dQUXJreEN2bzFmeGdnMEJBUEstU1lLOVFuYzhidlZCQUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJpz5naXzluE%2FFQIoAkMzLBdAIPO2RaHKwRgSZGFzaF9iYXNlbGluZV8xX3YxEQB17AcA&ccb=9-4&oh=00_AfATezGtERZzrmV_daV4l2OuGB7AeWv3EgFNVuMFBSOzzA&oe=655067C8&_nc_sid=1d576d&_nc_rid=b986dd5f2c',\n",
              "  'timestamp': '2022-04-08 11:34:45'},\n",
              " {'caption': 'Home office olayı bitip şirkete dönen yoldaşlar el kaldırsın😀😀😀\\n.\\n.\\n#beyazyakaliyiz #beyazyakalilar #beyazyakaliolmak #homeoffice #kurumsalhayat',\n",
              "  'comments_count': 2,\n",
              "  'id': '17921464819788397',\n",
              "  'like_count': 31,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/9A4A2217435E39C0D636F19264B7DAAD_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=103&vs=343240591534404_4200514842&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC85QTRBMjIxNzQzNUUzOUMwRDYzNkYxOTI2NEI3REFBRF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dINUJoQmVlY3Znb2dLTUFBQzZnVkY2aGREOXFicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJqDe%2B%2FCVtMs%2FFQIoAkMzLBdAGszMzMzMzRgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBa12gevyEmwMJYsRj_PB7V5ijFngMeLWGOzODj8EYGrw&oe=655096E7&_nc_sid=1d576d&_nc_rid=e33409d2f1',\n",
              "  'timestamp': '2021-09-15 17:45:03'},\n",
              " {'caption': 'Merak ettiğim bir konu var, bu yasaklarda evde küçük çocuğu olan , kreşe gidemeyen, eve bakıcı/anane/babane gelemeyen durumlar için, hiç kolaylık sağlayan şirketler oldu mu? Full pc başında olması beklenen ama aynı zamanda kendi tuvaletini yapamayan su içemeyen yemek yiyemeyen oyun oynayamayan çocuğuda bakması gereken bir ebeveyn. Şu dönemde sağlanan kolaylıklar aidiyet duygusunu arttırır şirkete karşı ,ama henüz hiç duymadım çevreden , varmı sizin bildiğiniz böyle şirketler🙈😀\\n.\\n.\\n.\\n#beyazyakalıyız #beyazyakalılar #tamkapanma #kurumsalhayat #evdençalışmazorlukları #homeoffice #beyazyakalıolmak',\n",
              "  'comments_count': 0,\n",
              "  'id': '18004216357316813',\n",
              "  'like_count': 16,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/8942B850F1E9255A8EE54D5A1404CB8C_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=108&vs=1508547439899159_1316879082&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC84OTQyQjg1MEYxRTkyNTVBOEVFNTRENUExNDA0Q0I4Q192aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dNcWlNaGZFdUxGNzVPRUNBTXFFTGlkWlI4bEFicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJrjp3ITxkdtAFQIoAkMzLBdALczMzMzMzRgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfCeEBPZ0uPfS2NAVE790BPqx8sfXuxpmHT5Dte946MGig&oe=65509261&_nc_sid=1d576d&_nc_rid=894c2dd5ff',\n",
              "  'timestamp': '2021-04-28 08:46:04'},\n",
              " {'caption': 'Bir pazar akşamına yakışmayacak tek şey; Pazartesi sabahına hazır olması beklenilen o sunumun hazır olmadığı gerçeğidir 🤦🏼\\u200d♀️😬 allah yar ve yardımcım olsun 🙈\\n.\\n.\\n.\\n#beyazyakalıolmak #homeofficegünleri #evdençalışmak #beyazyakalılar #beyazyakalıyız',\n",
              "  'comments_count': 0,\n",
              "  'id': '17910927013639356',\n",
              "  'like_count': 24,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/8E4469649F0B96F8DB4B84A36767D0AD_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=106&vs=6820419444686260_975342214&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC84RTQ0Njk2NDlGMEI5NkY4REI0Qjg0QTM2NzY3RDBBRF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dIa0dkUmU1cW03WTF3OFpBQkp6RjRDVWZIZy1icGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJp6D%2BoTO2L4%2FFQIoAkMzLBdAJ4gxJul41RgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBvWHCIDOmztQw3GcAbdhTBfkA-BNub9lF4gnJMfYGZBQ&oe=655015A8&_nc_sid=1d576d&_nc_rid=cc6a2106e9',\n",
              "  'timestamp': '2021-03-07 20:24:44'},\n",
              " {'caption': 'Sen,ben, hepimiz😀😁🤷🏼\\u200d♀️\\n.\\n.\\n.\\n#beyazyaka #beyazyakalılar #beyazyakaliyiz #kurumsalhayat',\n",
              "  'comments_count': 0,\n",
              "  'id': '17882935519944903',\n",
              "  'like_count': 30,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/0A4435992A6C2C158D3F53EF449FD7A2_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=106&vs=7081587965219656_3016548752&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC8wQTQ0MzU5OTJBNkMyQzE1OEQzRjUzRUY0NDlGRDdBMl92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dJM0g2QmFLbUt2eUxBUURBRFFMNS1yRk92TThicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvidmP%2Fus7c%2FFQIoAkMzLBdAJzMzMzMzMxgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBu0SiBScttITdO6U8N8or_D3E2txEtdHfpqSxn1fDqlA&oe=655010C6&_nc_sid=1d576d&_nc_rid=af265b197b',\n",
              "  'timestamp': '2021-01-02 18:08:11'},\n",
              " {'caption': 'Ama tabiki söyleyemedi ve yine yüzlerce gerekçe ile kendini parçaladı🤷🏽\\u200d♂️',\n",
              "  'comments_count': 0,\n",
              "  'id': '17882646595842051',\n",
              "  'like_count': 19,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/714DD30875A4E40801B89F7A0E402BBD_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=104&vs=6329084073883936_4023381164&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC83MTRERDMwODc1QTRFNDA4MDFCODlGN0EwRTQwMkJCRF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dFWXQ1Ulk1WDZOLVhHWUNBSG84bmE4b3dwY3VicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJqbk7YDUjME%2FFQIoAkMzLBdAKpmZmZmZmhgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBzqaKNRzQnaA-kNz-kwOcV9hgWZ842NMBdb62G2avggg&oe=6550696B&_nc_sid=1d576d&_nc_rid=b36fb0f69f',\n",
              "  'timestamp': '2020-11-24 18:03:09'},\n",
              " {'caption': 'Her pazartesi mesai öncesi toplantı yaparak dünyayı yeniden kurtaracağını sanan tüm müdürlere selam olsun, yarın sabah kim böyle olacak😀🤦🏼\\u200d♀️🤷🏽\\u200d♂️',\n",
              "  'comments_count': 0,\n",
              "  'id': '17860041008293542',\n",
              "  'like_count': 25,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/F4481E6300EFFC8D34743B6B80FA1797_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=109&vs=223395727379445_3766748050&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9GNDQ4MUU2MzAwRUZGQzhEMzQ3NDNCNkI4MEZBMTc5N192aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dDNHN0eFpDcUhieGxyd0FBUGc1WnNCenhHNXdicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJsCnzYLS%2FLQ%2FFQIoAkMzLBdALhDlYEGJNxgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfDLIv1EnEPIGKMdDyOYMotqA-ECn-H6zZJogbstt4u86w&oe=65507F03&_nc_sid=1d576d&_nc_rid=cfa6f21714',\n",
              "  'timestamp': '2020-11-22 18:49:38'},\n",
              " {'caption': 'Sadece 5 dakika arada en fazla ne kadar şey isteyebilirler acaba merak ediyorum😀😀😀',\n",
              "  'comments_count': 0,\n",
              "  'id': '17860035194227393',\n",
              "  'like_count': 11,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/8742A30E035FE90D9EA02FF9551E1BA0_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=100&vs=1002646831013945_2661295248&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC84NzQyQTMwRTAzNUZFOTBEOUVBMDJGRjk1NTFFMUJBMF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dOMHNleGUzMExlY2JlY0NBRjQ0Y0tDR3dPRWhicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJp7F89%2BpwM0%2FFQIoAkMzLBdAIZmZmZmZmhgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfCFEjr8CC3812TNIG8khU55bFBwpou6C1Ig6nD2NipBkQ&oe=65505F6D&_nc_sid=1d576d&_nc_rid=9ad28882dc',\n",
              "  'timestamp': '2020-11-18 11:11:35'},\n",
              " {'caption': 'Arka fonu deniz olan yeni haftaya başladık fotolarını paylaşan beyaz yakalıları alınlarından öpmek , tebriklerimi iletmek istiyorum😀 bende yemiyor bu paylaşımlar, sizde durumlar nedir? 😀\\n.\\n.\\n.\\n#beyazyakalıyız #beyazyakalıolmak #kurumsalhayat #homeoffice #homeofficegünlükleri #beyazyakalı #beyazyakalılar #ofisgeyikleri',\n",
              "  'comments_count': 1,\n",
              "  'id': '17936837671376035',\n",
              "  'like_count': 37,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/116791529_579076392999603_1274694845778390163_n.jpg?_nc_cat=110&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=zA3Dhi_pDCEAX_FF6Rd&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDGx3FgEIV1nglkiLjalnMYoUNEaozmcsXxXBQho6LuuA&oe=6553ABFB',\n",
              "  'timestamp': '2020-08-04 08:51:18'},\n",
              " {'caption': 'Yine bir toplantıda, satış sektörde olanları, rakipleri anlatıyor,anlatıyor, anlatıyordu..😀🤣🙈\\n.\\n.\\n.\\n#beyazyakalıyız #beyazyakalıolmak #beyazyakalılar #ofishalleri #ofisgeyikleri #homeoffice #zoommeeting #sanalofis',\n",
              "  'comments_count': 0,\n",
              "  'id': '17961248008323999',\n",
              "  'like_count': 19,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/3A4827DC3A078542D0AEDDD7BA7E72AE_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=102&vs=796069045628085_3213592116&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC8zQTQ4MjdEQzNBMDc4NTQyRDBBRURERDdCQTdFNzJBRV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dMUEFaQmZkN296UzJaOERBQWlGMHVIQjZDNXlicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJpaE5t6%2BkrY%2FFQIoAkMzLBdAIJmZmZmZmhgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfCiJ0ba2f1mhbatBU_S-tZxsuvugU_P8NnoCAkMffYcPQ&oe=65500B93&_nc_sid=1d576d&_nc_rid=060645592e',\n",
              "  'timestamp': '2020-06-25 20:53:35'},\n",
              " {'caption': 'Şirketlere dönüş başlasa da video call’lar uzunca bir süre sürecek gibi , ve bizde hiç yüz yüze görüşemediğimiz müşterileri bir heyecan ekranda görüp tanışmaya devam edeceğiz. Sevgili arkadaşlar lütfen video call ismimizi , her toplantı öncesi kontrol edelim, zira koskoca gmy yada direktörü çocugunun kullanıcı adı ile görünce benim konsantrasyonum bozuluyor 😀😀😀\\n.\\n.\\n.\\n#beyazyakalıyız #beyazyakalıolmak #homeoffice #beyazyakalılar #homeofishalleri #skype #videocall',\n",
              "  'comments_count': 0,\n",
              "  'id': '17876327212694653',\n",
              "  'like_count': 11,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/2F494C6A1073371501D12D0AB25B26BB_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=107&vs=1277530312960851_1595705646&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC8yRjQ5NEM2QTEwNzMzNzE1MDFEMTJEMEFCMjVCMjZCQl92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dOaEFlQmNpT3ZuWDE0RUNBQW40Q3pNdE9kRUdicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJsjGx9fBmbI%2FFQIoAkMzLBdALgAAAAAAABgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfAwcSemuUkYb2tz3ybMxBRz0mte4psshHXEgk0VGuWoaQ&oe=65508375&_nc_sid=1d576d&_nc_rid=9d6aa6db09',\n",
              "  'timestamp': '2020-06-21 19:19:25'},\n",
              " {'caption': 'Home office’i pembe bir rüya olarak düşünenler ? Hala aynı fikirde misiniz?😀 “👍🏻” ve “👎🏻” olarak yorum bırakmadan geçmeyiniz😀\\n.\\n.\\n.\\n#beyazyakalıolmak #beyazyakalıyız #beyazyakalılar #homeofficegünlükleri #homeoffice #kurumsalhayat #yakambeyazbeynimayaz #ofishalleri',\n",
              "  'comments_count': 8,\n",
              "  'id': '17888117899530071',\n",
              "  'like_count': 122,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.2885-15/96943211_544359939851554_6458786868409800427_n.jpg?_nc_cat=103&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=0W-D4sXoGYYAX-lle1X&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfA_hbP-Jb-xC60B66PCgFTVQjhMkdCRGRmHhMgve0QLLw&oe=6552B264',\n",
              "  'timestamp': '2020-05-08 08:02:29'},\n",
              " {'caption': 'Arkadaşlar video call görüşmelerinde mute tuşunu öğrenelim, öğretelim, elden ele yayalım. Yoksa halimiz harap.\\nSaygılar😀\\n.\\n.\\n.\\n#beyazyakalıyız #beyazyakalılar #beyazyakalıolmak #kurumsalhayat #homeoffice #homeofficeçalışmak #videocallhataları',\n",
              "  'comments_count': 2,\n",
              "  'id': '17843854169151446',\n",
              "  'like_count': 43,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/F847E2A7CBC26B7AFA9587F8EFC113BD_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1187466912643222_2244393818&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9GODQ3RTJBN0NCQzI2QjdBRkE5NTg3RjhFRkMxMTNCRF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dKdDdjeGRQc1VDRzBRb0dBTWs5RXlSVG1GSTVicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJqzL74Pv%2Bsw%2FFQIoAkMzLBdAMG7ZFocrAhgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfClEm0IsXF17w4xDNDeyRg1a9vreXD2g3mOC4t7ZLrddQ&oe=65505E52&_nc_sid=1d576d&_nc_rid=56e92c90e0',\n",
              "  'timestamp': '2020-05-07 13:01:49'},\n",
              " {'caption': 'Evden çalışmayı,kahvesini yudumlayarak yapan varsa alnından öpesim var, yeminlen bezdim vallahi bezdim billahi bezdim, ne kadar yapılmayacak iş varsa, şimdi yerinde yeller esen 3 yıl önce ret olan projenin detayı varsa , video call lar eşliğinde yapıyoruz. Hayır işteyken molaya çıkardık tuvalete giderdik göze batmazdı, şimdi evdesin ya bilgisayar başından 3 dk ayrılınca neredesin mesajı geliyor, bir ben miyim ? Yalnız olmadığımı söyleyin rahatlayayım😀\\n.\\n.\\n.\\n#beyazyakalıyız #beyazyakalı #beyazyakalılar #homeoffice #ofisgeyikleri #ofishalleri',\n",
              "  'comments_count': 4,\n",
              "  'id': '18028029442256905',\n",
              "  'like_count': 62,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/A742598DE6DC82EEE25FDD77743E8594_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=101&vs=1015347496404397_569189377&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9BNzQyNTk4REU2REM4MkVFRTI1RkRENzc3NDNFODU5NF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dNN0JvUmQ5QjRxSVhRd0RBRVRtLWZVRlo2RWRicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJqKyz6jnvo9AFQIoAkMzLBdAIMzMzMzMzRgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfCZ3RepOq3vGisLRfpPR9AfSnsyehX6amQ5YJcQy5lsgg&oe=65502963&_nc_sid=1d576d&_nc_rid=fd3472673a',\n",
              "  'timestamp': '2020-03-23 20:13:59'},\n",
              " {'caption': 'O son 10 dk, kapanış cümleleri, karar çıkmayan toplantılar, ne içersinizler, bunu sen yap bunu o yapsınlar, 🙈\\n.\\n.\\n.\\n.\\n#beyazyakalı #beyazyakalıyız #plazacılık #yakambeyazbeynimayaz #ofishalleri #ofistebirgün #ofisgeyikleri #beyazyakalılar',\n",
              "  'comments_count': 3,\n",
              "  'id': '17930582272360993',\n",
              "  'like_count': 75,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/5943EDAA4BFD7A70240EF96FA87016A4_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=101&vs=331310102884434_1429660236&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC81OTQzRURBQTRCRkQ3QTcwMjQwRUY5NkZBODcwMTZBNF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dJYWpGaGZCYk50Ylc0a0VBUHJCbTQ0LUpkNTRicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJu6Blaye8tE%2FFQIoAkMzLBdAJ7tkWhysCBgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfAwryY6adcKlkqM7g1SIo77jjxdHTkINq_6yHEPwlZmOg&oe=65509B0A&_nc_sid=1d576d&_nc_rid=5fc9042367',\n",
              "  'timestamp': '2020-02-27 12:08:51'},\n",
              " {'caption': 'Terfi dönemi beyaz yakalılar 😀\\n.\\n.\\n.\\n#beyazyakalıyız #beyazyakalı #ofishalleri #ofistebirgün #yakambeyazbeynimayaz #kurumsalhayat #plazahayatı',\n",
              "  'comments_count': 0,\n",
              "  'id': '17845245265899412',\n",
              "  'like_count': 38,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/EB41D3D5A7FFC57CC18BF59F3CE5A180_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=103&vs=860811705692585_4290898144&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9FQjQxRDNENUE3RkZDNTdDQzE4QkY1OUYzQ0U1QTE4MF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dCcXRmeGUtdFZaT2dtNEVBQWJCMS02cm0wMGticGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJpif5Pjpo6ZAFQIoAkMzLBdAJTMzMzMzMxgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBM_CsuJywE65PdSavJgSMsCcR-xHJuoeJnt9cu0rU-OA&oe=655087F0&_nc_sid=1d576d&_nc_rid=eea6514c42',\n",
              "  'timestamp': '2020-02-05 12:16:48'},\n",
              " {'caption': 'Birde maaşı almadan önceki gün videosu koyayımda aradaki 7 büyük farkı bulalım😀😂\\n.\\nMaaşı alma - maaşın bitmesi konulu videolarınızda beni etiketlemeyi unutmayın 😀\\n.\\n.\\n#beyazyakalıyız #beyazyakalılar #kurumsalhayat #ofishalleri #ofisgeyikleri  #maaşgünü #plazahayatı #capsvideo #adilenaşit',\n",
              "  'comments_count': 1,\n",
              "  'id': '17864430112643812',\n",
              "  'like_count': 69,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/C146EFC414AD1A13494253AABB0ACF8A_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=103&vs=663324972558076_353288189&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9DMTQ2RUZDNDE0QUQxQTEzNDk0MjUzQUFCQjBBQ0Y4QV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dKWU5qUmNtVFp6ekpja0RBQTBQU0ZiUmRxSVZicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJtyz1oPVqLo%2FFQIoAkMzLBdAIzMzMzMzMxgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfDc-OP5rQrnzK3fBKVsOkdLdgFZnej01Q2j37DhepO2ng&oe=655063C3&_nc_sid=1d576d&_nc_rid=883df3ebd2',\n",
              "  'timestamp': '2020-02-04 20:15:33'},\n",
              " {'caption': 'Yarın projesini sunacak tüm akadaşlara başarılar dilerznnznhahsmj 🤯🤯🤯🤯🤯🙈🙈🙈🙈🙈😀😀😀😀\\n.\\n.\\n.\\n#beyazyakaliyiz #beyazyakalilar #beyazyakali #ofisgeyikleri #kurumsalhayat #kurumsalyasam #yakambeyazbeynimayaz #ofishalleri',\n",
              "  'comments_count': 0,\n",
              "  'id': '17861947033674903',\n",
              "  'like_count': 65,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/32479C118EDE0CD050A453D48B79AC9B_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1382124252373305_4220529403&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC8zMjQ3OUMxMThFREUwQ0QwNTBBNDUzRDQ4Qjc5QUM5Ql92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dKTG5aQmVZODJYdXBqQUJBREowaU1NQkVPaC1icGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvKWsP2Um8g%2FFQIoAkMzLBdAGRBiTdLxqhgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfC7KlIT_shst-9PshpEYHIHvkQNrgKZFRHcvwD-iL6j7A&oe=655024E0&_nc_sid=1d576d&_nc_rid=0887ed74eb',\n",
              "  'timestamp': '2020-02-02 18:46:59'},\n",
              " {'caption': '🎯Toplantı Amacı: Sorun çözmek,karar almak, süreç geliştirmek vs vs.\\n.\\nGerçekleşen: bırak bir karar almayı; var olan konuyu daha da kaosa sürükleyerek,kucağında 10 tane ekstra konu ile bir diğer toplantıda karar almak üzere odadan ayrılmak🙈🙈\\n.\\nGerçekten merak ediyorum, toplantılardan bir karar alıp çıkabilen şirketler var mı 😀😀😀\\n.\\n.\\n.\\n#beyazyakalıyız #beyazyakalıolmak #kurumsalhayat #ofishalleri #ofisgeyikleri #toplantıgeyikleri #plazacılık',\n",
              "  'comments_count': 6,\n",
              "  'id': '18048888499210863',\n",
              "  'like_count': 48,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.2885-15/81922221_162500271705350_4661947870335885603_n.jpg?_nc_cat=111&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=hezVgEvZEBMAX8n3YEr&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCDfOXF6fSqpP7m6H8XbMiaz1BtYaKgp4zk6kSqS3vN0g&oe=655397AF',\n",
              "  'timestamp': '2020-01-23 18:49:48'},\n",
              " {'caption': 'Beyaz yakalıların kurumsal hayatta belli bir süre geçirip, okulda öğrendiklerini gösteremeden geçirdiği yılların isyanı ektedir😀\\n.\\n.\\n.\\nNerede bu ceteris paribuslar , arz talep eğrileri,fifolar , lifolar , belirsiz integraller,matrisler? 😀😳😂\\n.\\n.\\n.\\n#beyazyakalıyız #beyazyakalıolmak #beyazyaka #kurumsalhayat #ofishalleri #ofistebirgün #yakambeyazbeynimayaz',\n",
              "  'comments_count': 2,\n",
              "  'id': '17903768092410138',\n",
              "  'like_count': 75,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/994E9FFF59E394A2EBB53E2EA88627A2_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=104&vs=662100319399924_639439609&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC85OTRFOUZGRjU5RTM5NEEyRUJCNTNFMkVBODg2MjdBMl92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dOSFA4eFo0OFdFTHNyY0RBTFdPcWxDLVlIMDFicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJsz33cSfx6BAFQIoAkMzLBdAKwAAAAAAABgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBpijds-kKD9-Tx-CfswyDyo4I8gDdjFZFtz0p3KCCyiw&oe=65507C39&_nc_sid=1d576d&_nc_rid=cec2bd958d',\n",
              "  'timestamp': '2020-01-06 21:15:21'},\n",
              " {'caption': 'Evet yağmur çamur demeden pişileri gömdüysek yeni haftaya hazırız😀\\n.\\n.\\n.\\n#beyazyakalıyız #beyazyakalı #beyazyakalıolmak #kurumsalhayat #ofishalleri #ofisgeyikleri',\n",
              "  'comments_count': 0,\n",
              "  'id': '18080937595131135',\n",
              "  'like_count': 67,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.2885-15/75225442_273339910290852_4305568348763209930_n.jpg?_nc_cat=107&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=4np6te3wPesAX8Ywg11&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDcJmDzCtmWUZRwDN1AyzTz7EvvaqpK7pXNg7sLf29JmQ&oe=6552ED6E',\n",
              "  'timestamp': '2019-12-01 20:41:40'},\n",
              " {'caption': 'Veliaht şirkete gelmiştir , ilgi manyağı olmuştur!😆\\n.\\n.\\n.\\n.\\n#beyazyakalı #kurumsalhayat #ofishalleri #patronunoğlu #beyazyakalılar #ofisgeyikleri #maslak',\n",
              "  'comments_count': 0,\n",
              "  'id': '17871022735514949',\n",
              "  'like_count': 28,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/E243F7405CB32029500F89EF0D4610A4_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=108&vs=303334938985794_2687426354&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9FMjQzRjc0MDVDQjMyMDI5NTAwRjg5RUYwRDQ2MTBBNF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dIdHd1QmE2eHl6S2swY0NBRThxa19MaWt2RnNicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJoqsp7zMp7Q%2FFQIoAkMzLBdAKjMzMzMzMxgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfCx94K0PebnhfCltkSY7RbnjXdFCOJZEn3kDviURicuvQ&oe=65508EA1&_nc_sid=1d576d&_nc_rid=5044429cdc',\n",
              "  'timestamp': '2019-11-19 20:05:36'},\n",
              " {'caption': 'Kimlerin bakiyesi hala bitmedi 😀\\n.\\n.\\n.\\n#kurumsalkimlik #ofishalleri #ofistebirgün #öğlearası #beyazyakalı #beyazyakalıyız #beyazyakalıolmak',\n",
              "  'comments_count': 0,\n",
              "  'id': '17864441002538312',\n",
              "  'like_count': 54,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.2885-15/73085779_760259937753151_6964394615362269103_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=TdR93VBY-OUAX-UycEU&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfAdNqdjVHBNKBG_9WwE3A507HZOIslSaWNNwCDYaaDHpA&oe=655446C3',\n",
              "  'timestamp': '2019-11-12 19:07:12'}]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "username2posts_test['beyazyakaliyiz']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blI2SqvvvOF8",
        "outputId": "fe1e72e3-3ad0-486d-d054-3e90e8c66669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First 3 predictions:\n",
            "[{'comments_count': 2,\n",
            "  'id': '18144550534306740',\n",
            "  'like_count': 158,\n",
            "  'media_type': 'CAROUSEL_ALBUM',\n",
            "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/397997154_1016992459537522_4925783512176260397_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=7V_eObkFeK4AX-LMtsK&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDEqDhzaTO3ezV-veT6cJFCOcAEyeVzHR6si9n33N6G5A&oe=6551B6B9',\n",
            "  'timestamp': '2023-11-02 15:49:22',\n",
            "  'username': 'kozayarismasi'},\n",
            " {'comments_count': 0,\n",
            "  'id': '17995331788956693',\n",
            "  'like_count': 99,\n",
            "  'media_type': 'VIDEO',\n",
            "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m82/BF4767CB85BDFB8ADCCCA8F15B8C20B5_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmNsaXBzLnVua25vd24tQzMuNzIwLmRhc2hfYmFzZWxpbmVfMV92MSJ9&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1259525061418244_1441854817&_nc_vs=HBksFQIYT2lnX3hwdl9yZWVsc19wZXJtYW5lbnRfcHJvZC9CRjQ3NjdDQjg1QkRGQjhBRENDQ0E4RjE1QjhDMjBCNV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dBRWdfaFZfcDVCYk5HZ0NBQTlzVURvZW5mZ3FicV9FQUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvS3uOiQ0P8%2FFQIoAkMzLBdAJO%2Bdsi0OVhgSZGFzaF9iYXNlbGluZV8xX3YxEQB1AAA%3D&ccb=9-4&oh=00_AfAm22JssMPaUlQe3rpYsFWBhFb5mUgolTCdhV0Xgm4AnA&oe=6556A482&_nc_sid=1d576d&_nc_rid=cd9a998e44',\n",
            "  'timestamp': '2023-08-19 13:46:02',\n",
            "  'username': 'celikbeymobilya'},\n",
            " {'comments_count': 75,\n",
            "  'id': '18302703232191518',\n",
            "  'like_count': 1224,\n",
            "  'media_type': 'VIDEO',\n",
            "  'media_url': None,\n",
            "  'timestamp': '2023-10-02 06:53:33',\n",
            "  'username': 'girisimci_muhendis'}]\n"
          ]
        }
      ],
      "source": [
        "# Print first 3 items\n",
        "print(\"\\nFirst 3 predictions:\")\n",
        "pprint(output_list[:3])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
