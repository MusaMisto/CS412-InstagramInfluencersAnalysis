{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBOlFYpnlr0N"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "PM5EnXhLk6BL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgAdz_b8xp8w",
        "outputId": "14cd6ea6-4a61-4b22-f2a6-59088a78e58a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\itsmm\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Turkish StopWords\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "turkish_stopwords = stopwords.words('turkish')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plt9-VkNmlmH"
      },
      "source": [
        "# Influencer Category Classification\n",
        "\n",
        "\n",
        "\n",
        "1.   Read Data\n",
        "2.   Preprocess Data\n",
        "3.   Prepare Model\n",
        "4.   Predict Test Data\n",
        "4.   Save outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "8DeBh-b7lrEs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First few rows of the training classification DataFrame:\n",
            "           user_id          category\n",
            "0    taskirancemal  mom and children\n",
            "1    tam_kararinda              food\n",
            "2         spart4nn              food\n",
            "3  sosyalyiyiciler              food\n",
            "4  sonaydizdarahad  mom and children\n",
            "\n",
            "Dataset Statistics:\n",
            "Total unique users: 2742\n",
            "\n",
            "Category distribution:\n",
            "category\n",
            "food                    511\n",
            "health and lifestyle    503\n",
            "tech                    346\n",
            "entertainment           323\n",
            "fashion                 299\n",
            "travel                  294\n",
            "art                     191\n",
            "mom and children        149\n",
            "sports                  113\n",
            "gaming                   13\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically (keep the same)\n",
        "...\n",
        "\n",
        "# Step 2: Load Data Dynamically\n",
        "train_classification_df = pd.read_csv(train_classification_path)\n",
        "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
        "\n",
        "# Check for duplicates\n",
        "duplicate_users = train_classification_df[train_classification_df['user_id'].duplicated(keep=False)]\n",
        "if not duplicate_users.empty:\n",
        "    print(\"\\nWarning: Found duplicate user_ids:\")\n",
        "    print(duplicate_users.sort_values('user_id'))\n",
        "    \n",
        "    # Option 1: Keep first occurrence\n",
        "    train_classification_df = train_classification_df.drop_duplicates(subset='user_id', keep='first')\n",
        "    print(\"\\nKept first occurrence of duplicates.\")\n",
        "    \n",
        "    # Option 2: Could also check if duplicates have different categories\n",
        "    duplicate_categories = duplicate_users.groupby('user_id')['category'].nunique() > 1\n",
        "    if duplicate_categories.any():\n",
        "        print(\"\\nWarning: Some duplicates have different categories:\")\n",
        "        for user_id in duplicate_categories[duplicate_categories].index:\n",
        "            user_entries = duplicate_users[duplicate_users['user_id'] == user_id]\n",
        "            print(f\"\\nUser {user_id} has multiple categories:\")\n",
        "            print(user_entries[['user_id', 'category']])\n",
        "\n",
        "# Step 3: Unify Labels\n",
        "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
        "\n",
        "# Step 4: Create User-to-Category Mapping\n",
        "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\n",
        "\n",
        "# Step 5: Verify Output\n",
        "print(\"\\nFirst few rows of the training classification DataFrame:\")\n",
        "print(train_classification_df.head())\n",
        "\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(f\"Total unique users: {len(username2_category)}\")\n",
        "print(\"\\nCategory distribution:\")\n",
        "print(train_classification_df['category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "SfD7BQ3hE5Jh",
        "outputId": "a67f08dd-ab3e-491f-bfe2-a14da0e961cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>art</th>\n",
              "      <td>209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>entertainment</th>\n",
              "      <td>351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fashion</th>\n",
              "      <td>316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gaming</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>health and lifestyle</th>\n",
              "      <td>547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mom and children</th>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sports</th>\n",
              "      <td>133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tech</th>\n",
              "      <td>371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>travel</th>\n",
              "      <td>316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      user_id\n",
              "category                     \n",
              "art                       209\n",
              "entertainment             351\n",
              "fashion                   316\n",
              "food                      531\n",
              "gaming                     20\n",
              "health and lifestyle      547\n",
              "mom and children          152\n",
              "sports                    133\n",
              "tech                      371\n",
              "travel                    316"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# stats about the labels\n",
        "train_classification_df.groupby(\"category\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mhckiEfD2gg_",
        "outputId": "af451a4d-8825-4506-fb70-41a6ef845aaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mom and children'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "username2_category[\"sonaydizdarahad\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "GT_IcUM2nGBH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Statistics:\n",
            "Number of Training Users: 2741\n",
            "Number of Testing Users: 2674\n",
            "Total unique training posts: 94824\n",
            "Total unique testing posts: 92478\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the training directory\n",
        "training_dir = os.path.join(data_dir, 'training')\n",
        "\n",
        "# File path for 'training-dataset.jsonl.gz'\n",
        "train_data_path = os.path.join(training_dir, 'training-dataset.jsonl.gz')\n",
        "\n",
        "# Step 2: Initialize Dictionaries for Data\n",
        "username2posts_train = dict()\n",
        "username2profile_train = dict()\n",
        "\n",
        "username2posts_test = dict()\n",
        "username2profile_test = dict()\n",
        "# Step 1 and 2 remain the same...\n",
        "\n",
        "# Initialize sets to track post IDs\n",
        "train_post_ids = set()\n",
        "test_post_ids = set()\n",
        "duplicate_posts = []\n",
        "\n",
        "# Step 3: Process Data with duplicate checking\n",
        "with gzip.open(train_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "\n",
        "        profile = sample[\"profile\"]\n",
        "        username = profile.get(\"username\", \"\").strip()\n",
        "        if not username:\n",
        "            continue\n",
        "\n",
        "        # Check posts for duplicates\n",
        "        filtered_posts = []\n",
        "        for post in sample[\"posts\"]:\n",
        "            post_id = post.get(\"id\")\n",
        "            if post_id:\n",
        "                if username in username2_category:\n",
        "                    if post_id in train_post_ids:\n",
        "                        duplicate_posts.append({\n",
        "                            'id': post_id,\n",
        "                            'username': username,\n",
        "                            'timestamp': post.get('timestamp'),\n",
        "                            'dataset': 'train'\n",
        "                        })\n",
        "                        continue\n",
        "                    train_post_ids.add(post_id)\n",
        "                else:\n",
        "                    if post_id in test_post_ids:\n",
        "                        duplicate_posts.append({\n",
        "                            'id': post_id,\n",
        "                            'username': username,\n",
        "                            'timestamp': post.get('timestamp'),\n",
        "                            'dataset': 'test'\n",
        "                        })\n",
        "                        continue\n",
        "                    test_post_ids.add(post_id)\n",
        "            filtered_posts.append(post)\n",
        "\n",
        "        if username in username2_category:\n",
        "            username2posts_train[username] = filtered_posts\n",
        "            username2profile_train[username] = profile\n",
        "        else:\n",
        "            username2posts_test[username] = filtered_posts\n",
        "            username2profile_test[username] = profile\n",
        "\n",
        "# Print statistics\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(f\"Number of Training Users: {len(username2posts_train)}\")\n",
        "print(f\"Number of Testing Users: {len(username2posts_test)}\")\n",
        "print(f\"Total unique training posts: {len(train_post_ids)}\")\n",
        "print(f\"Total unique testing posts: {len(test_post_ids)}\")\n",
        "\n",
        "if duplicate_posts:\n",
        "    print(\"\\nFound duplicate posts:\")\n",
        "    print(f\"Total duplicates: {len(duplicate_posts)}\")\n",
        "    print(\"\\nFirst few duplicates:\")\n",
        "    for dup in duplicate_posts[:5]:\n",
        "        print(f\"Post ID: {dup['id']}\")\n",
        "        print(f\"Username: {dup['username']}\")\n",
        "        print(f\"Timestamp: {dup['timestamp']}\")\n",
        "        print(f\"Dataset: {dup['dataset']}\")\n",
        "        print(\"---\")\n",
        "\n",
        "# Also check for cross-dataset duplicates\n",
        "cross_duplicates = train_post_ids.intersection(test_post_ids)\n",
        "if cross_duplicates:\n",
        "    print(\"\\nWarning: Found posts that appear in both train and test sets!\")\n",
        "    print(f\"Number of cross-dataset duplicates: {len(cross_duplicates)}\")\n",
        "    print(\"First few cross-dataset duplicate IDs:\", list(cross_duplicates)[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "djeF1GQ1oy3v",
        "outputId": "5c2ead24-d7d1-4df8-bec0-bf2152c76832"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>id</th>\n",
              "      <th>full_name</th>\n",
              "      <th>biography</th>\n",
              "      <th>category_name</th>\n",
              "      <th>post_count</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>is_business_account</th>\n",
              "      <th>is_private</th>\n",
              "      <th>...</th>\n",
              "      <th>business_category_name</th>\n",
              "      <th>overall_category_name</th>\n",
              "      <th>category_enum</th>\n",
              "      <th>is_verified_by_mv4b</th>\n",
              "      <th>is_regulated_c18</th>\n",
              "      <th>profile_pic_url</th>\n",
              "      <th>should_show_category</th>\n",
              "      <th>should_show_public_contacts</th>\n",
              "      <th>show_account_transparency_details</th>\n",
              "      <th>profile_picture_base64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>deparmedya</td>\n",
              "      <td>3170700063</td>\n",
              "      <td>Depar Medya</td>\n",
              "      <td>#mediaplanning #mediabuying #sosyalmedya</td>\n",
              "      <td>Local business</td>\n",
              "      <td>None</td>\n",
              "      <td>1167</td>\n",
              "      <td>192</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>LOCAL</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     username          id    full_name  \\\n",
              "0  deparmedya  3170700063  Depar Medya   \n",
              "\n",
              "                                  biography   category_name post_count  \\\n",
              "0  #mediaplanning #mediabuying #sosyalmedya  Local business       None   \n",
              "\n",
              "  follower_count following_count is_business_account is_private  ...  \\\n",
              "0           1167             192                True      False  ...   \n",
              "\n",
              "  business_category_name overall_category_name category_enum  \\\n",
              "0                   None                  None         LOCAL   \n",
              "\n",
              "  is_verified_by_mv4b is_regulated_c18  \\\n",
              "0               False            False   \n",
              "\n",
              "                                     profile_pic_url should_show_category  \\\n",
              "0  https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....                 True   \n",
              "\n",
              "  should_show_public_contacts show_account_transparency_details  \\\n",
              "0                        True                              True   \n",
              "\n",
              "                              profile_picture_base64  \n",
              "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
              "\n",
              "[1 rows x 44 columns]"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Profile Dataframe\n",
        "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
        "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)\n",
        "\n",
        "train_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "15tvVohUBHK9",
        "outputId": "15be2533-51b2-41e8-e0f2-26a44933dbae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>id</th>\n",
              "      <th>full_name</th>\n",
              "      <th>biography</th>\n",
              "      <th>category_name</th>\n",
              "      <th>post_count</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>is_business_account</th>\n",
              "      <th>is_private</th>\n",
              "      <th>...</th>\n",
              "      <th>business_category_name</th>\n",
              "      <th>overall_category_name</th>\n",
              "      <th>category_enum</th>\n",
              "      <th>is_verified_by_mv4b</th>\n",
              "      <th>is_regulated_c18</th>\n",
              "      <th>profile_pic_url</th>\n",
              "      <th>should_show_category</th>\n",
              "      <th>should_show_public_contacts</th>\n",
              "      <th>show_account_transparency_details</th>\n",
              "      <th>profile_picture_base64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>beyazyakaliyiz</td>\n",
              "      <td>8634457436</td>\n",
              "      <td>Selam Beyaz YakalÄ±</td>\n",
              "      <td>Beyaz yakalÄ±larÄ±n dÃ¼nyasÄ±na hoÅŸgeldiniz ðŸ˜€ðŸ˜€ðŸ˜€</td>\n",
              "      <td>Personal blog</td>\n",
              "      <td>None</td>\n",
              "      <td>1265</td>\n",
              "      <td>665</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>PERSONAL_BLOG</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>https://instagram.fist6-1.fna.fbcdn.net/v/t51....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         username          id           full_name  \\\n",
              "0  beyazyakaliyiz  8634457436  Selam Beyaz YakalÄ±   \n",
              "\n",
              "                                     biography  category_name post_count  \\\n",
              "0  Beyaz yakalÄ±larÄ±n dÃ¼nyasÄ±na hoÅŸgeldiniz ðŸ˜€ðŸ˜€ðŸ˜€  Personal blog       None   \n",
              "\n",
              "  follower_count following_count is_business_account is_private  ...  \\\n",
              "0           1265             665                True      False  ...   \n",
              "\n",
              "  business_category_name overall_category_name  category_enum  \\\n",
              "0                   None                  None  PERSONAL_BLOG   \n",
              "\n",
              "  is_verified_by_mv4b is_regulated_c18  \\\n",
              "0               False            False   \n",
              "\n",
              "                                     profile_pic_url should_show_category  \\\n",
              "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
              "\n",
              "  should_show_public_contacts show_account_transparency_details  \\\n",
              "0                        True                              True   \n",
              "\n",
              "                              profile_picture_base64  \n",
              "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
              "\n",
              "[1 rows x 44 columns]"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_classification_df:\n",
            "['user_id', 'category']\n",
            "\n",
            "First few rows of the training classification DataFrame:\n",
            "           user_id          category\n",
            "0    taskirancemal  mom and children\n",
            "1    tam_kararinda              food\n",
            "2         spart4nn              food\n",
            "3  sosyalyiyiciler              food\n",
            "4  sonaydizdarahad  mom and children\n",
            "\n",
            "Label distribution:\n",
            "category\n",
            "health and lifestyle    547\n",
            "food                    531\n",
            "tech                    371\n",
            "entertainment           351\n",
            "travel                  316\n",
            "fashion                 316\n",
            "art                     209\n",
            "mom and children        152\n",
            "sports                  133\n",
            "gaming                   20\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Verify Output\n",
        "print(\"Columns in train_classification_df:\")\n",
        "print(train_classification_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of the training classification DataFrame:\")\n",
        "print(train_classification_df.head())\n",
        "\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(train_classification_df['category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_profile_df:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
            "\n",
            "First few rows of train_profile_df:\n",
            "     username          id    full_name  \\\n",
            "0  deparmedya  3170700063  Depar Medya   \n",
            "\n",
            "                                  biography   category_name post_count  \\\n",
            "0  #mediaplanning #mediabuying #sosyalmedya  Local business       None   \n",
            "\n",
            "  follower_count following_count is_business_account is_private  ...  \\\n",
            "0           1167             192                True      False  ...   \n",
            "\n",
            "  business_category_name overall_category_name category_enum  \\\n",
            "0                   None                  None         LOCAL   \n",
            "\n",
            "  is_verified_by_mv4b is_regulated_c18  \\\n",
            "0               False            False   \n",
            "\n",
            "                                     profile_pic_url should_show_category  \\\n",
            "0  https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....                 True   \n",
            "\n",
            "  should_show_public_contacts show_account_transparency_details  \\\n",
            "0                        True                              True   \n",
            "\n",
            "                              profile_picture_base64  \n",
            "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
            "\n",
            "[1 rows x 44 columns]\n",
            "\n",
            "Columns in test_profile_df:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
            "\n",
            "First few rows of test_profile_df:\n",
            "         username          id           full_name  \\\n",
            "0  beyazyakaliyiz  8634457436  Selam Beyaz YakalÄ±   \n",
            "\n",
            "                                     biography  category_name post_count  \\\n",
            "0  Beyaz yakalÄ±larÄ±n dÃ¼nyasÄ±na hoÅŸgeldiniz ðŸ˜€ðŸ˜€ðŸ˜€  Personal blog       None   \n",
            "\n",
            "  follower_count following_count is_business_account is_private  ...  \\\n",
            "0           1265             665                True      False  ...   \n",
            "\n",
            "  business_category_name overall_category_name  category_enum  \\\n",
            "0                   None                  None  PERSONAL_BLOG   \n",
            "\n",
            "  is_verified_by_mv4b is_regulated_c18  \\\n",
            "0               False            False   \n",
            "\n",
            "                                     profile_pic_url should_show_category  \\\n",
            "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
            "\n",
            "  should_show_public_contacts show_account_transparency_details  \\\n",
            "0                        True                              True   \n",
            "\n",
            "                              profile_picture_base64  \n",
            "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
            "\n",
            "[1 rows x 44 columns]\n"
          ]
        }
      ],
      "source": [
        "# Profile Dataframe\n",
        "print(\"Columns in train_profile_df:\")\n",
        "print(train_profile_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of train_profile_df:\")\n",
        "print(train_profile_df.head(1))\n",
        "\n",
        "print(\"\\nColumns in test_profile_df:\")\n",
        "print(test_profile_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of test_profile_df:\")\n",
        "print(test_profile_df.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_profile_df after dropping:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'bio_links', 'is_professional_account', 'business_category_name', 'overall_category_name']\n",
            "\n",
            "Columns in test_profile_df after dropping:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'bio_links', 'is_professional_account', 'business_category_name', 'overall_category_name']\n"
          ]
        }
      ],
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = [\n",
        "    'highlight_reel_count', 'entities', 'ai_agent_type', 'fb_profile_biolink',\n",
        "    'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid',\n",
        "    'has_clips', 'hide_like_and_view_counts', 'is_supervision_enabled',\n",
        "    'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user',\n",
        "    'is_embeds_disabled', 'is_joined_recently', 'business_address_json',\n",
        "    'business_contact_method', 'business_email', 'business_phone_number',\n",
        "    'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18',\n",
        "    'profile_pic_url', 'should_show_category', 'should_show_public_contacts',\n",
        "    'show_account_transparency_details', 'profile_picture_base64'\n",
        "]\n",
        "\n",
        "# Dropping specified columns from train_profile_df\n",
        "train_profile_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Dropping specified columns from test_profile_df\n",
        "test_profile_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Verify columns in train_profile_df after dropping\n",
        "print(\"Columns in train_profile_df after dropping:\")\n",
        "print(train_profile_df.columns.tolist())\n",
        "\n",
        "# Verify columns in test_profile_df after dropping\n",
        "print(\"\\nColumns in test_profile_df after dropping:\")\n",
        "print(test_profile_df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Z5UY0eYLsoTr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "Best Params: {'clf__C': 10, 'preprocessor__bio_tfidf__ngram_range': (1, 1), 'preprocessor__captions_tfidf__ngram_range': (1, 1)}\n",
            "Best CV Accuracy: 0.6452599388379205\n",
            "Training Accuracy: 1.0\n",
            "\n",
            "Training Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                 art       1.00      1.00      1.00       158\n",
            "       entertainment       1.00      1.00      1.00       274\n",
            "             fashion       1.00      1.00      1.00       251\n",
            "                food       1.00      1.00      1.00       417\n",
            "              gaming       1.00      1.00      1.00        12\n",
            "health and lifestyle       1.00      1.00      1.00       422\n",
            "    mom and children       1.00      1.00      1.00       120\n",
            "              sports       1.00      1.00      1.00        98\n",
            "                tech       1.00      1.00      1.00       290\n",
            "              travel       1.00      1.00      1.00       247\n",
            "\n",
            "            accuracy                           1.00      2289\n",
            "           macro avg       1.00      1.00      1.00      2289\n",
            "        weighted avg       1.00      1.00      1.00      2289\n",
            "\n",
            "Validation Accuracy: 0.6527050610820244\n",
            "\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                 art       0.36      0.30      0.33        40\n",
            "       entertainment       0.48      0.46      0.47        69\n",
            "             fashion       0.65      0.67      0.66        63\n",
            "                food       0.86      0.85      0.85       104\n",
            "              gaming       0.00      0.00      0.00         3\n",
            "health and lifestyle       0.62      0.71      0.66       105\n",
            "    mom and children       0.59      0.43      0.50        30\n",
            "              sports       0.93      0.58      0.72        24\n",
            "                tech       0.64      0.77      0.70        73\n",
            "              travel       0.69      0.68      0.68        62\n",
            "\n",
            "            accuracy                           0.65       573\n",
            "           macro avg       0.58      0.55      0.56       573\n",
            "        weighted avg       0.65      0.65      0.65       573\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Preprocessing Function\n",
        "import re\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    text = text.casefold()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼0-9\\s#@]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Build Corpus and Labels\n",
        "corpus = []\n",
        "train_usernames = []\n",
        "\n",
        "for username, posts in username2posts_train.items():\n",
        "    train_usernames.append(username)\n",
        "    cleaned_captions = []\n",
        "    for post in posts:\n",
        "        post_caption = post.get(\"caption\", \"\")\n",
        "        if post_caption is None:\n",
        "            continue\n",
        "        post_caption = preprocess_text(post_caption)\n",
        "        if post_caption != \"\":\n",
        "            cleaned_captions.append(post_caption)\n",
        "    user_post_captions = \"\\n\".join(cleaned_captions)\n",
        "    corpus.append(user_post_captions)\n",
        "\n",
        "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
        "\n",
        "# Incorporate Metadata\n",
        "records = []\n",
        "for idx, username in enumerate(train_usernames):\n",
        "    profile = username2profile_train.get(username, {})\n",
        "    biography_text = str(profile.get(\"biography\", \"\") or \"\")\n",
        "    follower_count = profile.get(\"follower_count\", 0)\n",
        "    following_count = profile.get(\"following_count\", 0)\n",
        "    post_count = profile.get(\"post_count\", 0) if profile.get(\"post_count\") else 0\n",
        "    row_dict = {\n",
        "        \"username\": username,\n",
        "        \"captions\": corpus[idx],\n",
        "        \"biography\": biography_text,\n",
        "        \"follower_count\": follower_count,\n",
        "        \"following_count\": following_count,\n",
        "        \"post_count\": post_count,\n",
        "        \"label\": y_train[idx]\n",
        "    }\n",
        "    records.append(row_dict)\n",
        "\n",
        "train_full_df = pd.DataFrame(records)\n",
        "\n",
        "# Define Pipeline Components\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Train-Validation Split\n",
        "X = train_full_df.drop(columns=[\"label\"])\n",
        "y = train_full_df[\"label\"]\n",
        "\n",
        "x_train_df, x_val_df, y_train_labels, y_val_labels = train_test_split(\n",
        "    X, \n",
        "    y, \n",
        "    test_size=0.2, \n",
        "    stratify=y, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define ColumnTransformer\n",
        "numeric_features = [\"follower_count\", \"following_count\", \"post_count\"]\n",
        "text_features_caps = \"captions\"\n",
        "text_features_bio = \"biography\"\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"captions_tfidf\", TfidfVectorizer(\n",
        "             stop_words=turkish_stopwords, \n",
        "             max_features=5000\n",
        "         ), text_features_caps),\n",
        "        (\"bio_tfidf\", TfidfVectorizer(\n",
        "             stop_words=turkish_stopwords, \n",
        "             max_features=5000\n",
        "         ), text_features_bio),\n",
        "        (\"numeric_scaler\", MinMaxScaler(), numeric_features)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# Build ImbPipeline (SMOTE + Classifier)\n",
        "pipeline = ImbPipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=42, sampling_strategy=\"auto\")),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        class_weight='balanced',\n",
        "        solver='liblinear',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Define a param grid\n",
        "param_grid = {\n",
        "    \"preprocessor__captions_tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"preprocessor__bio_tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"clf__C\": [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Initialize and fit GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(x_train_df, y_train_labels)\n",
        "\n",
        "print(\"Best Params:\", grid_search.best_params_)\n",
        "print(\"Best CV Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "\n",
        "# **New Section: Evaluate on Training Data**\n",
        "# Predict on the training data\n",
        "y_train_pred = best_pipeline.predict(x_train_df)\n",
        "\n",
        "# Calculate training accuracy\n",
        "train_acc = accuracy_score(y_train_labels, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "\n",
        "# Generate and print the training classification report\n",
        "print(\"\\nTraining Classification Report:\\n\",\n",
        "      classification_report(y_train_labels, y_train_pred, zero_division=0))\n",
        "\n",
        "# **End of New Section**\n",
        "\n",
        "# Predict on the validation data\n",
        "y_val_pred = best_pipeline.predict(x_val_df)\n",
        "\n",
        "# Calculate validation accuracy\n",
        "val_acc = accuracy_score(y_val_labels, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_acc)\n",
        "\n",
        "# Generate and print the validation classification report\n",
        "print(\"\\nClassification Report:\\n\",\n",
        "      classification_report(y_val_labels, y_val_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2289 entries, 1238 to 1441\n",
            "Data columns (total 6 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   username         2289 non-null   object \n",
            " 1   captions         2289 non-null   object \n",
            " 2   biography        2289 non-null   object \n",
            " 3   follower_count   2289 non-null   int64  \n",
            " 4   following_count  2289 non-null   int64  \n",
            " 5   post_count       2289 non-null   float64\n",
            "dtypes: float64(1), int64(2), object(3)\n",
            "memory usage: 125.2+ KB\n",
            "None\n",
            "\n",
            "Missing values in training data:\n",
            "username           0\n",
            "captions           0\n",
            "biography          0\n",
            "follower_count     0\n",
            "following_count    0\n",
            "post_count         0\n",
            "dtype: int64\n",
            "\n",
            "Performing cross-validation with tqdm progress bar...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cross-Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [30:12<00:00, 362.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cross-validation scores: [0.6637554585152838, 0.6637554585152838, 0.6200873362445415, 0.6375545851528385, 0.6323851203501094]\n",
            "Mean CV accuracy: 0.644 (+/- 0.035)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[94], line 224\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean CV accuracy: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m (+/- \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    220\u001b[0m     np\u001b[38;5;241m.\u001b[39mmean(scores), np\u001b[38;5;241m.\u001b[39mstd(scores) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    221\u001b[0m ))\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# 5) Train final model on the entire training set\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m \u001b[43menhanced_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_labels_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# 6) Evaluate on Training Data\u001b[39;00m\n\u001b[0;32m    227\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m enhanced_pipeline\u001b[38;5;241m.\u001b[39mpredict(x_train_df)\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\pipeline.py:526\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    521\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    522\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    523\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    524\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    525\u001b[0m         )\n\u001b[1;32m--> 526\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:349\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    347\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:81\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m     )\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVoting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:36\u001b[0m, in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m---> 36\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:532\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:610\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    603\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    604\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    605\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    606\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    607\u001b[0m         )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 610\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 245\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    250\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    258\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ------------------- Imports -------------------\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# scikit-learn utilities\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# For progress bar in cross-validation\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------- Enhanced Text Preprocessing -------------------\n",
        "def advanced_text_preprocessing(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    text = text.lower()\n",
        "    \n",
        "    # Extract hashtags and mentions\n",
        "    hashtags = re.findall(r'#\\w+', text)\n",
        "    mentions = re.findall(r'@\\w+', text)\n",
        "    \n",
        "    # Densities\n",
        "    text_length = len(text)\n",
        "    hashtag_density = len(hashtags) / (text_length + 1)\n",
        "    mention_density = len(mentions) / (text_length + 1)\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼0-9\\s#@]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Add density info as extra pseudo-tokens\n",
        "    density_info = f\" hashtag_density_{hashtag_density:.2f} mention_density_{mention_density:.2f}\"\n",
        "    \n",
        "    # Re-append the extracted hashtags and mentions\n",
        "    return f\"{text} {' '.join(hashtags)} {' '.join(mentions)} {density_info}\".strip()\n",
        "\n",
        "# ------------------- Enhanced Numeric Feature Extraction -------------------\n",
        "def extract_numeric_features(X):\n",
        "    # If X is a DataFrame, ensure columns exist\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        cols = ['follower_count', 'following_count', 'post_count']\n",
        "        if 'account_age_days' in X.columns:\n",
        "            cols.append('account_age_days')\n",
        "        \n",
        "        df = X[cols].copy()\n",
        "    else:\n",
        "        # If it's a NumPy array\n",
        "        df = pd.DataFrame(\n",
        "            X, \n",
        "            columns=['follower_count', 'following_count', 'post_count', 'account_age_days'][:X.shape[1]]\n",
        "        )\n",
        "    \n",
        "    df = df.fillna(0)\n",
        "    \n",
        "    df['following_count_safe'] = df['following_count'].replace(0, 1)\n",
        "    df['follower_count_safe']  = df['follower_count'].replace(0, 1)\n",
        "    \n",
        "    df['follower_ratio']   = df['follower_count'] / df['following_count_safe']\n",
        "    df['post_density']     = df['post_count'] / df['follower_count_safe']\n",
        "    df['engagement_score'] = np.log1p(df['follower_count']) * np.log1p(df['post_count'])\n",
        "    \n",
        "    if 'account_age_days' in df.columns:\n",
        "        df['activity_ratio'] = df['post_count'] / (df['account_age_days'] + 1)\n",
        "    else:\n",
        "        df['activity_ratio'] = df['post_count']\n",
        "    \n",
        "    df['follower_growth_rate'] = df['follower_count'] / (df['post_count'] + 1)\n",
        "    df['relative_engagement']  = (df['follower_count'] * df['post_count']) / (df['following_count_safe'])\n",
        "    \n",
        "    # Log transforms\n",
        "    df['log_followers']  = np.log1p(df['follower_count'])\n",
        "    df['log_following']  = np.log1p(df['following_count'])\n",
        "    df['log_posts']      = np.log1p(df['post_count'])\n",
        "    \n",
        "    df.drop(['following_count_safe','follower_count_safe'], axis=1, inplace=True)\n",
        "    \n",
        "    return df.values\n",
        "\n",
        "# ------------------- Build Pipeline -------------------\n",
        "def build_enhanced_pipeline(stopwords_list):\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('captions_tfidf', \n",
        "             TfidfVectorizer(\n",
        "                 stop_words=stopwords_list,\n",
        "                 max_features=3000,   \n",
        "                 ngram_range=(1, 1),  \n",
        "                 min_df=2,\n",
        "                 max_df=0.95\n",
        "             ), \n",
        "             'captions_clean'),\n",
        "            \n",
        "            ('bio_tfidf', \n",
        "             TfidfVectorizer(\n",
        "                 stop_words=stopwords_list,\n",
        "                 max_features=2000,   \n",
        "                 ngram_range=(1, 1),  \n",
        "                 min_df=2,\n",
        "                 max_df=0.95\n",
        "             ), \n",
        "             'biography_clean'),\n",
        "            \n",
        "            ('numeric', \n",
        "             Pipeline([\n",
        "                 ('feat_eng', FunctionTransformer(extract_numeric_features)),\n",
        "                 ('scaler', MinMaxScaler())\n",
        "             ]), \n",
        "             ['follower_count', 'following_count', 'post_count']\n",
        "             # add 'account_age_days' if available\n",
        "            )\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    \n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=100,    \n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        max_features='sqrt',\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=50,     \n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    lr = LogisticRegression(\n",
        "        C=2.0,\n",
        "        class_weight='balanced',\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', rf),\n",
        "            ('gb', gb),\n",
        "            ('lr', lr)\n",
        "        ],\n",
        "        voting='soft'\n",
        "    )\n",
        "    \n",
        "    pipeline = ImbPipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('classifier', ensemble)\n",
        "    ])\n",
        "    \n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# ------------------- Example Usage -------------------\n",
        "print(\"Training data info:\")\n",
        "print(x_train_df.info())\n",
        "print(\"\\nMissing values in training data:\")\n",
        "print(x_train_df.isnull().sum())\n",
        "\n",
        "# 1) Preprocess text once\n",
        "x_train_df['captions_clean'] = x_train_df['captions'].apply(advanced_text_preprocessing)\n",
        "x_train_df['biography_clean'] = x_train_df['biography'].apply(advanced_text_preprocessing)\n",
        "\n",
        "x_val_df['captions_clean'] = x_val_df['captions'].apply(advanced_text_preprocessing)\n",
        "x_val_df['biography_clean'] = x_val_df['biography'].apply(advanced_text_preprocessing)\n",
        "\n",
        "# 2) Convert y_train_labels to NumPy for direct integer indexing\n",
        "y_train_labels_array = y_train_labels.values\n",
        "\n",
        "# 3) Build pipeline\n",
        "enhanced_pipeline = build_enhanced_pipeline(stopwords_list=turkish_stopwords)\n",
        "\n",
        "# 4) Manual Cross-Validation with tqdm progress bar\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "\n",
        "print(\"\\nPerforming cross-validation with tqdm progress bar...\")\n",
        "for fold_idx, (train_idx, test_idx) in enumerate(\n",
        "    tqdm(cv.split(x_train_df, y_train_labels_array), \n",
        "         total=cv.get_n_splits(), \n",
        "         desc='Cross-Validation')\n",
        "):\n",
        "    # Split the data\n",
        "    X_fold_train = x_train_df.iloc[train_idx]\n",
        "    y_fold_train = y_train_labels_array[train_idx]\n",
        "    X_fold_test  = x_train_df.iloc[test_idx]\n",
        "    y_fold_test  = y_train_labels_array[test_idx]\n",
        "    \n",
        "    # Fit on this fold\n",
        "    enhanced_pipeline.fit(X_fold_train, y_fold_train)\n",
        "    \n",
        "    # Predict\n",
        "    preds = enhanced_pipeline.predict(X_fold_test)\n",
        "    \n",
        "    # Compute accuracy\n",
        "    acc = (preds == y_fold_test).mean()\n",
        "    scores.append(acc)\n",
        "\n",
        "print(\"\\nCross-validation scores:\", scores)\n",
        "print(\"Mean CV accuracy: {:.3f} (+/- {:.3f})\".format(\n",
        "    np.mean(scores), np.std(scores) * 2\n",
        "))\n",
        "\n",
        "# 5) Train final model on the entire training set\n",
        "enhanced_pipeline.fit(x_train_df, y_train_labels_array)\n",
        "\n",
        "# 6) Evaluate on Training Data\n",
        "y_train_pred = enhanced_pipeline.predict(x_train_df)\n",
        "print(\"\\nTraining Classification Report:\")\n",
        "print(classification_report(y_train_labels_array, y_train_pred))\n",
        "\n",
        "# 7) Evaluate on Validation Data\n",
        "#    => Implement \"if username in training\" logic here.\n",
        "\n",
        "# First, build a dictionary from training usernames to labels\n",
        "# We rely on the fact that x_train_df still has \"username\" column\n",
        "train_user2label = dict(zip(x_train_df[\"username\"], y_train_labels_array))\n",
        "\n",
        "# Now we define a function that uses the dictionary first:\n",
        "def predict_with_username_lookup(df, pipeline):\n",
        "    \"\"\"\n",
        "    If username is in training (train_user2label), \n",
        "    use that known label.\n",
        "    Otherwise, call pipeline.predict.\n",
        "    \"\"\"\n",
        "    # Identify known vs. unknown usernames\n",
        "    known_mask = df[\"username\"].isin(train_user2label)\n",
        "    \n",
        "    # For the unknown subset, run the pipeline\n",
        "    df_unknown = df[~known_mask].copy()\n",
        "    \n",
        "    # We must drop the columns that are not used by the pipeline \n",
        "    # (e.g., \"username\", \"captions_clean\", \"biography_clean\" are used internally \n",
        "    # by pipeline, so let's keep them. Actually the pipeline transforms \"captions_clean\" and \"biography_clean\".)\n",
        "    # The pipeline expects columns [follower_count, following_count, post_count, \n",
        "    #  captions_clean, biography_clean].\n",
        "    # So let's do NOT drop them. We only drop \"username\" if needed. The pipeline does remainder='drop' anyway.\n",
        "    \n",
        "    X_unknown = df_unknown.drop(columns=[\"username\"])  # pipeline doesn't need \"username\"\n",
        "    \n",
        "    # Predict\n",
        "    y_pred_unknown = pipeline.predict(X_unknown)\n",
        "    \n",
        "    # Rebuild final predictions in the original df order\n",
        "    y_pred_final = []\n",
        "    j = 0  # index for unknown predictions\n",
        "    \n",
        "    for idx in df.index:\n",
        "        if known_mask.loc[idx]:\n",
        "            # known => use label from training\n",
        "            user = df.loc[idx, \"username\"]\n",
        "            y_pred_final.append(train_user2label[user])\n",
        "        else:\n",
        "            # unknown => use pipeline result\n",
        "            y_pred_final.append(y_pred_unknown[j])\n",
        "            j += 1\n",
        "    \n",
        "    return np.array(y_pred_final)\n",
        "\n",
        "# Make predictions on the validation data, using the check\n",
        "# Ensure x_val_df has 'username' so we can do the lookup\n",
        "y_val_pred_custom = predict_with_username_lookup(x_val_df, enhanced_pipeline)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nValidation Classification Report (with username lookup):\")\n",
        "print(classification_report(y_val_labels, y_val_pred_custom))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading test data...\n",
            "Preparing features for 999 test users...\n",
            "Preprocessing test data...\n",
            "Making predictions on test data...\n",
            "Predictions saved to: c:\\Users\\itsmm\\OneDrive\\Desktop\\CS412\\CS412-InstagramInfluencersAnalysis\\data\\output\\prediction-classification-round1.json\n"
          ]
        }
      ],
      "source": [
        "def load_test_data():\n",
        "    \"\"\"Load and prepare test data\"\"\"\n",
        "    # Define file paths\n",
        "    current_notebook_dir = os.getcwd()\n",
        "    repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "    data_dir = os.path.join(repo_dir, 'data')\n",
        "    testing_dir = os.path.join(data_dir, 'testing')\n",
        "    test_data_path = os.path.join(testing_dir, 'test-classification-round1.dat')\n",
        "    \n",
        "    # Read test usernames\n",
        "    test_unames = []\n",
        "    with open(test_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "        for line in fh:\n",
        "            username = line.strip()\n",
        "            if username != \"screenname\":  # Skip screenname\n",
        "                test_unames.append(username)\n",
        "    \n",
        "    return test_unames\n",
        "\n",
        "def prepare_test_features(test_unames, username2posts_test, username2profile_test):\n",
        "    \"\"\"Prepare features for test data\"\"\"\n",
        "    test_data = []\n",
        "    for username in test_unames:\n",
        "        # Get posts and profile data\n",
        "        posts = username2posts_test.get(username, [])\n",
        "        profile = username2profile_test.get(username, {})\n",
        "        \n",
        "        # Prepare text data\n",
        "        captions = \" \".join([post.get('caption', '') for post in posts if post.get('caption')])\n",
        "        biography = profile.get('biography', '')\n",
        "        \n",
        "        # Prepare numeric data\n",
        "        row_data = {\n",
        "            'username': username,\n",
        "            'captions': captions,\n",
        "            'biography': biography,\n",
        "            'follower_count': profile.get('follower_count', 0),\n",
        "            'following_count': profile.get('following_count', 0),\n",
        "            'post_count': profile.get('post_count', 0)\n",
        "        }\n",
        "        test_data.append(row_data)\n",
        "    \n",
        "    return pd.DataFrame(test_data)\n",
        "\n",
        "# Add this category mapping dictionary at the top level\n",
        "CATEGORY_MAPPING = {\n",
        "    'art': 'Art',\n",
        "    'entertainment': 'Entertainment',\n",
        "    'fashion': 'Fashion',\n",
        "    'food': 'Food',\n",
        "    'gaming': 'Gaming',\n",
        "    'health and lifestyle': 'Health and Lifestyle',\n",
        "    'mom and children': 'Mom and Children',\n",
        "    'sports': 'Sports',\n",
        "    'tech': 'Tech',\n",
        "    'travel': 'Travel'\n",
        "}\n",
        "\n",
        "def save_predictions(predictions, usernames, output_dir):\n",
        "    \"\"\"Save predictions to JSON file with proper capitalization\"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Create output dictionary with proper capitalization\n",
        "    output = {}\n",
        "    for username, prediction in zip(usernames, predictions):\n",
        "        # Map the lowercase prediction to its proper capitalized form\n",
        "        capitalized_prediction = CATEGORY_MAPPING.get(prediction.lower(), prediction)\n",
        "        output[username] = capitalized_prediction\n",
        "    \n",
        "    # Save to file\n",
        "    output_path = os.path.join(output_dir, 'prediction-classification-round1.json')\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(output, f, indent=4)\n",
        "    \n",
        "    print(f\"Predictions saved to: {output_path}\")\n",
        "\n",
        "def main():\n",
        "    \n",
        "    # Load and prepare test data\n",
        "    print(\"\\nLoading test data...\")\n",
        "    test_unames = load_test_data()\n",
        "    \n",
        "    print(f\"Preparing features for {len(test_unames)} test users...\")\n",
        "    x_test_df = prepare_test_features(test_unames, username2posts_test, username2profile_test)\n",
        "    \n",
        "    # Preprocess test text data\n",
        "    print(\"Preprocessing test data...\")\n",
        "    x_test_df['captions_clean'] = x_test_df['captions'].apply(advanced_text_preprocessing)\n",
        "    x_test_df['biography_clean'] = x_test_df['biography'].apply(advanced_text_preprocessing)\n",
        "    \n",
        "    # Make predictions on test data\n",
        "    print(\"Making predictions on test data...\")\n",
        "    test_predictions = predict_with_username_lookup(x_test_df, enhanced_pipeline)\n",
        "    \n",
        "    # Save predictions\n",
        "    output_dir = os.path.join(os.path.dirname(os.getcwd()), 'data', 'output')\n",
        "    save_predictions(test_predictions.tolist(), test_unames, output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khu0eryhNZNN"
      },
      "source": [
        "# Naive Base Classifier\n",
        "\n",
        "### Now we can pass the numerical values to a classifier, Let's try Naive Base!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC7KXsQZL7Kp"
      },
      "source": [
        "# Like Count Prediction\n",
        "\n",
        "\n",
        "Here, we use the average like_count of the user's previous posts to predict each post's like_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "nolpagasSuBq"
      },
      "outputs": [],
      "source": [
        "def predict_like_count(username, current_post=None):\n",
        "  def get_avg_like_count(posts:list):\n",
        "    total = 0.\n",
        "    for post in posts:\n",
        "      if current_post is not None and post[\"id\"] == current_post[\"id\"]:\n",
        "        continue\n",
        "\n",
        "      like_count = post.get(\"like_count\", 0)\n",
        "      if like_count is None:\n",
        "        like_count = 0\n",
        "      total += like_count\n",
        "\n",
        "    if len(posts) == 0:\n",
        "      return 0.\n",
        "\n",
        "    return total / len(posts)\n",
        "\n",
        "  if username in username2posts_train:\n",
        "    return get_avg_like_count(username2posts_train[username])\n",
        "  elif username in username2posts_test:\n",
        "    return get_avg_like_count(username2posts_test[username])\n",
        "  else:\n",
        "    print(f\"No data available for {username}\")\n",
        "    return -1\n",
        "  \n",
        "def log_mse_like_counts(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculate the Log Mean Squared Error (Log MSE) for like counts (log(like_count + 1)).\n",
        "\n",
        "  Parameters:\n",
        "  - y_true: array-like, actual like counts\n",
        "  - y_pred: array-like, predicted like counts\n",
        "\n",
        "  Returns:\n",
        "  - log_mse: float, Log Mean Squared Error\n",
        "  \"\"\"\n",
        "  # Ensure inputs are numpy arrays\n",
        "  y_true = np.array(y_true)\n",
        "  y_pred = np.array(y_pred)\n",
        "\n",
        "  # Log transformation: log(like_count + 1)\n",
        "  log_y_true = np.log1p(y_true)\n",
        "  log_y_pred = np.log1p(y_pred)\n",
        "\n",
        "  # Compute squared errors\n",
        "  squared_errors = (log_y_true - log_y_pred) ** 2\n",
        "\n",
        "  # Return the mean of squared errors\n",
        "  return np.mean(squared_errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2741/2741 [00:01<00:00, 1848.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log MSE Train= 1.1623716815622944\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def enhanced_predict_like_count(username, current_post=None):\n",
        "    \"\"\"Enhanced like count prediction using average likes and time-based adjustments\"\"\"\n",
        "    \n",
        "    # Get base prediction from average method\n",
        "    base_prediction = predict_like_count(username, current_post)\n",
        "    \n",
        "    # If no current post, return base prediction\n",
        "    if not current_post:\n",
        "        return base_prediction\n",
        "    \n",
        "    # Safe get function for numeric values\n",
        "    def safe_get(dict_obj, key, default=0):\n",
        "        value = dict_obj.get(key, default)\n",
        "        return default if value is None else value\n",
        "    \n",
        "    # Extract timestamp if available\n",
        "    adjustment = 1.0\n",
        "\n",
        "    media_type = safe_get(current_post, 'media_type', 'PHOTO')\n",
        "    try:\n",
        "        timestamp = safe_get(current_post, 'timestamp', '00:00:00')\n",
        "        hour = int(timestamp.split()[1].split(':')[0])\n",
        "        \n",
        "        # Time of day adjustment\n",
        "        prime_time_hours = {19, 20, 21, 22}  # Evening hours\n",
        "        if hour in prime_time_hours:\n",
        "            adjustment *= 1.1\n",
        "        elif 9 <= hour <= 17:  # Work hours\n",
        "            adjustment *= 0.9\n",
        "        elif 0 <= hour <= 5:   # Late night\n",
        "            adjustment *= 0.8\n",
        "        # Get user's recent posts (last 5)\n",
        "        recent_posts = []\n",
        "        if username in username2posts_train:\n",
        "            posts = username2posts_train[username]\n",
        "            recent_posts = sorted(posts, key=lambda x: x.get('timestamp', ''), reverse=True)[:5]\n",
        "        \n",
        "        if recent_posts:\n",
        "            recent_likes = [p.get('like_count', 0) for p in recent_posts if p.get('like_count') is not None]\n",
        "            if recent_likes:\n",
        "                recent_avg = sum(recent_likes) / len(recent_likes)\n",
        "                overall_avg = base_prediction\n",
        "                \n",
        "                # If recent performance is different from overall\n",
        "                if recent_avg > overall_avg * 1.2:  # Recent posts doing better\n",
        "                    adjustment *= 1.1\n",
        "                elif recent_avg < overall_avg * 0.8:  # Recent posts doing worse\n",
        "                    adjustment *= 0.9\n",
        "        \n",
        "        if username in username2posts_train:\n",
        "            posts = username2posts_train[username]\n",
        "            media_type_likes = {}\n",
        "            \n",
        "            for post in posts:\n",
        "                post_type = post.get('media_type', 'PHOTO')\n",
        "                if post.get('like_count') is not None:\n",
        "                    if post_type not in media_type_likes:\n",
        "                        media_type_likes[post_type] = []\n",
        "                    media_type_likes[post_type].append(post['like_count'])\n",
        "            \n",
        "            # If this media type historically performs better/worse for this user\n",
        "            if media_type in media_type_likes and len(media_type_likes[media_type]) > 0:\n",
        "                type_avg = sum(media_type_likes[media_type]) / len(media_type_likes[media_type])\n",
        "                all_likes = [l for likes in media_type_likes.values() for l in likes]\n",
        "                overall_avg = sum(all_likes) / len(all_likes)\n",
        "                \n",
        "                if type_avg > overall_avg:\n",
        "                    adjustment *= 1.1\n",
        "                elif type_avg < overall_avg:\n",
        "                    adjustment *= 0.9\n",
        "    except:\n",
        "        pass  # If timestamp parsing fails, keep original adjustment\n",
        "    \n",
        "    # Apply adjustment to base prediction\n",
        "    final_prediction = base_prediction * adjustment\n",
        "    \n",
        "    return int(final_prediction)\n",
        "\n",
        "# Modified evaluation code\n",
        "y_like_count_train_true = []\n",
        "y_like_count_train_pred = []\n",
        "\n",
        "for uname, posts in tqdm(username2posts_train.items(), desc=\"Evaluating predictions\"):\n",
        "    for post in posts:\n",
        "        pred_val = enhanced_predict_like_count(uname, post)\n",
        "        true_val = post.get(\"like_count\", 0)\n",
        "        if true_val is None:\n",
        "            true_val = 0\n",
        "            \n",
        "        y_like_count_train_true.append(true_val)\n",
        "        y_like_count_train_pred.append(pred_val)\n",
        "\n",
        "print(f\"Log MSE Train= {log_mse_like_counts(y_like_count_train_true, y_like_count_train_pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating validation predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2674/2674 [00:00<00:00, 7082.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log MSE Validation= 0.7795603524418687\n"
          ]
        }
      ],
      "source": [
        "# Initialize lists to store true and predicted like counts for validation\n",
        "y_like_count_val_true = []\n",
        "y_like_count_val_pred = []\n",
        "\n",
        "# Iterate over each user and their posts in the validation dataset\n",
        "for uname, posts in tqdm(username2posts_test.items(), desc=\"Evaluating validation predictions\"):\n",
        "    for post in posts:\n",
        "        # Make prediction using the enhanced_predict_like_count function\n",
        "        pred_val = enhanced_predict_like_count(uname, post)\n",
        "        \n",
        "        # Extract the true like count, defaulting to 0 if not available\n",
        "        true_val = post.get(\"like_count\", 0)\n",
        "        if true_val is None:\n",
        "            true_val = 0\n",
        "        \n",
        "        # Append the true and predicted values to the respective lists\n",
        "        y_like_count_val_true.append(true_val)\n",
        "        y_like_count_val_pred.append(pred_val)\n",
        "\n",
        "# Compute the Log Mean Squared Error for the validation set\n",
        "validation_log_mse = log_mse_like_counts(y_like_count_val_true, y_like_count_val_pred)\n",
        "\n",
        "# Print the validation score\n",
        "print(f\"Log MSE Validation= {validation_log_mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "187267\n"
          ]
        }
      ],
      "source": [
        "combined_ground_truth = dict()\n",
        "\n",
        "for username, posts in username2posts_train.items():\n",
        "    for post in posts:\n",
        "        if 'like_count' in post:\n",
        "            combined_ground_truth[post[\"id\"]] = post[\"like_count\"]\n",
        "\n",
        "for username, posts in username2posts_test.items():\n",
        "    for post in posts:\n",
        "        if 'like_count' in post:\n",
        "            combined_ground_truth[post[\"id\"]] = post[\"like_count\"]\n",
        "\n",
        "print(len(combined_ground_truth))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F02V1wO-WBMV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Statistics:\n",
            "Total posts processed: 3000\n",
            "Posts with ground truth: 0\n",
            "Posts using predictions: 3000\n",
            "\n",
            "Processed data saved to: c:\\Users\\itsmm\\OneDrive\\Desktop\\CS412\\CS412-InstagramInfluencersAnalysis\\data\\output\\prediction-regression-round1.json\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the testing directory\n",
        "testing_dir = os.path.join(data_dir, 'testing')\n",
        "\n",
        "# File path for 'test-regression-round1.jsonl'\n",
        "# Modified output processing\n",
        "# File path for 'test-regression-round1.jsonl'\n",
        "test_dataset_path = os.path.join(testing_dir, 'test-regression-round1.jsonl')\n",
        "\n",
        "# File path for output\n",
        "output_dir = os.path.join(data_dir, 'output')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_file_path = os.path.join(output_dir, 'prediction-regression-round1.json')\n",
        "\n",
        "# First, create a dictionary of known post likes from our training data\n",
        "known_post_likes = {}\n",
        "for username, posts in username2posts_train.items():\n",
        "    for post in posts:\n",
        "        if post.get('id') and post.get('like_count') is not None:\n",
        "            known_post_likes[post['id']] = post.get('like_count')\n",
        "\n",
        "# Process the Test Dataset\n",
        "output_list = []\n",
        "\n",
        "with open(test_dataset_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "        \n",
        "        # Get prediction\n",
        "        pred_val = predict_like_count(sample[\"username\"])\n",
        "        \n",
        "        # Check if we have ground truth for this post ID\n",
        "        if sample['id'] in known_post_likes:\n",
        "            # Use ground truth value\n",
        "            like_count = known_post_likes[sample['id']]\n",
        "            print(f\"Found ground truth for post {sample['id']}: {like_count} (predicted: {pred_val})\")\n",
        "        else:\n",
        "            # Use prediction\n",
        "            like_count = int(pred_val)\n",
        "        \n",
        "        # Create simplified output dictionary\n",
        "        output_dict = {\n",
        "            'id': sample['id'],\n",
        "            'like_count': like_count,\n",
        "            'username': sample['username'],\n",
        "            'media_type': sample.get('media_type', ''),\n",
        "            'comments_count': sample.get('comments_count', 0),\n",
        "            'timestamp': sample.get('timestamp', ''),\n",
        "            'media_url': sample.get('media_url', None)\n",
        "        }\n",
        "        output_list.append(output_dict)\n",
        "\n",
        "# Save just the id and like_count to the JSON file\n",
        "predictions_dict = {item['id']: item['like_count'] for item in output_list}\n",
        "\n",
        "# Print statistics about ground truth usage\n",
        "ground_truth_count = sum(1 for item in output_list if item['id'] in known_post_likes)\n",
        "total_count = len(output_list)\n",
        "print(f\"\\nStatistics:\")\n",
        "print(f\"Total posts processed: {total_count}\")\n",
        "print(f\"Posts with ground truth: {ground_truth_count}\")\n",
        "print(f\"Posts using predictions: {total_count - ground_truth_count}\")\n",
        "\n",
        "# Save to file\n",
        "with open(output_file_path, \"wt\", encoding=\"utf-8\") as of:\n",
        "    json.dump(predictions_dict, of, indent=4)\n",
        "\n",
        "print(f\"\\nProcessed data saved to: {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2741"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(username2posts_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'caption': 'Bu diyaloÄŸun yaÅŸanmadÄ±ÄŸÄ± bir online toplantÄ± olmaz olamaz ðŸ˜‚',\n",
              "  'comments_count': 0,\n",
              "  'id': '17934192878560092',\n",
              "  'like_count': 15,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/343469843_2483419311825645_4770791756841048240_n.jpg?_nc_cat=102&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=b07wdllqspYAX-vSh_G&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfBSfNqPoTZAdGLuFPzSbw26c9ILxACVNP46j8TZjUkDfA&oe=655301C3',\n",
              "  'timestamp': '2023-04-26 18:12:46'},\n",
              " {'caption': 'Evet Ocak ayÄ±nda beyaz yakalÄ± whatsup gruplarÄ±nda en Ã§ok sorulan soru, ðŸ˜€ hayÄ±r post gecikmeli deÄŸil, hala Ã¶ÄŸrenememiÅŸ olan binlerce kiÅŸi olduÄŸunu sÃ¶yleyebilirim ama ispat edememðŸ™ˆðŸ˜€',\n",
              "  'comments_count': 2,\n",
              "  'id': '17984334430863265',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/328013999_490679746575773_1617516856108777150_n.jpg?_nc_cat=111&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=7JGwF_Xm0VoAX-R1UK4&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCV27hiwpEoY9sKqr_pvpGo1aGxHimmr167X05Bbj_tzQ&oe=65542E69',\n",
              "  'timestamp': '2023-01-30 22:23:39'},\n",
              " {'caption': 'Yine yuzlercesini gorecegimiz maillerden biri ðŸ˜€ðŸ™ˆ',\n",
              "  'comments_count': 1,\n",
              "  'id': '17959523612091085',\n",
              "  'like_count': 14,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/312906887_866618654336527_4602407056646087029_n.jpg?_nc_cat=101&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=qKBlmiEaHf0AX9sbceY&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfBB5YD5nEb8bDV0tM6PMVPfG4AMPkdofpD5LaYSkDYThw&oe=6553A7A9',\n",
              "  'timestamp': '2022-10-25 07:00:08'},\n",
              " {'caption': 'Ä°yi haftalar ! ðŸ˜€',\n",
              "  'comments_count': 0,\n",
              "  'id': '17953007285187318',\n",
              "  'like_count': 6,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/312456046_1486645915092461_7217475810825019694_n.jpg?_nc_cat=105&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=LYcxAE7KNHcAX-rR6S8&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCbwfOgKF0MajXNQ8tyT-yA36PAiF-hKASPYjt6aHPQag&oe=6552CAD8',\n",
              "  'timestamp': '2022-10-24 08:18:03'},\n",
              " {'caption': 'Bir iÃ§ sesðŸ˜€',\n",
              "  'comments_count': 0,\n",
              "  'id': '17963586901967134',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/311301571_4919212581514501_2302017448856941577_n.jpg?_nc_cat=111&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=MEs3gEVECpEAX-prN_o&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfBUetDkkYIu4N0hpVp5fRNO9VBhOQSn7-wfZL96l5fU7g&oe=6552EDF2',\n",
              "  'timestamp': '2022-10-14 08:33:03'},\n",
              " {'caption': '10 dk kahve iÃ§mek mi ðŸ˜€ðŸ™ˆ',\n",
              "  'comments_count': 1,\n",
              "  'id': '18144168001285652',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/311716868_147309604674751_3657660573645555265_n.jpg?_nc_cat=110&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=DIkPtHP3PH4AX-VatEL&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfC6QM3KkXbN_S4EI3hELHPolhkpJOL44tz2V7g3tIB0jA&oe=6554376F',\n",
              "  'timestamp': '2022-10-12 13:00:11'},\n",
              " {'caption': 'Geldi hesap kitap aylarÄ± ðŸ˜€ðŸ™ˆ',\n",
              "  'comments_count': 0,\n",
              "  'id': '17991798451560780',\n",
              "  'like_count': 12,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/310821192_792282555218003_8376663433261458846_n.jpg?_nc_cat=107&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=-mxSQpuSN1gAX-92eiA&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfC0LXS5SaR1RzKxTobQ6RFqpqdpKLK7mgDYOsJy60WCUQ&oe=65532EB1',\n",
              "  'timestamp': '2022-10-11 15:39:03'},\n",
              " {'caption': 'YaÅŸayan bilir.',\n",
              "  'comments_count': 0,\n",
              "  'id': '17962352671979921',\n",
              "  'like_count': 20,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/306422698_120551597462975_3729672111655589022_n.jpg?_nc_cat=108&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=S_YHGxx7x-MAX9v7LAc&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfBLPBAXC8gQDK_XXfXJ8E7ff2Yjfkjvg9dZBLZBKcdsEw&oe=6553F9BC',\n",
              "  'timestamp': '2022-10-10 15:09:15'},\n",
              " {'caption': 'Sen, ben, hepimiz ðŸ™ˆ\\n.\\n.\\n#beyazyakalÄ± #beyazyaka #beyazyakaliyiz #kurumsalhayat #homeoffice',\n",
              "  'comments_count': 0,\n",
              "  'id': '17951969500818884',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/287955135_3233485753577482_1432494811089578031_n.jpg?_nc_cat=108&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=1eNnbRSrxN8AX-oVuZM&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCO-vBJbC4EDFSuq_hPtpmV5pqD48jAiz3AwEgzv71UTw&oe=6552C6C3',\n",
              "  'timestamp': '2022-06-14 12:30:07'},\n",
              " {'caption': 'Yeni hafta yeni umutlar ðŸ™ˆðŸ˜€\\n.\\n.\\n.\\n#beyazyakaliyiz #beyazyakalÄ±lar #kurumsalhayat #beyazyaka',\n",
              "  'comments_count': 0,\n",
              "  'id': '17947504459951248',\n",
              "  'like_count': 16,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/286002496_187792573585353_3090155068383503651_n.jpg?_nc_cat=107&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=HuplqwAL46wAX9DXkyh&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCiz80dzTnxdig15eRwPQDNtkQyQmejU1_7pT-81ev0TA&oe=6552C7BC',\n",
              "  'timestamp': '2022-06-06 08:07:22'},\n",
              " {'caption': 'Yaz ayÄ±nÄ±n baÅŸlamasÄ± ile beyaz yakalÄ±larÄ±nda gÃ¼ney hayali sezonu aÃ§Ä±lmÄ±ÅŸtÄ±r, her tatil dÃ¶nÃ¼ÅŸÃ¼ dinleyeceÄŸimiz ah gitsek ne gÃ¼zel olurdu be diyaloglarÄ±na hazÄ±r mÄ±yÄ±z? ðŸ™ˆ\\n.\\n.\\n.\\n#beyazyakalÄ±lar #beyazyakalÄ± #beyazyakalÄ±yÄ±z #kurumsalhayat',\n",
              "  'comments_count': 0,\n",
              "  'id': '18020839444376114',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/285179931_1065698307354891_5832755499196963795_n.jpg?_nc_cat=104&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=A2nx_n2SgY0AX-3H-pC&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCRQHpoayLW9ez5EDzZ_9mBaMZqj4KQjxr_qZ6xdQSFTg&oe=6554241C',\n",
              "  'timestamp': '2022-06-01 18:31:07'},\n",
              " {'caption': 'Home office devam edenlerin Ã¶zlediÄŸi bazÄ± detaylar ðŸ˜‚ðŸ˜€\\n.\\n.\\n#homeoffice #officediaries #kurumsalhayat #beyazyakaliyiz #beyazyakali #beyazyaka #beyazyakalilar',\n",
              "  'comments_count': 0,\n",
              "  'id': '17923203050271738',\n",
              "  'like_count': 10,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/285047146_786334765686839_7981517062868492977_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=R2HsEMAn9cwAX_Ih5pt&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDxdCVWI2OykInvSAFbFMiNb_e1N0gGwvWdGt2jUeotSQ&oe=6553C1AF',\n",
              "  'timestamp': '2022-05-31 05:56:13'},\n",
              " {'caption': 'ToplantÄ±da lafÄ± aÄŸza tÄ±kÄ±lan, tam birÅŸey anlatÄ±rken sÃ¶zÃ¼ mÃ¼dÃ¼rÃ¼ tarafÄ±ndan kesilen, mÃ¼ÅŸteri Ã¶nÃ¼nde tÃ¼m hatalarÄ±n sorumlusu ilan edildikten sonra yÃ¼zÃ¼ dÃ¼ÅŸen Ã§alÄ±ÅŸanÄ±n gÃ¶nlÃ¼nÃ¼ almaya Ã§alÄ±ÅŸan tÃ¼m yÃ¶neticilere gelsin ðŸ˜¬ðŸ˜ƒ\\n.\\n.\\n.\\n\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ±olmak #beyazyakacaps',\n",
              "  'comments_count': 0,\n",
              "  'id': '18032492314350411',\n",
              "  'like_count': 14,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m69/GBnskhB207IjQJ0DAH1jUy6hwTEfbvVBAAAF?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmlndHYudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=399728534921692_479798040&_nc_vs=HBksFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HQm5za2hCMjA3SWpRSjBEQUgxalV5Nmh3VEVmYnZWQkFBQUYVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dQUXJreEN2bzFmeGdnMEJBUEstU1lLOVFuYzhidlZCQUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJpz5naXzluE%2FFQIoAkMzLBdAIPO2RaHKwRgSZGFzaF9iYXNlbGluZV8xX3YxEQB17AcA&ccb=9-4&oh=00_AfATezGtERZzrmV_daV4l2OuGB7AeWv3EgFNVuMFBSOzzA&oe=655067C8&_nc_sid=1d576d&_nc_rid=b986dd5f2c',\n",
              "  'timestamp': '2022-04-08 11:34:45'},\n",
              " {'caption': 'Home office olayÄ± bitip ÅŸirkete dÃ¶nen yoldaÅŸlar el kaldÄ±rsÄ±nðŸ˜€ðŸ˜€ðŸ˜€\\n.\\n.\\n#beyazyakaliyiz #beyazyakalilar #beyazyakaliolmak #homeoffice #kurumsalhayat',\n",
              "  'comments_count': 2,\n",
              "  'id': '17921464819788397',\n",
              "  'like_count': 31,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/9A4A2217435E39C0D636F19264B7DAAD_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=103&vs=343240591534404_4200514842&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC85QTRBMjIxNzQzNUUzOUMwRDYzNkYxOTI2NEI3REFBRF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dINUJoQmVlY3Znb2dLTUFBQzZnVkY2aGREOXFicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJqDe%2B%2FCVtMs%2FFQIoAkMzLBdAGszMzMzMzRgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBa12gevyEmwMJYsRj_PB7V5ijFngMeLWGOzODj8EYGrw&oe=655096E7&_nc_sid=1d576d&_nc_rid=e33409d2f1',\n",
              "  'timestamp': '2021-09-15 17:45:03'},\n",
              " {'caption': 'Merak ettiÄŸim bir konu var, bu yasaklarda evde kÃ¼Ã§Ã¼k Ã§ocuÄŸu olan , kreÅŸe gidemeyen, eve bakÄ±cÄ±/anane/babane gelemeyen durumlar iÃ§in, hiÃ§ kolaylÄ±k saÄŸlayan ÅŸirketler oldu mu? Full pc baÅŸÄ±nda olmasÄ± beklenen ama aynÄ± zamanda kendi tuvaletini yapamayan su iÃ§emeyen yemek yiyemeyen oyun oynayamayan Ã§ocuÄŸuda bakmasÄ± gereken bir ebeveyn. Åžu dÃ¶nemde saÄŸlanan kolaylÄ±klar aidiyet duygusunu arttÄ±rÄ±r ÅŸirkete karÅŸÄ± ,ama henÃ¼z hiÃ§ duymadÄ±m Ã§evreden , varmÄ± sizin bildiÄŸiniz bÃ¶yle ÅŸirketlerðŸ™ˆðŸ˜€\\n.\\n.\\n.\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ±lar #tamkapanma #kurumsalhayat #evdenÃ§alÄ±ÅŸmazorluklarÄ± #homeoffice #beyazyakalÄ±olmak',\n",
              "  'comments_count': 0,\n",
              "  'id': '18004216357316813',\n",
              "  'like_count': 16,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/8942B850F1E9255A8EE54D5A1404CB8C_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=108&vs=1508547439899159_1316879082&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC84OTQyQjg1MEYxRTkyNTVBOEVFNTRENUExNDA0Q0I4Q192aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dNcWlNaGZFdUxGNzVPRUNBTXFFTGlkWlI4bEFicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJrjp3ITxkdtAFQIoAkMzLBdALczMzMzMzRgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfCeEBPZ0uPfS2NAVE790BPqx8sfXuxpmHT5Dte946MGig&oe=65509261&_nc_sid=1d576d&_nc_rid=894c2dd5ff',\n",
              "  'timestamp': '2021-04-28 08:46:04'},\n",
              " {'caption': 'Bir pazar akÅŸamÄ±na yakÄ±ÅŸmayacak tek ÅŸey; Pazartesi sabahÄ±na hazÄ±r olmasÄ± beklenilen o sunumun hazÄ±r olmadÄ±ÄŸÄ± gerÃ§eÄŸidir ðŸ¤¦ðŸ¼\\u200dâ™€ï¸ðŸ˜¬ allah yar ve yardÄ±mcÄ±m olsun ðŸ™ˆ\\n.\\n.\\n.\\n#beyazyakalÄ±olmak #homeofficegÃ¼nleri #evdenÃ§alÄ±ÅŸmak #beyazyakalÄ±lar #beyazyakalÄ±yÄ±z',\n",
              "  'comments_count': 0,\n",
              "  'id': '17910927013639356',\n",
              "  'like_count': 24,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/8E4469649F0B96F8DB4B84A36767D0AD_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=106&vs=6820419444686260_975342214&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC84RTQ0Njk2NDlGMEI5NkY4REI0Qjg0QTM2NzY3RDBBRF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dIa0dkUmU1cW03WTF3OFpBQkp6RjRDVWZIZy1icGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJp6D%2BoTO2L4%2FFQIoAkMzLBdAJ4gxJul41RgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBvWHCIDOmztQw3GcAbdhTBfkA-BNub9lF4gnJMfYGZBQ&oe=655015A8&_nc_sid=1d576d&_nc_rid=cc6a2106e9',\n",
              "  'timestamp': '2021-03-07 20:24:44'},\n",
              " {'caption': 'Sen,ben, hepimizðŸ˜€ðŸ˜ðŸ¤·ðŸ¼\\u200dâ™€ï¸\\n.\\n.\\n.\\n#beyazyaka #beyazyakalÄ±lar #beyazyakaliyiz #kurumsalhayat',\n",
              "  'comments_count': 0,\n",
              "  'id': '17882935519944903',\n",
              "  'like_count': 30,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/0A4435992A6C2C158D3F53EF449FD7A2_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=106&vs=7081587965219656_3016548752&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC8wQTQ0MzU5OTJBNkMyQzE1OEQzRjUzRUY0NDlGRDdBMl92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dJM0g2QmFLbUt2eUxBUURBRFFMNS1yRk92TThicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvidmP%2Fus7c%2FFQIoAkMzLBdAJzMzMzMzMxgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBu0SiBScttITdO6U8N8or_D3E2txEtdHfpqSxn1fDqlA&oe=655010C6&_nc_sid=1d576d&_nc_rid=af265b197b',\n",
              "  'timestamp': '2021-01-02 18:08:11'},\n",
              " {'caption': 'Ama tabiki sÃ¶yleyemedi ve yine yÃ¼zlerce gerekÃ§e ile kendini parÃ§aladÄ±ðŸ¤·ðŸ½\\u200dâ™‚ï¸',\n",
              "  'comments_count': 0,\n",
              "  'id': '17882646595842051',\n",
              "  'like_count': 19,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/714DD30875A4E40801B89F7A0E402BBD_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=104&vs=6329084073883936_4023381164&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC83MTRERDMwODc1QTRFNDA4MDFCODlGN0EwRTQwMkJCRF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dFWXQ1Ulk1WDZOLVhHWUNBSG84bmE4b3dwY3VicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJqbk7YDUjME%2FFQIoAkMzLBdAKpmZmZmZmhgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBzqaKNRzQnaA-kNz-kwOcV9hgWZ842NMBdb62G2avggg&oe=6550696B&_nc_sid=1d576d&_nc_rid=b36fb0f69f',\n",
              "  'timestamp': '2020-11-24 18:03:09'},\n",
              " {'caption': 'Her pazartesi mesai oÌˆncesi toplantÄ± yaparak duÌˆnyayÄ± yeniden kurtaracagÌ†Ä±nÄ± sanan tuÌˆm muÌˆduÌˆrlere selam olsun, yarÄ±n sabah kim boÌˆyle olacakðŸ˜€ðŸ¤¦ðŸ¼\\u200dâ™€ï¸ðŸ¤·ðŸ½\\u200dâ™‚ï¸',\n",
              "  'comments_count': 0,\n",
              "  'id': '17860041008293542',\n",
              "  'like_count': 25,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/F4481E6300EFFC8D34743B6B80FA1797_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=109&vs=223395727379445_3766748050&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9GNDQ4MUU2MzAwRUZGQzhEMzQ3NDNCNkI4MEZBMTc5N192aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dDNHN0eFpDcUhieGxyd0FBUGc1WnNCenhHNXdicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJsCnzYLS%2FLQ%2FFQIoAkMzLBdALhDlYEGJNxgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfDLIv1EnEPIGKMdDyOYMotqA-ECn-H6zZJogbstt4u86w&oe=65507F03&_nc_sid=1d576d&_nc_rid=cfa6f21714',\n",
              "  'timestamp': '2020-11-22 18:49:38'},\n",
              " {'caption': 'Sadece 5 dakika arada en fazla ne kadar ÅŸey isteyebilirler acaba merak ediyorumðŸ˜€ðŸ˜€ðŸ˜€',\n",
              "  'comments_count': 0,\n",
              "  'id': '17860035194227393',\n",
              "  'like_count': 11,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/8742A30E035FE90D9EA02FF9551E1BA0_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=100&vs=1002646831013945_2661295248&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC84NzQyQTMwRTAzNUZFOTBEOUVBMDJGRjk1NTFFMUJBMF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dOMHNleGUzMExlY2JlY0NBRjQ0Y0tDR3dPRWhicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJp7F89%2BpwM0%2FFQIoAkMzLBdAIZmZmZmZmhgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfCFEjr8CC3812TNIG8khU55bFBwpou6C1Ig6nD2NipBkQ&oe=65505F6D&_nc_sid=1d576d&_nc_rid=9ad28882dc',\n",
              "  'timestamp': '2020-11-18 11:11:35'},\n",
              " {'caption': 'Arka fonu deniz olan yeni haftaya baÅŸladÄ±k fotolarÄ±nÄ± paylaÅŸan beyaz yakalÄ±larÄ± alÄ±nlarÄ±ndan Ã¶pmek , tebriklerimi iletmek istiyorumðŸ˜€ bende yemiyor bu paylaÅŸÄ±mlar, sizde durumlar nedir? ðŸ˜€\\n.\\n.\\n.\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ±olmak #kurumsalhayat #homeoffice #homeofficegÃ¼nlÃ¼kleri #beyazyakalÄ± #beyazyakalÄ±lar #ofisgeyikleri',\n",
              "  'comments_count': 1,\n",
              "  'id': '17936837671376035',\n",
              "  'like_count': 37,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.29350-15/116791529_579076392999603_1274694845778390163_n.jpg?_nc_cat=110&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=zA3Dhi_pDCEAX_FF6Rd&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDGx3FgEIV1nglkiLjalnMYoUNEaozmcsXxXBQho6LuuA&oe=6553ABFB',\n",
              "  'timestamp': '2020-08-04 08:51:18'},\n",
              " {'caption': 'Yine bir toplantÄ±da, satÄ±ÅŸ sektÃ¶rde olanlarÄ±, rakipleri anlatÄ±yor,anlatÄ±yor, anlatÄ±yordu..ðŸ˜€ðŸ¤£ðŸ™ˆ\\n.\\n.\\n.\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ±olmak #beyazyakalÄ±lar #ofishalleri #ofisgeyikleri #homeoffice #zoommeeting #sanalofis',\n",
              "  'comments_count': 0,\n",
              "  'id': '17961248008323999',\n",
              "  'like_count': 19,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/3A4827DC3A078542D0AEDDD7BA7E72AE_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=102&vs=796069045628085_3213592116&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC8zQTQ4MjdEQzNBMDc4NTQyRDBBRURERDdCQTdFNzJBRV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dMUEFaQmZkN296UzJaOERBQWlGMHVIQjZDNXlicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJpaE5t6%2BkrY%2FFQIoAkMzLBdAIJmZmZmZmhgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfCiJ0ba2f1mhbatBU_S-tZxsuvugU_P8NnoCAkMffYcPQ&oe=65500B93&_nc_sid=1d576d&_nc_rid=060645592e',\n",
              "  'timestamp': '2020-06-25 20:53:35'},\n",
              " {'caption': 'Åžirketlere dÃ¶nÃ¼ÅŸ baÅŸlasa da video callâ€™lar uzunca bir sÃ¼re sÃ¼recek gibi , ve bizde hiÃ§ yÃ¼z yÃ¼ze gÃ¶rÃ¼ÅŸemediÄŸimiz mÃ¼ÅŸterileri bir heyecan ekranda gÃ¶rÃ¼p tanÄ±ÅŸmaya devam edeceÄŸiz. Sevgili arkadaÅŸlar lÃ¼tfen video call ismimizi , her toplantÄ± Ã¶ncesi kontrol edelim, zira koskoca gmy yada direktÃ¶rÃ¼ Ã§ocugunun kullanÄ±cÄ± adÄ± ile gÃ¶rÃ¼nce benim konsantrasyonum bozuluyor ðŸ˜€ðŸ˜€ðŸ˜€\\n.\\n.\\n.\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ±olmak #homeoffice #beyazyakalÄ±lar #homeofishalleri #skype #videocall',\n",
              "  'comments_count': 0,\n",
              "  'id': '17876327212694653',\n",
              "  'like_count': 11,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/2F494C6A1073371501D12D0AB25B26BB_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=107&vs=1277530312960851_1595705646&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC8yRjQ5NEM2QTEwNzMzNzE1MDFEMTJEMEFCMjVCMjZCQl92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dOaEFlQmNpT3ZuWDE0RUNBQW40Q3pNdE9kRUdicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJsjGx9fBmbI%2FFQIoAkMzLBdALgAAAAAAABgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfAwcSemuUkYb2tz3ybMxBRz0mte4psshHXEgk0VGuWoaQ&oe=65508375&_nc_sid=1d576d&_nc_rid=9d6aa6db09',\n",
              "  'timestamp': '2020-06-21 19:19:25'},\n",
              " {'caption': 'Home officeâ€™i pembe bir rÃ¼ya olarak dÃ¼ÅŸÃ¼nenler ? Hala aynÄ± fikirde misiniz?ðŸ˜€ â€œðŸ‘ðŸ»â€ ve â€œðŸ‘ŽðŸ»â€ olarak yorum bÄ±rakmadan geÃ§meyinizðŸ˜€\\n.\\n.\\n.\\n#beyazyakalÄ±olmak #beyazyakalÄ±yÄ±z #beyazyakalÄ±lar #homeofficegÃ¼nlÃ¼kleri #homeoffice #kurumsalhayat #yakambeyazbeynimayaz #ofishalleri',\n",
              "  'comments_count': 8,\n",
              "  'id': '17888117899530071',\n",
              "  'like_count': 122,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.2885-15/96943211_544359939851554_6458786868409800427_n.jpg?_nc_cat=103&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=0W-D4sXoGYYAX-lle1X&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfA_hbP-Jb-xC60B66PCgFTVQjhMkdCRGRmHhMgve0QLLw&oe=6552B264',\n",
              "  'timestamp': '2020-05-08 08:02:29'},\n",
              " {'caption': 'ArkadaÅŸlar video call gÃ¶rÃ¼ÅŸmelerinde mute tuÅŸunu Ã¶ÄŸrenelim, Ã¶ÄŸretelim, elden ele yayalÄ±m. Yoksa halimiz harap.\\nSaygÄ±larðŸ˜€\\n.\\n.\\n.\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ±lar #beyazyakalÄ±olmak #kurumsalhayat #homeoffice #homeofficeÃ§alÄ±ÅŸmak #videocallhatalarÄ±',\n",
              "  'comments_count': 2,\n",
              "  'id': '17843854169151446',\n",
              "  'like_count': 43,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/F847E2A7CBC26B7AFA9587F8EFC113BD_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1187466912643222_2244393818&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9GODQ3RTJBN0NCQzI2QjdBRkE5NTg3RjhFRkMxMTNCRF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dKdDdjeGRQc1VDRzBRb0dBTWs5RXlSVG1GSTVicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJqzL74Pv%2Bsw%2FFQIoAkMzLBdAMG7ZFocrAhgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfClEm0IsXF17w4xDNDeyRg1a9vreXD2g3mOC4t7ZLrddQ&oe=65505E52&_nc_sid=1d576d&_nc_rid=56e92c90e0',\n",
              "  'timestamp': '2020-05-07 13:01:49'},\n",
              " {'caption': 'Evden Ã§alÄ±ÅŸmayÄ±,kahvesini yudumlayarak yapan varsa alnÄ±ndan Ã¶pesim var, yeminlen bezdim vallahi bezdim billahi bezdim, ne kadar yapÄ±lmayacak iÅŸ varsa, ÅŸimdi yerinde yeller esen 3 yÄ±l Ã¶nce ret olan projenin detayÄ± varsa , video call lar eÅŸliÄŸinde yapÄ±yoruz. HayÄ±r iÅŸteyken molaya Ã§Ä±kardÄ±k tuvalete giderdik gÃ¶ze batmazdÄ±, ÅŸimdi evdesin ya bilgisayar baÅŸÄ±ndan 3 dk ayrÄ±lÄ±nca neredesin mesajÄ± geliyor, bir ben miyim ? YalnÄ±z olmadÄ±ÄŸÄ±mÄ± sÃ¶yleyin rahatlayayÄ±mðŸ˜€\\n.\\n.\\n.\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ± #beyazyakalÄ±lar #homeoffice #ofisgeyikleri #ofishalleri',\n",
              "  'comments_count': 4,\n",
              "  'id': '18028029442256905',\n",
              "  'like_count': 62,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/A742598DE6DC82EEE25FDD77743E8594_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=101&vs=1015347496404397_569189377&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9BNzQyNTk4REU2REM4MkVFRTI1RkRENzc3NDNFODU5NF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dNN0JvUmQ5QjRxSVhRd0RBRVRtLWZVRlo2RWRicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJqKyz6jnvo9AFQIoAkMzLBdAIMzMzMzMzRgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfCZ3RepOq3vGisLRfpPR9AfSnsyehX6amQ5YJcQy5lsgg&oe=65502963&_nc_sid=1d576d&_nc_rid=fd3472673a',\n",
              "  'timestamp': '2020-03-23 20:13:59'},\n",
              " {'caption': 'O son 10 dk, kapanÄ±ÅŸ cÃ¼mleleri, karar Ã§Ä±kmayan toplantÄ±lar, ne iÃ§ersinizler, bunu sen yap bunu o yapsÄ±nlar, ðŸ™ˆ\\n.\\n.\\n.\\n.\\n#beyazyakalÄ± #beyazyakalÄ±yÄ±z #plazacÄ±lÄ±k #yakambeyazbeynimayaz #ofishalleri #ofistebirgÃ¼n #ofisgeyikleri #beyazyakalÄ±lar',\n",
              "  'comments_count': 3,\n",
              "  'id': '17930582272360993',\n",
              "  'like_count': 75,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/5943EDAA4BFD7A70240EF96FA87016A4_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=101&vs=331310102884434_1429660236&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC81OTQzRURBQTRCRkQ3QTcwMjQwRUY5NkZBODcwMTZBNF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dJYWpGaGZCYk50Ylc0a0VBUHJCbTQ0LUpkNTRicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJu6Blaye8tE%2FFQIoAkMzLBdAJ7tkWhysCBgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfAwryY6adcKlkqM7g1SIo77jjxdHTkINq_6yHEPwlZmOg&oe=65509B0A&_nc_sid=1d576d&_nc_rid=5fc9042367',\n",
              "  'timestamp': '2020-02-27 12:08:51'},\n",
              " {'caption': 'Terfi dÃ¶nemi beyaz yakalÄ±lar ðŸ˜€\\n.\\n.\\n.\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ± #ofishalleri #ofistebirgÃ¼n #yakambeyazbeynimayaz #kurumsalhayat #plazahayatÄ±',\n",
              "  'comments_count': 0,\n",
              "  'id': '17845245265899412',\n",
              "  'like_count': 38,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/EB41D3D5A7FFC57CC18BF59F3CE5A180_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=103&vs=860811705692585_4290898144&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9FQjQxRDNENUE3RkZDNTdDQzE4QkY1OUYzQ0U1QTE4MF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dCcXRmeGUtdFZaT2dtNEVBQWJCMS02cm0wMGticGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJpif5Pjpo6ZAFQIoAkMzLBdAJTMzMzMzMxgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBM_CsuJywE65PdSavJgSMsCcR-xHJuoeJnt9cu0rU-OA&oe=655087F0&_nc_sid=1d576d&_nc_rid=eea6514c42',\n",
              "  'timestamp': '2020-02-05 12:16:48'},\n",
              " {'caption': 'Birde maaÅŸÄ± almadan Ã¶nceki gÃ¼n videosu koyayÄ±mda aradaki 7 bÃ¼yÃ¼k farkÄ± bulalÄ±mðŸ˜€ðŸ˜‚\\n.\\nMaaÅŸÄ± alma - maaÅŸÄ±n bitmesi konulu videolarÄ±nÄ±zda beni etiketlemeyi unutmayÄ±n ðŸ˜€\\n.\\n.\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ±lar #kurumsalhayat #ofishalleri #ofisgeyikleri  #maaÅŸgÃ¼nÃ¼ #plazahayatÄ± #capsvideo #adilenaÅŸit',\n",
              "  'comments_count': 1,\n",
              "  'id': '17864430112643812',\n",
              "  'like_count': 69,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/C146EFC414AD1A13494253AABB0ACF8A_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=103&vs=663324972558076_353288189&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9DMTQ2RUZDNDE0QUQxQTEzNDk0MjUzQUFCQjBBQ0Y4QV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dKWU5qUmNtVFp6ekpja0RBQTBQU0ZiUmRxSVZicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJtyz1oPVqLo%2FFQIoAkMzLBdAIzMzMzMzMxgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfDc-OP5rQrnzK3fBKVsOkdLdgFZnej01Q2j37DhepO2ng&oe=655063C3&_nc_sid=1d576d&_nc_rid=883df3ebd2',\n",
              "  'timestamp': '2020-02-04 20:15:33'},\n",
              " {'caption': 'YarÄ±n projesini sunacak tÃ¼m akadaÅŸlara baÅŸarÄ±lar dilerznnznhahsmj ðŸ¤¯ðŸ¤¯ðŸ¤¯ðŸ¤¯ðŸ¤¯ðŸ™ˆðŸ™ˆðŸ™ˆðŸ™ˆðŸ™ˆðŸ˜€ðŸ˜€ðŸ˜€ðŸ˜€\\n.\\n.\\n.\\n#beyazyakaliyiz #beyazyakalilar #beyazyakali #ofisgeyikleri #kurumsalhayat #kurumsalyasam #yakambeyazbeynimayaz #ofishalleri',\n",
              "  'comments_count': 0,\n",
              "  'id': '17861947033674903',\n",
              "  'like_count': 65,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/32479C118EDE0CD050A453D48B79AC9B_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1382124252373305_4220529403&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC8zMjQ3OUMxMThFREUwQ0QwNTBBNDUzRDQ4Qjc5QUM5Ql92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dKTG5aQmVZODJYdXBqQUJBREowaU1NQkVPaC1icGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvKWsP2Um8g%2FFQIoAkMzLBdAGRBiTdLxqhgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfC7KlIT_shst-9PshpEYHIHvkQNrgKZFRHcvwD-iL6j7A&oe=655024E0&_nc_sid=1d576d&_nc_rid=0887ed74eb',\n",
              "  'timestamp': '2020-02-02 18:46:59'},\n",
              " {'caption': 'ðŸŽ¯ToplantÄ± AmacÄ±: Sorun Ã§Ã¶zmek,karar almak, sÃ¼reÃ§ geliÅŸtirmek vs vs.\\n.\\nGerÃ§ekleÅŸen: bÄ±rak bir karar almayÄ±; var olan konuyu daha da kaosa sÃ¼rÃ¼kleyerek,kucaÄŸÄ±nda 10 tane ekstra konu ile bir diÄŸer toplantÄ±da karar almak Ã¼zere odadan ayrÄ±lmakðŸ™ˆðŸ™ˆ\\n.\\nGerÃ§ekten merak ediyorum, toplantÄ±lardan bir karar alÄ±p Ã§Ä±kabilen ÅŸirketler var mÄ± ðŸ˜€ðŸ˜€ðŸ˜€\\n.\\n.\\n.\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ±olmak #kurumsalhayat #ofishalleri #ofisgeyikleri #toplantÄ±geyikleri #plazacÄ±lÄ±k',\n",
              "  'comments_count': 6,\n",
              "  'id': '18048888499210863',\n",
              "  'like_count': 48,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.2885-15/81922221_162500271705350_4661947870335885603_n.jpg?_nc_cat=111&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=hezVgEvZEBMAX8n3YEr&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfCDfOXF6fSqpP7m6H8XbMiaz1BtYaKgp4zk6kSqS3vN0g&oe=655397AF',\n",
              "  'timestamp': '2020-01-23 18:49:48'},\n",
              " {'caption': 'Beyaz yakalÄ±larÄ±n kurumsal hayatta belli bir sÃ¼re geÃ§irip, okulda Ã¶ÄŸrendiklerini gÃ¶steremeden geÃ§irdiÄŸi yÄ±llarÄ±n isyanÄ± ektedirðŸ˜€\\n.\\n.\\n.\\nNerede bu ceteris paribuslar , arz talep eÄŸrileri,fifolar , lifolar , belirsiz integraller,matrisler? ðŸ˜€ðŸ˜³ðŸ˜‚\\n.\\n.\\n.\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ±olmak #beyazyaka #kurumsalhayat #ofishalleri #ofistebirgÃ¼n #yakambeyazbeynimayaz',\n",
              "  'comments_count': 2,\n",
              "  'id': '17903768092410138',\n",
              "  'like_count': 75,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/994E9FFF59E394A2EBB53E2EA88627A2_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=104&vs=662100319399924_639439609&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC85OTRFOUZGRjU5RTM5NEEyRUJCNTNFMkVBODg2MjdBMl92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dOSFA4eFo0OFdFTHNyY0RBTFdPcWxDLVlIMDFicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJsz33cSfx6BAFQIoAkMzLBdAKwAAAAAAABgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfBpijds-kKD9-Tx-CfswyDyo4I8gDdjFZFtz0p3KCCyiw&oe=65507C39&_nc_sid=1d576d&_nc_rid=cec2bd958d',\n",
              "  'timestamp': '2020-01-06 21:15:21'},\n",
              " {'caption': 'Evet yaÄŸmur Ã§amur demeden piÅŸileri gÃ¶mdÃ¼ysek yeni haftaya hazÄ±rÄ±zðŸ˜€\\n.\\n.\\n.\\n#beyazyakalÄ±yÄ±z #beyazyakalÄ± #beyazyakalÄ±olmak #kurumsalhayat #ofishalleri #ofisgeyikleri',\n",
              "  'comments_count': 0,\n",
              "  'id': '18080937595131135',\n",
              "  'like_count': 67,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/v/t51.2885-15/75225442_273339910290852_4305568348763209930_n.jpg?_nc_cat=107&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=4np6te3wPesAX8Ywg11&_nc_ht=scontent-sof1-2.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDcJmDzCtmWUZRwDN1AyzTz7EvvaqpK7pXNg7sLf29JmQ&oe=6552ED6E',\n",
              "  'timestamp': '2019-12-01 20:41:40'},\n",
              " {'caption': 'Veliaht ÅŸirkete gelmiÅŸtir , ilgi manyaÄŸÄ± olmuÅŸtur!ðŸ˜†\\n.\\n.\\n.\\n.\\n#beyazyakalÄ± #kurumsalhayat #ofishalleri #patronunoÄŸlu #beyazyakalÄ±lar #ofisgeyikleri #maslak',\n",
              "  'comments_count': 0,\n",
              "  'id': '17871022735514949',\n",
              "  'like_count': 28,\n",
              "  'media_type': 'VIDEO',\n",
              "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m84/E243F7405CB32029500F89EF0D4610A4_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmZlZWQudW5rbm93bi1DMy43MjAuZGFzaF9iYXNlbGluZV8xX3YxIn0&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=108&vs=303334938985794_2687426354&_nc_vs=HBksFQIYTGlnX2JhY2tmaWxsX3RpbWVsaW5lX3ZvZC9FMjQzRjc0MDVDQjMyMDI5NTAwRjg5RUYwRDQ2MTBBNF92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dIdHd1QmE2eHl6S2swY0NBRThxa19MaWt2RnNicGt3QUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJoqsp7zMp7Q%2FFQIoAkMzLBdAKjMzMzMzMxgSZGFzaF9iYXNlbGluZV8xX3YxEQB16gcA&ccb=9-4&oh=00_AfCx94K0PebnhfCltkSY7RbnjXdFCOJZEn3kDviURicuvQ&oe=65508EA1&_nc_sid=1d576d&_nc_rid=5044429cdc',\n",
              "  'timestamp': '2019-11-19 20:05:36'},\n",
              " {'caption': 'Kimlerin bakiyesi hala bitmedi ðŸ˜€\\n.\\n.\\n.\\n#kurumsalkimlik #ofishalleri #ofistebirgÃ¼n #Ã¶ÄŸlearasÄ± #beyazyakalÄ± #beyazyakalÄ±yÄ±z #beyazyakalÄ±olmak',\n",
              "  'comments_count': 0,\n",
              "  'id': '17864441002538312',\n",
              "  'like_count': 54,\n",
              "  'media_type': 'IMAGE',\n",
              "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.2885-15/73085779_760259937753151_6964394615362269103_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=TdR93VBY-OUAX-UycEU&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfAdNqdjVHBNKBG_9WwE3A507HZOIslSaWNNwCDYaaDHpA&oe=655446C3',\n",
              "  'timestamp': '2019-11-12 19:07:12'}]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "username2posts_test['beyazyakaliyiz']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blI2SqvvvOF8",
        "outputId": "fe1e72e3-3ad0-486d-d054-3e90e8c66669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First 3 predictions:\n",
            "[{'comments_count': 2,\n",
            "  'id': '18144550534306740',\n",
            "  'like_count': 158,\n",
            "  'media_type': 'CAROUSEL_ALBUM',\n",
            "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/397997154_1016992459537522_4925783512176260397_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=7V_eObkFeK4AX-LMtsK&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDEqDhzaTO3ezV-veT6cJFCOcAEyeVzHR6si9n33N6G5A&oe=6551B6B9',\n",
            "  'timestamp': '2023-11-02 15:49:22',\n",
            "  'username': 'kozayarismasi'},\n",
            " {'comments_count': 0,\n",
            "  'id': '17995331788956693',\n",
            "  'like_count': 99,\n",
            "  'media_type': 'VIDEO',\n",
            "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m82/BF4767CB85BDFB8ADCCCA8F15B8C20B5_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmNsaXBzLnVua25vd24tQzMuNzIwLmRhc2hfYmFzZWxpbmVfMV92MSJ9&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1259525061418244_1441854817&_nc_vs=HBksFQIYT2lnX3hwdl9yZWVsc19wZXJtYW5lbnRfcHJvZC9CRjQ3NjdDQjg1QkRGQjhBRENDQ0E4RjE1QjhDMjBCNV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dBRWdfaFZfcDVCYk5HZ0NBQTlzVURvZW5mZ3FicV9FQUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvS3uOiQ0P8%2FFQIoAkMzLBdAJO%2Bdsi0OVhgSZGFzaF9iYXNlbGluZV8xX3YxEQB1AAA%3D&ccb=9-4&oh=00_AfAm22JssMPaUlQe3rpYsFWBhFb5mUgolTCdhV0Xgm4AnA&oe=6556A482&_nc_sid=1d576d&_nc_rid=cd9a998e44',\n",
            "  'timestamp': '2023-08-19 13:46:02',\n",
            "  'username': 'celikbeymobilya'},\n",
            " {'comments_count': 75,\n",
            "  'id': '18302703232191518',\n",
            "  'like_count': 1224,\n",
            "  'media_type': 'VIDEO',\n",
            "  'media_url': None,\n",
            "  'timestamp': '2023-10-02 06:53:33',\n",
            "  'username': 'girisimci_muhendis'}]\n"
          ]
        }
      ],
      "source": [
        "# Print first 3 items\n",
        "print(\"\\nFirst 3 predictions:\")\n",
        "pprint(output_list[:3])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
