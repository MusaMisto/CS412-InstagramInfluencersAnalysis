{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBOlFYpnlr0N"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PM5EnXhLk6BL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgAdz_b8xp8w",
        "outputId": "14cd6ea6-4a61-4b22-f2a6-59088a78e58a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\itsmm\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Turkish StopWords\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "turkish_stopwords = stopwords.words('turkish')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plt9-VkNmlmH"
      },
      "source": [
        "# Influencer Category Classification\n",
        "\n",
        "\n",
        "\n",
        "1.   Read Data\n",
        "2.   Preprocess Data\n",
        "3.   Prepare Model\n",
        "4.   Predict Test Data\n",
        "4.   Save outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8DeBh-b7lrEs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows of the training classification DataFrame:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>taskirancemal</td>\n",
              "      <td>mom and children</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tam_kararinda</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spart4nn</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sosyalyiyiciler</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sonaydizdarahad</td>\n",
              "      <td>mom and children</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           user_id          category\n",
              "0    taskirancemal  mom and children\n",
              "1    tam_kararinda              food\n",
              "2         spart4nn              food\n",
              "3  sosyalyiyiciler              food\n",
              "4  sonaydizdarahad  mom and children"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the training directory\n",
        "training_dir = os.path.join(data_dir, 'training')\n",
        "\n",
        "# File path for 'train-classification.csv'\n",
        "train_classification_path = os.path.join(training_dir, 'train-classification.csv')\n",
        "\n",
        "# Step 2: Load Data Dynamically\n",
        "train_classification_df = pd.read_csv(train_classification_path)\n",
        "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
        "\n",
        "# Step 3: Unify Labels\n",
        "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
        "\n",
        "# Step 4: Create User-to-Category Mapping\n",
        "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\n",
        "\n",
        "# Step 5: Verify Output\n",
        "print(\"First few rows of the training classification DataFrame:\")\n",
        "train_classification_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "SfD7BQ3hE5Jh",
        "outputId": "a67f08dd-ab3e-491f-bfe2-a14da0e961cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>art</th>\n",
              "      <td>191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>entertainment</th>\n",
              "      <td>323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fashion</th>\n",
              "      <td>299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gaming</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>health and lifestyle</th>\n",
              "      <td>503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mom and children</th>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sports</th>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tech</th>\n",
              "      <td>346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>travel</th>\n",
              "      <td>294</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      user_id\n",
              "category                     \n",
              "art                       191\n",
              "entertainment             323\n",
              "fashion                   299\n",
              "food                      511\n",
              "gaming                     13\n",
              "health and lifestyle      503\n",
              "mom and children          149\n",
              "sports                    113\n",
              "tech                      346\n",
              "travel                    294"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# stats about the labels\n",
        "train_classification_df.groupby(\"category\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mhckiEfD2gg_",
        "outputId": "af451a4d-8825-4506-fb70-41a6ef845aaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mom and children'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "username2_category[\"sonaydizdarahad\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GT_IcUM2nGBH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Training Users: 2741\n",
            "Number of Testing Users: 2674\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the training directory\n",
        "training_dir = os.path.join(data_dir, 'training')\n",
        "\n",
        "# File path for 'training-dataset.jsonl.gz'\n",
        "train_data_path = os.path.join(training_dir, 'training-dataset.jsonl.gz')\n",
        "\n",
        "# Step 2: Initialize Dictionaries for Data\n",
        "username2posts_train = dict()\n",
        "username2profile_train = dict()\n",
        "\n",
        "username2posts_test = dict()\n",
        "username2profile_test = dict()\n",
        "\n",
        "# Step 3: Process Data from 'training-dataset.jsonl.gz'\n",
        "with gzip.open(train_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "\n",
        "        profile = sample[\"profile\"]\n",
        "        username = profile.get(\"username\", \"\").strip()  # Handle missing or empty usernames\n",
        "        if not username:\n",
        "            continue  # Skip if username is missing or empty\n",
        "\n",
        "        if username in username2_category:\n",
        "            # Train data info\n",
        "            username2posts_train[username] = sample[\"posts\"]\n",
        "            username2profile_train[username] = profile\n",
        "        else:\n",
        "            # Test data info\n",
        "            username2posts_test[username] = sample[\"posts\"]\n",
        "            username2profile_test[username] = profile\n",
        "\n",
        "# Step 4: Verify Output\n",
        "print(f\"Number of Training Users: {len(username2posts_train)}\")\n",
        "print(f\"Number of Testing Users: {len(username2posts_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "djeF1GQ1oy3v",
        "outputId": "5c2ead24-d7d1-4df8-bec0-bf2152c76832"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>id</th>\n",
              "      <th>full_name</th>\n",
              "      <th>biography</th>\n",
              "      <th>category_name</th>\n",
              "      <th>post_count</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>is_business_account</th>\n",
              "      <th>is_private</th>\n",
              "      <th>...</th>\n",
              "      <th>business_category_name</th>\n",
              "      <th>overall_category_name</th>\n",
              "      <th>category_enum</th>\n",
              "      <th>is_verified_by_mv4b</th>\n",
              "      <th>is_regulated_c18</th>\n",
              "      <th>profile_pic_url</th>\n",
              "      <th>should_show_category</th>\n",
              "      <th>should_show_public_contacts</th>\n",
              "      <th>show_account_transparency_details</th>\n",
              "      <th>profile_picture_base64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>deparmedya</td>\n",
              "      <td>3170700063</td>\n",
              "      <td>Depar Medya</td>\n",
              "      <td>#mediaplanning #mediabuying #sosyalmedya</td>\n",
              "      <td>Local business</td>\n",
              "      <td>None</td>\n",
              "      <td>1167</td>\n",
              "      <td>192</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>LOCAL</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     username          id    full_name  \\\n",
              "0  deparmedya  3170700063  Depar Medya   \n",
              "\n",
              "                                  biography   category_name post_count  \\\n",
              "0  #mediaplanning #mediabuying #sosyalmedya  Local business       None   \n",
              "\n",
              "  follower_count following_count is_business_account is_private  ...  \\\n",
              "0           1167             192                True      False  ...   \n",
              "\n",
              "  business_category_name overall_category_name category_enum  \\\n",
              "0                   None                  None         LOCAL   \n",
              "\n",
              "  is_verified_by_mv4b is_regulated_c18  \\\n",
              "0               False            False   \n",
              "\n",
              "                                     profile_pic_url should_show_category  \\\n",
              "0  https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....                 True   \n",
              "\n",
              "  should_show_public_contacts show_account_transparency_details  \\\n",
              "0                        True                              True   \n",
              "\n",
              "                              profile_picture_base64  \n",
              "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
              "\n",
              "[1 rows x 44 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Profile Dataframe\n",
        "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
        "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)\n",
        "\n",
        "train_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "15tvVohUBHK9",
        "outputId": "15be2533-51b2-41e8-e0f2-26a44933dbae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>id</th>\n",
              "      <th>full_name</th>\n",
              "      <th>biography</th>\n",
              "      <th>category_name</th>\n",
              "      <th>post_count</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>is_business_account</th>\n",
              "      <th>is_private</th>\n",
              "      <th>...</th>\n",
              "      <th>business_category_name</th>\n",
              "      <th>overall_category_name</th>\n",
              "      <th>category_enum</th>\n",
              "      <th>is_verified_by_mv4b</th>\n",
              "      <th>is_regulated_c18</th>\n",
              "      <th>profile_pic_url</th>\n",
              "      <th>should_show_category</th>\n",
              "      <th>should_show_public_contacts</th>\n",
              "      <th>show_account_transparency_details</th>\n",
              "      <th>profile_picture_base64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>beyazyakaliyiz</td>\n",
              "      <td>8634457436</td>\n",
              "      <td>Selam Beyaz Yakalı</td>\n",
              "      <td>Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀</td>\n",
              "      <td>Personal blog</td>\n",
              "      <td>None</td>\n",
              "      <td>1265</td>\n",
              "      <td>665</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>PERSONAL_BLOG</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>https://instagram.fist6-1.fna.fbcdn.net/v/t51....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         username          id           full_name  \\\n",
              "0  beyazyakaliyiz  8634457436  Selam Beyaz Yakalı   \n",
              "\n",
              "                                     biography  category_name post_count  \\\n",
              "0  Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀  Personal blog       None   \n",
              "\n",
              "  follower_count following_count is_business_account is_private  ...  \\\n",
              "0           1265             665                True      False  ...   \n",
              "\n",
              "  business_category_name overall_category_name  category_enum  \\\n",
              "0                   None                  None  PERSONAL_BLOG   \n",
              "\n",
              "  is_verified_by_mv4b is_regulated_c18  \\\n",
              "0               False            False   \n",
              "\n",
              "                                     profile_pic_url should_show_category  \\\n",
              "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
              "\n",
              "  should_show_public_contacts show_account_transparency_details  \\\n",
              "0                        True                              True   \n",
              "\n",
              "                              profile_picture_base64  \n",
              "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
              "\n",
              "[1 rows x 44 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_classification_df:\n",
            "['user_id', 'category']\n",
            "\n",
            "First few rows of the training classification DataFrame:\n",
            "           user_id          category\n",
            "0    taskirancemal  mom and children\n",
            "1    tam_kararinda              food\n",
            "2         spart4nn              food\n",
            "3  sosyalyiyiciler              food\n",
            "4  sonaydizdarahad  mom and children\n",
            "\n",
            "Label distribution:\n",
            "category\n",
            "food                    511\n",
            "health and lifestyle    503\n",
            "tech                    346\n",
            "entertainment           323\n",
            "fashion                 299\n",
            "travel                  294\n",
            "art                     191\n",
            "mom and children        149\n",
            "sports                  113\n",
            "gaming                   13\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Verify Output\n",
        "print(\"Columns in train_classification_df:\")\n",
        "print(train_classification_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of the training classification DataFrame:\")\n",
        "print(train_classification_df.head())\n",
        "\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(train_classification_df['category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_profile_df:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
            "\n",
            "First few rows of train_profile_df:\n",
            "     username          id    full_name  \\\n",
            "0  deparmedya  3170700063  Depar Medya   \n",
            "\n",
            "                                  biography   category_name post_count  \\\n",
            "0  #mediaplanning #mediabuying #sosyalmedya  Local business       None   \n",
            "\n",
            "  follower_count following_count is_business_account is_private  ...  \\\n",
            "0           1167             192                True      False  ...   \n",
            "\n",
            "  business_category_name overall_category_name category_enum  \\\n",
            "0                   None                  None         LOCAL   \n",
            "\n",
            "  is_verified_by_mv4b is_regulated_c18  \\\n",
            "0               False            False   \n",
            "\n",
            "                                     profile_pic_url should_show_category  \\\n",
            "0  https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....                 True   \n",
            "\n",
            "  should_show_public_contacts show_account_transparency_details  \\\n",
            "0                        True                              True   \n",
            "\n",
            "                              profile_picture_base64  \n",
            "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
            "\n",
            "[1 rows x 44 columns]\n",
            "\n",
            "Columns in test_profile_df:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
            "\n",
            "First few rows of test_profile_df:\n",
            "         username          id           full_name  \\\n",
            "0  beyazyakaliyiz  8634457436  Selam Beyaz Yakalı   \n",
            "\n",
            "                                     biography  category_name post_count  \\\n",
            "0  Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀  Personal blog       None   \n",
            "\n",
            "  follower_count following_count is_business_account is_private  ...  \\\n",
            "0           1265             665                True      False  ...   \n",
            "\n",
            "  business_category_name overall_category_name  category_enum  \\\n",
            "0                   None                  None  PERSONAL_BLOG   \n",
            "\n",
            "  is_verified_by_mv4b is_regulated_c18  \\\n",
            "0               False            False   \n",
            "\n",
            "                                     profile_pic_url should_show_category  \\\n",
            "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
            "\n",
            "  should_show_public_contacts show_account_transparency_details  \\\n",
            "0                        True                              True   \n",
            "\n",
            "                              profile_picture_base64  \n",
            "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
            "\n",
            "[1 rows x 44 columns]\n"
          ]
        }
      ],
      "source": [
        "# Profile Dataframe\n",
        "print(\"Columns in train_profile_df:\")\n",
        "print(train_profile_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of train_profile_df:\")\n",
        "print(train_profile_df.head(1))\n",
        "\n",
        "print(\"\\nColumns in test_profile_df:\")\n",
        "print(test_profile_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of test_profile_df:\")\n",
        "print(test_profile_df.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_profile_df after dropping:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'bio_links', 'is_professional_account', 'business_category_name', 'overall_category_name']\n",
            "\n",
            "Columns in test_profile_df after dropping:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'bio_links', 'is_professional_account', 'business_category_name', 'overall_category_name']\n"
          ]
        }
      ],
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = [\n",
        "    'highlight_reel_count', 'entities', 'ai_agent_type', 'fb_profile_biolink',\n",
        "    'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid',\n",
        "    'has_clips', 'hide_like_and_view_counts', 'is_supervision_enabled',\n",
        "    'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user',\n",
        "    'is_embeds_disabled', 'is_joined_recently', 'business_address_json',\n",
        "    'business_contact_method', 'business_email', 'business_phone_number',\n",
        "    'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18',\n",
        "    'profile_pic_url', 'should_show_category', 'should_show_public_contacts',\n",
        "    'show_account_transparency_details', 'profile_picture_base64'\n",
        "]\n",
        "\n",
        "# Dropping specified columns from train_profile_df\n",
        "train_profile_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Dropping specified columns from test_profile_df\n",
        "test_profile_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Verify columns in train_profile_df after dropping\n",
        "print(\"Columns in train_profile_df after dropping:\")\n",
        "print(train_profile_df.columns.tolist())\n",
        "\n",
        "# Verify columns in test_profile_df after dropping\n",
        "print(\"\\nColumns in test_profile_df after dropping:\")\n",
        "print(test_profile_df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Z5UY0eYLsoTr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "Best Params: {'clf__C': 1, 'preprocessor__bio_tfidf__ngram_range': (1, 1), 'preprocessor__captions_tfidf__ngram_range': (1, 2)}\n",
            "Best CV Accuracy: 0.6455259262035492\n",
            "Training Accuracy: 0.9516423357664233\n",
            "\n",
            "Training Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                 art       0.94      0.95      0.94       153\n",
            "       entertainment       0.97      0.91      0.94       258\n",
            "             fashion       0.92      0.97      0.95       239\n",
            "                food       0.98      0.97      0.97       409\n",
            "              gaming       1.00      1.00      1.00        10\n",
            "health and lifestyle       0.97      0.91      0.94       402\n",
            "    mom and children       0.92      0.97      0.94       119\n",
            "              sports       0.98      0.99      0.98        90\n",
            "                tech       0.92      0.97      0.95       277\n",
            "              travel       0.94      0.96      0.95       235\n",
            "\n",
            "            accuracy                           0.95      2192\n",
            "           macro avg       0.95      0.96      0.96      2192\n",
            "        weighted avg       0.95      0.95      0.95      2192\n",
            "\n",
            "Validation Accuracy: 0.644808743169399\n",
            "\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                 art       0.29      0.18      0.23        38\n",
            "       entertainment       0.44      0.40      0.42        65\n",
            "             fashion       0.49      0.55      0.52        60\n",
            "                food       0.89      0.87      0.88       102\n",
            "              gaming       0.50      0.33      0.40         3\n",
            "health and lifestyle       0.75      0.71      0.73       100\n",
            "    mom and children       0.48      0.50      0.49        30\n",
            "              sports       0.65      0.65      0.65        23\n",
            "                tech       0.67      0.83      0.74        69\n",
            "              travel       0.65      0.68      0.66        59\n",
            "\n",
            "            accuracy                           0.64       549\n",
            "           macro avg       0.58      0.57      0.57       549\n",
            "        weighted avg       0.64      0.64      0.64       549\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Preprocessing Function\n",
        "import re\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    text = text.casefold()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Build Corpus and Labels\n",
        "corpus = []\n",
        "train_usernames = []\n",
        "\n",
        "for username, posts in username2posts_train.items():\n",
        "    train_usernames.append(username)\n",
        "    cleaned_captions = []\n",
        "    for post in posts:\n",
        "        post_caption = post.get(\"caption\", \"\")\n",
        "        if post_caption is None:\n",
        "            continue\n",
        "        post_caption = preprocess_text(post_caption)\n",
        "        if post_caption != \"\":\n",
        "            cleaned_captions.append(post_caption)\n",
        "    user_post_captions = \"\\n\".join(cleaned_captions)\n",
        "    corpus.append(user_post_captions)\n",
        "\n",
        "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
        "\n",
        "# Incorporate Metadata\n",
        "records = []\n",
        "for idx, username in enumerate(train_usernames):\n",
        "    profile = username2profile_train.get(username, {})\n",
        "    biography_text = str(profile.get(\"biography\", \"\") or \"\")\n",
        "    follower_count = profile.get(\"follower_count\", 0)\n",
        "    following_count = profile.get(\"following_count\", 0)\n",
        "    post_count = profile.get(\"post_count\", 0) if profile.get(\"post_count\") else 0\n",
        "    row_dict = {\n",
        "        \"username\": username,\n",
        "        \"captions\": corpus[idx],\n",
        "        \"biography\": biography_text,\n",
        "        \"follower_count\": follower_count,\n",
        "        \"following_count\": following_count,\n",
        "        \"post_count\": post_count,\n",
        "        \"label\": y_train[idx]\n",
        "    }\n",
        "    records.append(row_dict)\n",
        "\n",
        "train_full_df = pd.DataFrame(records)\n",
        "\n",
        "# Define Pipeline Components\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Train-Validation Split\n",
        "X = train_full_df.drop(columns=[\"label\", \"username\"])\n",
        "y = train_full_df[\"label\"]\n",
        "\n",
        "x_train_df, x_val_df, y_train_labels, y_val_labels = train_test_split(\n",
        "    X, \n",
        "    y, \n",
        "    test_size=0.2, \n",
        "    stratify=y, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define ColumnTransformer\n",
        "numeric_features = [\"follower_count\", \"following_count\", \"post_count\"]\n",
        "text_features_caps = \"captions\"\n",
        "text_features_bio = \"biography\"\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"captions_tfidf\", TfidfVectorizer(\n",
        "             stop_words=turkish_stopwords, \n",
        "             max_features=5000\n",
        "         ), text_features_caps),\n",
        "        (\"bio_tfidf\", TfidfVectorizer(\n",
        "             stop_words=turkish_stopwords, \n",
        "             max_features=5000\n",
        "         ), text_features_bio),\n",
        "        (\"numeric_scaler\", MinMaxScaler(), numeric_features)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# Build ImbPipeline (SMOTE + Classifier)\n",
        "pipeline = ImbPipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=42, sampling_strategy=\"auto\")),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        class_weight='balanced',\n",
        "        solver='liblinear',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Define a param grid\n",
        "param_grid = {\n",
        "    \"preprocessor__captions_tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"preprocessor__bio_tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"clf__C\": [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Initialize and fit GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(x_train_df, y_train_labels)\n",
        "\n",
        "print(\"Best Params:\", grid_search.best_params_)\n",
        "print(\"Best CV Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "\n",
        "# **New Section: Evaluate on Training Data**\n",
        "# Predict on the training data\n",
        "y_train_pred = best_pipeline.predict(x_train_df)\n",
        "\n",
        "# Calculate training accuracy\n",
        "train_acc = accuracy_score(y_train_labels, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "\n",
        "# Generate and print the training classification report\n",
        "print(\"\\nTraining Classification Report:\\n\",\n",
        "      classification_report(y_train_labels, y_train_pred, zero_division=0))\n",
        "\n",
        "# **End of New Section**\n",
        "\n",
        "# Predict on the validation data\n",
        "y_val_pred = best_pipeline.predict(x_val_df)\n",
        "\n",
        "# Calculate validation accuracy\n",
        "val_acc = accuracy_score(y_val_labels, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_acc)\n",
        "\n",
        "# Generate and print the validation classification report\n",
        "print(\"\\nClassification Report:\\n\",\n",
        "      classification_report(y_val_labels, y_val_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2192 entries, 2638 to 2667\n",
            "Data columns (total 5 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   captions         2192 non-null   object \n",
            " 1   biography        2192 non-null   object \n",
            " 2   follower_count   2192 non-null   int64  \n",
            " 3   following_count  2192 non-null   int64  \n",
            " 4   post_count       2192 non-null   float64\n",
            "dtypes: float64(1), int64(2), object(2)\n",
            "memory usage: 102.8+ KB\n",
            "None\n",
            "\n",
            "Missing values in training data:\n",
            "captions           0\n",
            "biography          0\n",
            "follower_count     0\n",
            "following_count    0\n",
            "post_count         0\n",
            "dtype: int64\n",
            "\n",
            "Cross-validation scores: [0.66514806 0.61503417 0.61872146 0.61872146 0.63013699]\n",
            "Mean CV accuracy: 0.630 (+/- 0.037)\n"
          ]
        }
      ],
      "source": [
        "# Add these imports if not already present\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Modified text preprocessing\n",
        "def advanced_text_preprocessing(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    text = str(text).lower()\n",
        "    # Extract hashtags and mentions\n",
        "    hashtags = ' '.join(re.findall(r'#\\w+', text))\n",
        "    mentions = ' '.join(re.findall(r'@\\w+', text))\n",
        "    \n",
        "    # Clean text\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return f\"{text} {hashtags} {mentions}\"\n",
        "\n",
        "# Modified TF-IDF Vectorizer\n",
        "class EnhancedTfidfVectorizer(TfidfVectorizer):\n",
        "    def fit_transform(self, X, y=None):\n",
        "        # Ensure X is a Series or list\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.iloc[:, 0]\n",
        "        X_prep = [advanced_text_preprocessing(doc) for doc in X]\n",
        "        return super().fit_transform(X_prep)\n",
        "    \n",
        "    def transform(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.iloc[:, 0]\n",
        "        X_prep = [advanced_text_preprocessing(doc) for doc in X]\n",
        "        return super().transform(X_prep)\n",
        "\n",
        "# Feature engineering for numeric data\n",
        "def extract_numeric_features(X):\n",
        "    # Convert numpy array back to DataFrame with column names\n",
        "    df = pd.DataFrame(X, columns=['follower_count', 'following_count', 'post_count'])\n",
        "    \n",
        "    # Fill NaN values with 0\n",
        "    df = df.fillna(0)\n",
        "    \n",
        "    # Calculate ratios safely\n",
        "    df['follower_ratio'] = df['follower_count'] / (df['following_count'].replace(0, 1))\n",
        "    df['post_density'] = df['post_count'] / (df['follower_count'].replace(0, 1))\n",
        "    df['engagement_score'] = np.log1p(df['follower_count']) * np.log1p(df['post_count'])\n",
        "    \n",
        "    return df.values\n",
        "\n",
        "# Build the enhanced pipeline\n",
        "def build_enhanced_pipeline():\n",
        "    # Create preprocessor\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('captions', EnhancedTfidfVectorizer(\n",
        "                stop_words=turkish_stopwords,\n",
        "                max_features=3000,\n",
        "                ngram_range=(1, 2),\n",
        "                min_df=2,\n",
        "                max_df=0.95\n",
        "            ), 'captions'),\n",
        "            \n",
        "            ('biography', EnhancedTfidfVectorizer(\n",
        "                stop_words=turkish_stopwords,\n",
        "                max_features=2000,\n",
        "                ngram_range=(1, 2),\n",
        "                min_df=2,\n",
        "                max_df=0.95\n",
        "            ), 'biography'),\n",
        "            \n",
        "            ('numeric', Pipeline([\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('features', FunctionTransformer(extract_numeric_features))\n",
        "            ]), ['follower_count', 'following_count', 'post_count'])\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    # Create ensemble classifiers\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        max_features='sqrt',\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    lr = LogisticRegression(\n",
        "        C=1.0,\n",
        "        class_weight='balanced',\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    # Create voting classifier\n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', rf),\n",
        "            ('gb', gb),\n",
        "            ('lr', lr)\n",
        "        ],\n",
        "        voting='soft'\n",
        "    )\n",
        "    \n",
        "    # Build final pipeline\n",
        "    pipeline = ImbPipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('classifier', ensemble)\n",
        "    ])\n",
        "    \n",
        "    return pipeline\n",
        "\n",
        "# Before training, let's verify the data\n",
        "print(\"Training data info:\")\n",
        "print(x_train_df.info())\n",
        "print(\"\\nMissing values in training data:\")\n",
        "print(x_train_df.isnull().sum())\n",
        "\n",
        "# Create and train the enhanced pipeline\n",
        "enhanced_pipeline = build_enhanced_pipeline()\n",
        "\n",
        "# Perform cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(enhanced_pipeline, x_train_df, y_train_labels, \n",
        "                          cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "print(\"\\nCross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV accuracy: {:.3f} (+/- {:.3f})\".format(\n",
        "    cv_scores.mean(), cv_scores.std() * 2))\n",
        "\n",
        "# Train final model\n",
        "enhanced_pipeline.fit(x_train_df, y_train_labels)\n",
        "\n",
        "# Evaluate on training data\n",
        "y_train_pred = enhanced_pipeline.predict(x_train_df)\n",
        "print(\"\\nTraining Classification Report:\")\n",
        "print(classification_report(y_train_labels, y_train_pred))\n",
        "\n",
        "# Evaluate on validation data\n",
        "y_val_pred = enhanced_pipeline.predict(x_val_df)\n",
        "print(\"\\nValidation Classification Report:\")\n",
        "print(classification_report(y_val_labels, y_val_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_full_df:\n",
            "['username', 'captions', 'biography', 'follower_count', 'following_count', 'post_count', 'label']\n",
            "\n",
            "First few rows of train_full_df:\n",
            "               username                                           captions  \\\n",
            "0            deparmedya  cumhuriyetimizin 100yılı kutlu olsun\\noriflame...   \n",
            "1            kafesfirin  bugün bir fincan köpüklü türk kahvesiyle taçla...   \n",
            "2              vimerang  saygı ve özlemle #atatürk #10kasım #10kasim193...   \n",
            "3     mustafa_yalcinn38  altınoluk çevre şehircilik ve iklim değişikliğ...   \n",
            "4  zorluenergysolutions  güne enerjik bir sohbet ile devam etmek ister ...   \n",
            "\n",
            "                                           biography  follower_count  \\\n",
            "0           #mediaplanning #mediabuying #sosyalmedya            1167   \n",
            "1  📍Söğütözü📍FTZ AVM\\n🛒Ankara macro▲center v...           11997   \n",
            "2       Dijital İletişim Yönetimi🎬info@vimerang.comq            2321   \n",
            "3                            Talas Belediye Başkanı           13647   \n",
            "4  Türkiye’nin 81 ilindeki en yaygın elektrikli ş...            7917   \n",
            "\n",
            "   following_count  post_count                 label  \n",
            "0              192         0.0                  tech  \n",
            "1               17         0.0                  food  \n",
            "2              454         0.0                  tech  \n",
            "3               29         0.0  health and lifestyle  \n",
            "4               11         0.0                  tech  \n"
          ]
        }
      ],
      "source": [
        "# After merging captions and profile data\n",
        "print(\"Columns in train_full_df:\")\n",
        "print(train_full_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of train_full_df:\")\n",
        "print(train_full_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features (X) columns:\n",
            "['captions', 'biography', 'follower_count', 'following_count', 'post_count']\n",
            "\n",
            "Labels (y) distribution:\n",
            "label\n",
            "food                    511\n",
            "health and lifestyle    502\n",
            "tech                    346\n",
            "entertainment           323\n",
            "fashion                 299\n",
            "travel                  294\n",
            "art                     191\n",
            "mom and children        149\n",
            "sports                  113\n",
            "gaming                   13\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Features and Labels\n",
        "print(\"Features (X) columns:\")\n",
        "print(X.columns.tolist())\n",
        "\n",
        "print(\"\\nLabels (y) distribution:\")\n",
        "print(y.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set columns:\n",
            "['captions', 'biography', 'follower_count', 'following_count', 'post_count']\n",
            "\n",
            "Validation set columns:\n",
            "['captions', 'biography', 'follower_count', 'following_count', 'post_count']\n",
            "\n",
            "Training set preview:\n",
            "                                               captions  \\\n",
            "2638  bugün sofralarımızın vazgeçilmezi #düşükprotei...   \n",
            "206   dün gece #kanald ekranlarında bizi 90lı yıllar...   \n",
            "2073  ulu önderimiz gazi mustafa kemal atatürkün bed...   \n",
            "354   formula 1 in teknoloji alt yapısı için lenovoy...   \n",
            "1736  yaşasin cumhuriyet\\n10102005\\nsağlıklı ve mutl...   \n",
            "\n",
            "                                              biography  follower_count  \\\n",
            "2638  Sosyal medya hesaplarımızın kullanım kuralları...            2562   \n",
            "206   Her cumartesi saat 20.00’de @kanald ekranların...           66804   \n",
            "2073  Cerrahpaşa Tıp Fakültesi, Nükleer Tıp Anabi...            3297   \n",
            "354   Herkes için daha akıllı teknoloji, Dünyanın v...           59517   \n",
            "1736                                                              24217   \n",
            "\n",
            "      following_count  post_count  \n",
            "2638                3         0.0  \n",
            "206                14         0.0  \n",
            "2073                0         0.0  \n",
            "354               134         0.0  \n",
            "1736             3634         0.0  \n",
            "\n",
            "Validation set preview:\n",
            "                                               captions  \\\n",
            "865   her ortamda her yerde iki kişi bir araya geldi...   \n",
            "2728  bademli profiterol görünce biz sana ısmarlamas...   \n",
            "1316  dünle beraber gitme dün yeni bir günle beraber...   \n",
            "2429  daima senin ışığında senin izinde\\ncumhuriyeti...   \n",
            "2448  avşanın meşhur adakarası üzümü coğrafi işaret ...   \n",
            "\n",
            "                                              biography  follower_count  \\\n",
            "865   𝘽𝙚𝙮𝙠𝙤𝙯 𝘽𝙚𝙡𝙚𝙙𝙞𝙮𝙚𝙨𝙞 𝙆𝙪̈𝙡𝙩𝙪̈𝙧 𝙫𝙚 𝙎𝙤𝙨𝙮𝙖𝙡 𝙄̇𝙨̧𝙡𝙚𝙧 𝙈...           34718   \n",
            "2728                                     İyi zamanlar 🎉          147860   \n",
            "1316  Photographer\\n #photohadrian \\n📸  Landscape ph...            8000   \n",
            "2429                                   #focusonsolution             121   \n",
            "2448  Otelimiz merkezde, denize sıfır 🏖\\nSabah akşam...            1027   \n",
            "\n",
            "      following_count  post_count  \n",
            "865                41         0.0  \n",
            "2728               19         0.0  \n",
            "1316             6833         0.0  \n",
            "2429                6         0.0  \n",
            "2448                7         0.0  \n"
          ]
        }
      ],
      "source": [
        "print(\"Training set columns:\")\n",
        "print(x_train_df.columns.tolist())\n",
        "\n",
        "print(\"\\nValidation set columns:\")\n",
        "print(x_val_df.columns.tolist())\n",
        "\n",
        "print(\"\\nTraining set preview:\")\n",
        "print(x_train_df.head())\n",
        "\n",
        "print(\"\\nValidation set preview:\")\n",
        "print(x_val_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ozhotelstr\n",
            "elleturkiye\n",
            "sozerinsaatorhangazi\n",
            "sanliurfapiazzaavym\n",
            "rusanozden\n",
            "*****\n",
            "['ozhotelstr', 'elleturkiye', 'sozerinsaatorhangazi', 'sanliurfapiazzaavym', 'rusanozden']\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the testing directory\n",
        "testing_dir = os.path.join(data_dir, 'testing')\n",
        "\n",
        "# File path for 'test-classification-round1.dat'\n",
        "test_data_path = os.path.join(testing_dir, 'test-classification-round1.dat')\n",
        "\n",
        "# Step 2: Preview First 5 Lines of the Test File\n",
        "with open(test_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for i, line in enumerate(fh):\n",
        "        print(line.strip())\n",
        "        if i == 4:  # Print only the first 5 lines\n",
        "            break\n",
        "\n",
        "print(\"*****\")\n",
        "\n",
        "# Step 3: Extract Usernames from Test Data\n",
        "test_unames = []\n",
        "with open(test_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        test_unames.append(line.strip())\n",
        "\n",
        "# Step 4: Verify Output\n",
        "print(test_unames[:5])  # Display the first 5 usernames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khu0eryhNZNN"
      },
      "source": [
        "# Naive Base Classifier\n",
        "\n",
        "### Now we can pass the numerical values to a classifier, Let's try Naive Base!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC7KXsQZL7Kp"
      },
      "source": [
        "# Like Count Prediction\n",
        "\n",
        "\n",
        "Here, we use the average like_count of the user's previous posts to predict each post's like_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nolpagasSuBq"
      },
      "outputs": [],
      "source": [
        "def predict_like_count(username, current_post=None):\n",
        "  def get_avg_like_count(posts:list):\n",
        "    total = 0.\n",
        "    for post in posts:\n",
        "      if current_post is not None and post[\"id\"] == current_post[\"id\"]:\n",
        "        continue\n",
        "\n",
        "      like_count = post.get(\"like_count\", 0)\n",
        "      if like_count is None:\n",
        "        like_count = 0\n",
        "      total += like_count\n",
        "\n",
        "    if len(posts) == 0:\n",
        "      return 0.\n",
        "\n",
        "    return total / len(posts)\n",
        "\n",
        "  if username in username2posts_train:\n",
        "    return get_avg_like_count(username2posts_train[username])\n",
        "  elif username in username2posts_test:\n",
        "    return get_avg_like_count(username2posts_test[username])\n",
        "  else:\n",
        "    print(f\"No data available for {username}\")\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZNWTjLZ6XAuj"
      },
      "outputs": [],
      "source": [
        "def log_mse_like_counts(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculate the Log Mean Squared Error (Log MSE) for like counts (log(like_count + 1)).\n",
        "\n",
        "  Parameters:\n",
        "  - y_true: array-like, actual like counts\n",
        "  - y_pred: array-like, predicted like counts\n",
        "\n",
        "  Returns:\n",
        "  - log_mse: float, Log Mean Squared Error\n",
        "  \"\"\"\n",
        "  # Ensure inputs are numpy arrays\n",
        "  y_true = np.array(y_true)\n",
        "  y_pred = np.array(y_pred)\n",
        "\n",
        "  # Log transformation: log(like_count + 1)\n",
        "  log_y_true = np.log1p(y_true)\n",
        "  log_y_pred = np.log1p(y_pred)\n",
        "\n",
        "  # Compute squared errors\n",
        "  squared_errors = (log_y_true - log_y_pred) ** 2\n",
        "\n",
        "  # Return the mean of squared errors\n",
        "  return np.mean(squared_errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1jETdAuXA0H",
        "outputId": "871164d0-74fb-49cd-ea77-b8a3e0793e81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log MSE Train= 1.2271047744059362\n"
          ]
        }
      ],
      "source": [
        "#@title Train Dataset evaluation\n",
        "\n",
        "y_like_count_train_true = []\n",
        "y_like_count_train_pred = []\n",
        "for uname, posts in username2posts_train.items():\n",
        "  for post in posts:\n",
        "    pred_val = predict_like_count(uname, post)\n",
        "    true_val = post.get(\"like_count\", 0)\n",
        "    if true_val is None:\n",
        "      true_val = 0\n",
        "\n",
        "    y_like_count_train_true.append(true_val)\n",
        "    y_like_count_train_pred.append(pred_val)\n",
        "\n",
        "print(f\"Log MSE Train= {log_mse_like_counts(y_like_count_train_true, y_like_count_train_pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F02V1wO-WBMV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed data saved to: c:\\Users\\itsmm\\OneDrive\\Desktop\\CS412\\CS412-InstagramInfluencersAnalysis\\data\\output\\test-regression-round1.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the testing directory\n",
        "testing_dir = os.path.join(data_dir, 'testing')\n",
        "\n",
        "# File path for 'test-regression-round1.jsonl'\n",
        "test_dataset_path = os.path.join(testing_dir, 'test-regression-round1.jsonl')\n",
        "\n",
        "# File path for output\n",
        "output_dir = os.path.join(data_dir, 'output')\n",
        "os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
        "output_file_path = os.path.join(output_dir, 'test-regression-round1.jsonl')\n",
        "\n",
        "# Step 2: Process the Test Dataset\n",
        "to_predict_like_counts_usernames = []\n",
        "output_list = []\n",
        "\n",
        "with open(test_dataset_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "\n",
        "        # Perform prediction\n",
        "        pred_val = predict_like_count(sample[\"username\"])  # Ensure `predict_like_count` is defined\n",
        "        sample[\"like_count\"] = int(pred_val)\n",
        "        output_list.append(sample)\n",
        "\n",
        "# Step 3: Save the Output to a File\n",
        "with open(output_file_path, \"wt\", encoding=\"utf-8\") as of:\n",
        "    json.dump(output_list, of)\n",
        "\n",
        "# Step 4: Output Verification\n",
        "print(f\"Processed data saved to: {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blI2SqvvvOF8",
        "outputId": "fe1e72e3-3ad0-486d-d054-3e90e8c66669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'caption': 'KOZA 2023 2.si Damla’nın koleksiyonu, Latincede ‘Memento Mori’ '\n",
            "             'olarak bilinen ‘ölümlü olduğunu hatırla’ anlamındaki ifadeden '\n",
            "             'esinleniyor. Koleksiyon, hayatın ve ölümün, para, işçi, kral ve '\n",
            "             'kraliçe kavramları üzerinden yaratıcı görünümlerle bir araya '\n",
            "             'getirilmesini amaçlıyor. Ölüm sembollerinden esinlenen desenler '\n",
            "             'kullanan Damla, “kağıt parçasından ibaret olmak” kavramını '\n",
            "             'vurguluyor. Koleksiyon, yaşamın ve ölümün aynı anda ifade '\n",
            "             'edilmesini hedefliyor; kırmızı ve mavi ışıklarla veya '\n",
            "             'gözlüklerle görülen hologram efekti kullanılarak bu konsept '\n",
            "             'sahneye taşınıyor. Kırmızı renk ölümü, mavi ise yaşamı '\n",
            "             'simgeliyor. Koleksiyon, ofis giyimlerinden esinlenerek '\n",
            "             'kravatlar, gömlekler ve evrak çantaları içeriyor. Klasik sivri '\n",
            "             'burun çizmelerin üzerine spor ayakkabıların üst yüzeyi '\n",
            "             'yerleştirilerek, iş dünyasının koşuşturması ve cenaze '\n",
            "             'temalarının aynı anda ifade edilmesi amaçlanıyor. Para kazanma '\n",
            "             'arzusu, kırmızı zambak desenleri ve büyük mücevher görünümleri '\n",
            "             'ile koleksiyon tamamlanıyor.\\n'\n",
            "             '\\n'\n",
            "             'Tebrikler Damla!\\n'\n",
            "             '\\n'\n",
            "             '#GencModaTasarimcilari #Koza2023 #KozaYarismasi '\n",
            "             '#TasarimYarismasi #Moda #Fashion #ModaTasarımı',\n",
            "  'comments_count': 2,\n",
            "  'id': '18144550534306740',\n",
            "  'like_count': 158,\n",
            "  'media_type': 'CAROUSEL_ALBUM',\n",
            "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/397997154_1016992459537522_4925783512176260397_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=7V_eObkFeK4AX-LMtsK&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDEqDhzaTO3ezV-veT6cJFCOcAEyeVzHR6si9n33N6G5A&oe=6551B6B9',\n",
            "  'timestamp': '2023-11-02 15:49:22',\n",
            "  'username': 'kozayarismasi'},\n",
            " {'caption': 'Tüm Türkiye ve Avrupa’ya sevkiyatlarımız aralıksız devam ediyor! '\n",
            "             'Aracımız Bursa’dan Ordu’ya müşterimizin ürünleri için yola '\n",
            "             'çıkıyor.\\n'\n",
            "             '\\n'\n",
            "             '👉Tuna Mah. Etibank Cad. No:134 Osmangazi/BURSA\\n'\n",
            "             '\\n'\n",
            "             'www.celikbeymobilya.com sitemizden tüm modelleri '\n",
            "             'inceleyebilirsiniz. \\n'\n",
            "             '\\n'\n",
            "             '#bursa #almanya #fransa',\n",
            "  'comments_count': 0,\n",
            "  'id': '17995331788956693',\n",
            "  'like_count': 99,\n",
            "  'media_type': 'VIDEO',\n",
            "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m82/BF4767CB85BDFB8ADCCCA8F15B8C20B5_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmNsaXBzLnVua25vd24tQzMuNzIwLmRhc2hfYmFzZWxpbmVfMV92MSJ9&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1259525061418244_1441854817&_nc_vs=HBksFQIYT2lnX3hwdl9yZWVsc19wZXJtYW5lbnRfcHJvZC9CRjQ3NjdDQjg1QkRGQjhBRENDQ0E4RjE1QjhDMjBCNV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dBRWdfaFZfcDVCYk5HZ0NBQTlzVURvZW5mZ3FicV9FQUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvS3uOiQ0P8%2FFQIoAkMzLBdAJO%2Bdsi0OVhgSZGFzaF9iYXNlbGluZV8xX3YxEQB1AAA%3D&ccb=9-4&oh=00_AfAm22JssMPaUlQe3rpYsFWBhFb5mUgolTCdhV0Xgm4AnA&oe=6556A482&_nc_sid=1d576d&_nc_rid=cd9a998e44',\n",
            "  'timestamp': '2023-08-19 13:46:02',\n",
            "  'username': 'celikbeymobilya'},\n",
            " {'caption': '🤩\\n'\n",
            "             '\\n'\n",
            "             '.\\n'\n",
            "             '.\\n'\n",
            "             'Daha Fazlası İçin Beğenmeyi ve Takip Etmeyi Unutmayın\\n'\n",
            "             '.\\n'\n",
            "             'girisimci_muhendis ✅\\n'\n",
            "             '.\\n'\n",
            "             '📣Bu Bilgi Hakkında Ne Düşünüyorsunuz.\\n'\n",
            "             '.\\n'\n",
            "             '✅Görmesini İstediğin Arkadaşını Etiketle.\\n'\n",
            "             '.\\n'\n",
            "             '🔔Gönderi Bildirimlerini Açarak, Bilgileri Anında '\n",
            "             'Öğrenebilirsiniz.\\n'\n",
            "             '\\n'\n",
            "             '.\\n'\n",
            "             '\\n'\n",
            "             'Source: Unknown\\n'\n",
            "             'Dm for Credit or removal\\n'\n",
            "             '.\\n'\n",
            "             '.............................................................\\n'\n",
            "             'All rights and credits reserved to the respective owner(s). If '\n",
            "             'you are the main copyright owner rather than the one mentioned '\n",
            "             'here of this content, contact me to claim credit or content '\n",
            "             'removal',\n",
            "  'comments_count': 75,\n",
            "  'id': '18302703232191518',\n",
            "  'like_count': 1224,\n",
            "  'media_type': 'VIDEO',\n",
            "  'media_url': None,\n",
            "  'timestamp': '2023-10-02 06:53:33',\n",
            "  'username': 'girisimci_muhendis'}]\n"
          ]
        }
      ],
      "source": [
        "# output_list first 3 items\n",
        "pprint(output_list[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYP0TzdoGbpt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
