{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBOlFYpnlr0N"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PM5EnXhLk6BL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgAdz_b8xp8w",
        "outputId": "14cd6ea6-4a61-4b22-f2a6-59088a78e58a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\itsmm\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Turkish StopWords\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "turkish_stopwords = stopwords.words('turkish')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plt9-VkNmlmH"
      },
      "source": [
        "# Influencer Category Classification\n",
        "\n",
        "\n",
        "\n",
        "1.   Read Data\n",
        "2.   Preprocess Data\n",
        "3.   Prepare Model\n",
        "4.   Predict Test Data\n",
        "4.   Save outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8DeBh-b7lrEs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows of the training classification DataFrame:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>taskirancemal</td>\n",
              "      <td>mom and children</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tam_kararinda</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spart4nn</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sosyalyiyiciler</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sonaydizdarahad</td>\n",
              "      <td>mom and children</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           user_id          category\n",
              "0    taskirancemal  mom and children\n",
              "1    tam_kararinda              food\n",
              "2         spart4nn              food\n",
              "3  sosyalyiyiciler              food\n",
              "4  sonaydizdarahad  mom and children"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the training directory\n",
        "training_dir = os.path.join(data_dir, 'training')\n",
        "\n",
        "# File path for 'train-classification.csv'\n",
        "train_classification_path = os.path.join(training_dir, 'train-classification.csv')\n",
        "\n",
        "# Step 2: Load Data Dynamically\n",
        "train_classification_df = pd.read_csv(train_classification_path)\n",
        "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
        "\n",
        "# Step 3: Unify Labels\n",
        "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
        "\n",
        "# Step 4: Create User-to-Category Mapping\n",
        "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\n",
        "\n",
        "# Step 5: Verify Output\n",
        "print(\"First few rows of the training classification DataFrame:\")\n",
        "train_classification_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "SfD7BQ3hE5Jh",
        "outputId": "a67f08dd-ab3e-491f-bfe2-a14da0e961cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>art</th>\n",
              "      <td>191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>entertainment</th>\n",
              "      <td>323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fashion</th>\n",
              "      <td>299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gaming</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>health and lifestyle</th>\n",
              "      <td>503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mom and children</th>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sports</th>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tech</th>\n",
              "      <td>346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>travel</th>\n",
              "      <td>294</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      user_id\n",
              "category                     \n",
              "art                       191\n",
              "entertainment             323\n",
              "fashion                   299\n",
              "food                      511\n",
              "gaming                     13\n",
              "health and lifestyle      503\n",
              "mom and children          149\n",
              "sports                    113\n",
              "tech                      346\n",
              "travel                    294"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# stats about the labels\n",
        "train_classification_df.groupby(\"category\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mhckiEfD2gg_",
        "outputId": "af451a4d-8825-4506-fb70-41a6ef845aaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mom and children'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "username2_category[\"sonaydizdarahad\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GT_IcUM2nGBH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Training Users: 2741\n",
            "Number of Testing Users: 2674\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the training directory\n",
        "training_dir = os.path.join(data_dir, 'training')\n",
        "\n",
        "# File path for 'training-dataset.jsonl.gz'\n",
        "train_data_path = os.path.join(training_dir, 'training-dataset.jsonl.gz')\n",
        "\n",
        "# Step 2: Initialize Dictionaries for Data\n",
        "username2posts_train = dict()\n",
        "username2profile_train = dict()\n",
        "\n",
        "username2posts_test = dict()\n",
        "username2profile_test = dict()\n",
        "\n",
        "# Step 3: Process Data from 'training-dataset.jsonl.gz'\n",
        "with gzip.open(train_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "\n",
        "        profile = sample[\"profile\"]\n",
        "        username = profile.get(\"username\", \"\").strip()  # Handle missing or empty usernames\n",
        "        if not username:\n",
        "            continue  # Skip if username is missing or empty\n",
        "\n",
        "        if username in username2_category:\n",
        "            # Train data info\n",
        "            username2posts_train[username] = sample[\"posts\"]\n",
        "            username2profile_train[username] = profile\n",
        "        else:\n",
        "            # Test data info\n",
        "            username2posts_test[username] = sample[\"posts\"]\n",
        "            username2profile_test[username] = profile\n",
        "\n",
        "# Step 4: Verify Output\n",
        "print(f\"Number of Training Users: {len(username2posts_train)}\")\n",
        "print(f\"Number of Testing Users: {len(username2posts_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "djeF1GQ1oy3v",
        "outputId": "5c2ead24-d7d1-4df8-bec0-bf2152c76832"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>id</th>\n",
              "      <th>full_name</th>\n",
              "      <th>biography</th>\n",
              "      <th>category_name</th>\n",
              "      <th>post_count</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>is_business_account</th>\n",
              "      <th>is_private</th>\n",
              "      <th>...</th>\n",
              "      <th>business_category_name</th>\n",
              "      <th>overall_category_name</th>\n",
              "      <th>category_enum</th>\n",
              "      <th>is_verified_by_mv4b</th>\n",
              "      <th>is_regulated_c18</th>\n",
              "      <th>profile_pic_url</th>\n",
              "      <th>should_show_category</th>\n",
              "      <th>should_show_public_contacts</th>\n",
              "      <th>show_account_transparency_details</th>\n",
              "      <th>profile_picture_base64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>deparmedya</td>\n",
              "      <td>3170700063</td>\n",
              "      <td>Depar Medya</td>\n",
              "      <td>#mediaplanning #mediabuying #sosyalmedya</td>\n",
              "      <td>Local business</td>\n",
              "      <td>None</td>\n",
              "      <td>1167</td>\n",
              "      <td>192</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>LOCAL</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     username          id    full_name  \\\n",
              "0  deparmedya  3170700063  Depar Medya   \n",
              "\n",
              "                                  biography   category_name post_count  \\\n",
              "0  #mediaplanning #mediabuying #sosyalmedya  Local business       None   \n",
              "\n",
              "  follower_count following_count is_business_account is_private  ...  \\\n",
              "0           1167             192                True      False  ...   \n",
              "\n",
              "  business_category_name overall_category_name category_enum  \\\n",
              "0                   None                  None         LOCAL   \n",
              "\n",
              "  is_verified_by_mv4b is_regulated_c18  \\\n",
              "0               False            False   \n",
              "\n",
              "                                     profile_pic_url should_show_category  \\\n",
              "0  https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....                 True   \n",
              "\n",
              "  should_show_public_contacts show_account_transparency_details  \\\n",
              "0                        True                              True   \n",
              "\n",
              "                              profile_picture_base64  \n",
              "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
              "\n",
              "[1 rows x 44 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Profile Dataframe\n",
        "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
        "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)\n",
        "\n",
        "train_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "15tvVohUBHK9",
        "outputId": "15be2533-51b2-41e8-e0f2-26a44933dbae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>id</th>\n",
              "      <th>full_name</th>\n",
              "      <th>biography</th>\n",
              "      <th>category_name</th>\n",
              "      <th>post_count</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>is_business_account</th>\n",
              "      <th>is_private</th>\n",
              "      <th>...</th>\n",
              "      <th>business_category_name</th>\n",
              "      <th>overall_category_name</th>\n",
              "      <th>category_enum</th>\n",
              "      <th>is_verified_by_mv4b</th>\n",
              "      <th>is_regulated_c18</th>\n",
              "      <th>profile_pic_url</th>\n",
              "      <th>should_show_category</th>\n",
              "      <th>should_show_public_contacts</th>\n",
              "      <th>show_account_transparency_details</th>\n",
              "      <th>profile_picture_base64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>beyazyakaliyiz</td>\n",
              "      <td>8634457436</td>\n",
              "      <td>Selam Beyaz YakalÄ±</td>\n",
              "      <td>Beyaz yakalÄ±larÄ±n dÃ¼nyasÄ±na hoÅŸgeldiniz ðŸ˜€ðŸ˜€ðŸ˜€</td>\n",
              "      <td>Personal blog</td>\n",
              "      <td>None</td>\n",
              "      <td>1265</td>\n",
              "      <td>665</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>PERSONAL_BLOG</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>https://instagram.fist6-1.fna.fbcdn.net/v/t51....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         username          id           full_name  \\\n",
              "0  beyazyakaliyiz  8634457436  Selam Beyaz YakalÄ±   \n",
              "\n",
              "                                     biography  category_name post_count  \\\n",
              "0  Beyaz yakalÄ±larÄ±n dÃ¼nyasÄ±na hoÅŸgeldiniz ðŸ˜€ðŸ˜€ðŸ˜€  Personal blog       None   \n",
              "\n",
              "  follower_count following_count is_business_account is_private  ...  \\\n",
              "0           1265             665                True      False  ...   \n",
              "\n",
              "  business_category_name overall_category_name  category_enum  \\\n",
              "0                   None                  None  PERSONAL_BLOG   \n",
              "\n",
              "  is_verified_by_mv4b is_regulated_c18  \\\n",
              "0               False            False   \n",
              "\n",
              "                                     profile_pic_url should_show_category  \\\n",
              "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
              "\n",
              "  should_show_public_contacts show_account_transparency_details  \\\n",
              "0                        True                              True   \n",
              "\n",
              "                              profile_picture_base64  \n",
              "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
              "\n",
              "[1 rows x 44 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_classification_df:\n",
            "['user_id', 'category']\n",
            "\n",
            "First few rows of the training classification DataFrame:\n",
            "           user_id          category\n",
            "0    taskirancemal  mom and children\n",
            "1    tam_kararinda              food\n",
            "2         spart4nn              food\n",
            "3  sosyalyiyiciler              food\n",
            "4  sonaydizdarahad  mom and children\n",
            "\n",
            "Label distribution:\n",
            "category\n",
            "food                    511\n",
            "health and lifestyle    503\n",
            "tech                    346\n",
            "entertainment           323\n",
            "fashion                 299\n",
            "travel                  294\n",
            "art                     191\n",
            "mom and children        149\n",
            "sports                  113\n",
            "gaming                   13\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Verify Output\n",
        "print(\"Columns in train_classification_df:\")\n",
        "print(train_classification_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of the training classification DataFrame:\")\n",
        "print(train_classification_df.head())\n",
        "\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(train_classification_df['category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_profile_df:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
            "\n",
            "First few rows of train_profile_df:\n",
            "     username          id    full_name  \\\n",
            "0  deparmedya  3170700063  Depar Medya   \n",
            "\n",
            "                                  biography   category_name post_count  \\\n",
            "0  #mediaplanning #mediabuying #sosyalmedya  Local business       None   \n",
            "\n",
            "  follower_count following_count is_business_account is_private  ...  \\\n",
            "0           1167             192                True      False  ...   \n",
            "\n",
            "  business_category_name overall_category_name category_enum  \\\n",
            "0                   None                  None         LOCAL   \n",
            "\n",
            "  is_verified_by_mv4b is_regulated_c18  \\\n",
            "0               False            False   \n",
            "\n",
            "                                     profile_pic_url should_show_category  \\\n",
            "0  https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....                 True   \n",
            "\n",
            "  should_show_public_contacts show_account_transparency_details  \\\n",
            "0                        True                              True   \n",
            "\n",
            "                              profile_picture_base64  \n",
            "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
            "\n",
            "[1 rows x 44 columns]\n",
            "\n",
            "Columns in test_profile_df:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
            "\n",
            "First few rows of test_profile_df:\n",
            "         username          id           full_name  \\\n",
            "0  beyazyakaliyiz  8634457436  Selam Beyaz YakalÄ±   \n",
            "\n",
            "                                     biography  category_name post_count  \\\n",
            "0  Beyaz yakalÄ±larÄ±n dÃ¼nyasÄ±na hoÅŸgeldiniz ðŸ˜€ðŸ˜€ðŸ˜€  Personal blog       None   \n",
            "\n",
            "  follower_count following_count is_business_account is_private  ...  \\\n",
            "0           1265             665                True      False  ...   \n",
            "\n",
            "  business_category_name overall_category_name  category_enum  \\\n",
            "0                   None                  None  PERSONAL_BLOG   \n",
            "\n",
            "  is_verified_by_mv4b is_regulated_c18  \\\n",
            "0               False            False   \n",
            "\n",
            "                                     profile_pic_url should_show_category  \\\n",
            "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
            "\n",
            "  should_show_public_contacts show_account_transparency_details  \\\n",
            "0                        True                              True   \n",
            "\n",
            "                              profile_picture_base64  \n",
            "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
            "\n",
            "[1 rows x 44 columns]\n"
          ]
        }
      ],
      "source": [
        "# Profile Dataframe\n",
        "print(\"Columns in train_profile_df:\")\n",
        "print(train_profile_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of train_profile_df:\")\n",
        "print(train_profile_df.head(1))\n",
        "\n",
        "print(\"\\nColumns in test_profile_df:\")\n",
        "print(test_profile_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of test_profile_df:\")\n",
        "print(test_profile_df.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_profile_df after dropping:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'bio_links', 'is_professional_account', 'business_category_name', 'overall_category_name']\n",
            "\n",
            "Columns in test_profile_df after dropping:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'bio_links', 'is_professional_account', 'business_category_name', 'overall_category_name']\n"
          ]
        }
      ],
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = [\n",
        "    'highlight_reel_count', 'entities', 'ai_agent_type', 'fb_profile_biolink',\n",
        "    'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid',\n",
        "    'has_clips', 'hide_like_and_view_counts', 'is_supervision_enabled',\n",
        "    'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user',\n",
        "    'is_embeds_disabled', 'is_joined_recently', 'business_address_json',\n",
        "    'business_contact_method', 'business_email', 'business_phone_number',\n",
        "    'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18',\n",
        "    'profile_pic_url', 'should_show_category', 'should_show_public_contacts',\n",
        "    'show_account_transparency_details', 'profile_picture_base64'\n",
        "]\n",
        "\n",
        "# Dropping specified columns from train_profile_df\n",
        "train_profile_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Dropping specified columns from test_profile_df\n",
        "test_profile_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Verify columns in train_profile_df after dropping\n",
        "print(\"Columns in train_profile_df after dropping:\")\n",
        "print(train_profile_df.columns.tolist())\n",
        "\n",
        "# Verify columns in test_profile_df after dropping\n",
        "print(\"\\nColumns in test_profile_df after dropping:\")\n",
        "print(test_profile_df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z5UY0eYLsoTr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "Best Params: {'clf__C': 1, 'preprocessor__bio_tfidf__ngram_range': (1, 1), 'preprocessor__captions_tfidf__ngram_range': (1, 2)}\n",
            "Best CV Accuracy: 0.6455259262035492\n",
            "Training Accuracy: 0.9516423357664233\n",
            "\n",
            "Training Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                 art       0.94      0.95      0.94       153\n",
            "       entertainment       0.97      0.91      0.94       258\n",
            "             fashion       0.92      0.97      0.95       239\n",
            "                food       0.98      0.97      0.97       409\n",
            "              gaming       1.00      1.00      1.00        10\n",
            "health and lifestyle       0.97      0.91      0.94       402\n",
            "    mom and children       0.92      0.97      0.94       119\n",
            "              sports       0.98      0.99      0.98        90\n",
            "                tech       0.92      0.97      0.95       277\n",
            "              travel       0.94      0.96      0.95       235\n",
            "\n",
            "            accuracy                           0.95      2192\n",
            "           macro avg       0.95      0.96      0.96      2192\n",
            "        weighted avg       0.95      0.95      0.95      2192\n",
            "\n",
            "Validation Accuracy: 0.644808743169399\n",
            "\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                 art       0.29      0.18      0.23        38\n",
            "       entertainment       0.44      0.40      0.42        65\n",
            "             fashion       0.49      0.55      0.52        60\n",
            "                food       0.89      0.87      0.88       102\n",
            "              gaming       0.50      0.33      0.40         3\n",
            "health and lifestyle       0.75      0.71      0.73       100\n",
            "    mom and children       0.48      0.50      0.49        30\n",
            "              sports       0.65      0.65      0.65        23\n",
            "                tech       0.67      0.83      0.74        69\n",
            "              travel       0.65      0.68      0.66        59\n",
            "\n",
            "            accuracy                           0.64       549\n",
            "           macro avg       0.58      0.57      0.57       549\n",
            "        weighted avg       0.64      0.64      0.64       549\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Preprocessing Function\n",
        "import re\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    text = text.casefold()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼0-9\\s#@]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Build Corpus and Labels\n",
        "corpus = []\n",
        "train_usernames = []\n",
        "\n",
        "for username, posts in username2posts_train.items():\n",
        "    train_usernames.append(username)\n",
        "    cleaned_captions = []\n",
        "    for post in posts:\n",
        "        post_caption = post.get(\"caption\", \"\")\n",
        "        if post_caption is None:\n",
        "            continue\n",
        "        post_caption = preprocess_text(post_caption)\n",
        "        if post_caption != \"\":\n",
        "            cleaned_captions.append(post_caption)\n",
        "    user_post_captions = \"\\n\".join(cleaned_captions)\n",
        "    corpus.append(user_post_captions)\n",
        "\n",
        "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
        "\n",
        "# Incorporate Metadata\n",
        "records = []\n",
        "for idx, username in enumerate(train_usernames):\n",
        "    profile = username2profile_train.get(username, {})\n",
        "    biography_text = str(profile.get(\"biography\", \"\") or \"\")\n",
        "    follower_count = profile.get(\"follower_count\", 0)\n",
        "    following_count = profile.get(\"following_count\", 0)\n",
        "    post_count = profile.get(\"post_count\", 0) if profile.get(\"post_count\") else 0\n",
        "    row_dict = {\n",
        "        \"username\": username,\n",
        "        \"captions\": corpus[idx],\n",
        "        \"biography\": biography_text,\n",
        "        \"follower_count\": follower_count,\n",
        "        \"following_count\": following_count,\n",
        "        \"post_count\": post_count,\n",
        "        \"label\": y_train[idx]\n",
        "    }\n",
        "    records.append(row_dict)\n",
        "\n",
        "train_full_df = pd.DataFrame(records)\n",
        "\n",
        "# Define Pipeline Components\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Train-Validation Split\n",
        "X = train_full_df.drop(columns=[\"label\"])\n",
        "y = train_full_df[\"label\"]\n",
        "\n",
        "x_train_df, x_val_df, y_train_labels, y_val_labels = train_test_split(\n",
        "    X, \n",
        "    y, \n",
        "    test_size=0.2, \n",
        "    stratify=y, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define ColumnTransformer\n",
        "numeric_features = [\"follower_count\", \"following_count\", \"post_count\"]\n",
        "text_features_caps = \"captions\"\n",
        "text_features_bio = \"biography\"\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"captions_tfidf\", TfidfVectorizer(\n",
        "             stop_words=turkish_stopwords, \n",
        "             max_features=5000\n",
        "         ), text_features_caps),\n",
        "        (\"bio_tfidf\", TfidfVectorizer(\n",
        "             stop_words=turkish_stopwords, \n",
        "             max_features=5000\n",
        "         ), text_features_bio),\n",
        "        (\"numeric_scaler\", MinMaxScaler(), numeric_features)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# Build ImbPipeline (SMOTE + Classifier)\n",
        "pipeline = ImbPipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=42, sampling_strategy=\"auto\")),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        class_weight='balanced',\n",
        "        solver='liblinear',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Define a param grid\n",
        "param_grid = {\n",
        "    \"preprocessor__captions_tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"preprocessor__bio_tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"clf__C\": [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Initialize and fit GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(x_train_df, y_train_labels)\n",
        "\n",
        "print(\"Best Params:\", grid_search.best_params_)\n",
        "print(\"Best CV Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "\n",
        "# **New Section: Evaluate on Training Data**\n",
        "# Predict on the training data\n",
        "y_train_pred = best_pipeline.predict(x_train_df)\n",
        "\n",
        "# Calculate training accuracy\n",
        "train_acc = accuracy_score(y_train_labels, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "\n",
        "# Generate and print the training classification report\n",
        "print(\"\\nTraining Classification Report:\\n\",\n",
        "      classification_report(y_train_labels, y_train_pred, zero_division=0))\n",
        "\n",
        "# **End of New Section**\n",
        "\n",
        "# Predict on the validation data\n",
        "y_val_pred = best_pipeline.predict(x_val_df)\n",
        "\n",
        "# Calculate validation accuracy\n",
        "val_acc = accuracy_score(y_val_labels, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_acc)\n",
        "\n",
        "# Generate and print the validation classification report\n",
        "print(\"\\nClassification Report:\\n\",\n",
        "      classification_report(y_val_labels, y_val_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2192 entries, 2638 to 2667\n",
            "Data columns (total 6 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   username         2192 non-null   object \n",
            " 1   captions         2192 non-null   object \n",
            " 2   biography        2192 non-null   object \n",
            " 3   follower_count   2192 non-null   int64  \n",
            " 4   following_count  2192 non-null   int64  \n",
            " 5   post_count       2192 non-null   float64\n",
            "dtypes: float64(1), int64(2), object(3)\n",
            "memory usage: 119.9+ KB\n",
            "None\n",
            "\n",
            "Missing values in training data:\n",
            "username           0\n",
            "captions           0\n",
            "biography          0\n",
            "follower_count     0\n",
            "following_count    0\n",
            "post_count         0\n",
            "dtype: int64\n",
            "\n",
            "Performing cross-validation with tqdm progress bar...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cross-Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [29:04<00:00, 348.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cross-validation scores: [0.6560364464692483, 0.662870159453303, 0.6552511415525114, 0.6643835616438356, 0.6484018264840182]\n",
            "Mean CV accuracy: 0.657 (+/- 0.012)\n",
            "\n",
            "Training Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "                 art       0.99      1.00      0.99       153\n",
            "       entertainment       1.00      0.97      0.98       258\n",
            "             fashion       0.98      1.00      0.99       239\n",
            "                food       1.00      1.00      1.00       409\n",
            "              gaming       1.00      1.00      1.00        10\n",
            "health and lifestyle       1.00      0.98      0.99       402\n",
            "    mom and children       1.00      1.00      1.00       119\n",
            "              sports       1.00      1.00      1.00        90\n",
            "                tech       0.98      1.00      0.99       277\n",
            "              travel       0.99      1.00      0.99       235\n",
            "\n",
            "            accuracy                           0.99      2192\n",
            "           macro avg       0.99      0.99      0.99      2192\n",
            "        weighted avg       0.99      0.99      0.99      2192\n",
            "\n",
            "\n",
            "Validation Classification Report (with username lookup):\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "                 art       0.31      0.13      0.19        38\n",
            "       entertainment       0.37      0.32      0.34        65\n",
            "             fashion       0.58      0.73      0.65        60\n",
            "                food       0.85      0.91      0.88       102\n",
            "              gaming       0.00      0.00      0.00         3\n",
            "health and lifestyle       0.64      0.71      0.67       100\n",
            "    mom and children       0.56      0.47      0.51        30\n",
            "              sports       0.77      0.74      0.76        23\n",
            "                tech       0.75      0.83      0.79        69\n",
            "              travel       0.68      0.64      0.66        59\n",
            "\n",
            "            accuracy                           0.66       549\n",
            "           macro avg       0.55      0.55      0.54       549\n",
            "        weighted avg       0.63      0.66      0.64       549\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# ------------------- Imports -------------------\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# scikit-learn utilities\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# For progress bar in cross-validation\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------- Enhanced Text Preprocessing -------------------\n",
        "def advanced_text_preprocessing(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    text = text.lower()\n",
        "    \n",
        "    # Extract hashtags and mentions\n",
        "    hashtags = re.findall(r'#\\w+', text)\n",
        "    mentions = re.findall(r'@\\w+', text)\n",
        "    \n",
        "    # Densities\n",
        "    text_length = len(text)\n",
        "    hashtag_density = len(hashtags) / (text_length + 1)\n",
        "    mention_density = len(mentions) / (text_length + 1)\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼0-9\\s#@]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Add density info as extra pseudo-tokens\n",
        "    density_info = f\" hashtag_density_{hashtag_density:.2f} mention_density_{mention_density:.2f}\"\n",
        "    \n",
        "    # Re-append the extracted hashtags and mentions\n",
        "    return f\"{text} {' '.join(hashtags)} {' '.join(mentions)} {density_info}\".strip()\n",
        "\n",
        "# ------------------- Enhanced Numeric Feature Extraction -------------------\n",
        "def extract_numeric_features(X):\n",
        "    # If X is a DataFrame, ensure columns exist\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        cols = ['follower_count', 'following_count', 'post_count']\n",
        "        if 'account_age_days' in X.columns:\n",
        "            cols.append('account_age_days')\n",
        "        \n",
        "        df = X[cols].copy()\n",
        "    else:\n",
        "        # If it's a NumPy array\n",
        "        df = pd.DataFrame(\n",
        "            X, \n",
        "            columns=['follower_count', 'following_count', 'post_count', 'account_age_days'][:X.shape[1]]\n",
        "        )\n",
        "    \n",
        "    df = df.fillna(0)\n",
        "    \n",
        "    df['following_count_safe'] = df['following_count'].replace(0, 1)\n",
        "    df['follower_count_safe']  = df['follower_count'].replace(0, 1)\n",
        "    \n",
        "    df['follower_ratio']   = df['follower_count'] / df['following_count_safe']\n",
        "    df['post_density']     = df['post_count'] / df['follower_count_safe']\n",
        "    df['engagement_score'] = np.log1p(df['follower_count']) * np.log1p(df['post_count'])\n",
        "    \n",
        "    if 'account_age_days' in df.columns:\n",
        "        df['activity_ratio'] = df['post_count'] / (df['account_age_days'] + 1)\n",
        "    else:\n",
        "        df['activity_ratio'] = df['post_count']\n",
        "    \n",
        "    df['follower_growth_rate'] = df['follower_count'] / (df['post_count'] + 1)\n",
        "    df['relative_engagement']  = (df['follower_count'] * df['post_count']) / (df['following_count_safe'])\n",
        "    \n",
        "    # Log transforms\n",
        "    df['log_followers']  = np.log1p(df['follower_count'])\n",
        "    df['log_following']  = np.log1p(df['following_count'])\n",
        "    df['log_posts']      = np.log1p(df['post_count'])\n",
        "    \n",
        "    df.drop(['following_count_safe','follower_count_safe'], axis=1, inplace=True)\n",
        "    \n",
        "    return df.values\n",
        "\n",
        "# ------------------- Build Pipeline -------------------\n",
        "def build_enhanced_pipeline(stopwords_list):\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('captions_tfidf', \n",
        "             TfidfVectorizer(\n",
        "                 stop_words=stopwords_list,\n",
        "                 max_features=3000,   \n",
        "                 ngram_range=(1, 1),  \n",
        "                 min_df=2,\n",
        "                 max_df=0.95\n",
        "             ), \n",
        "             'captions_clean'),\n",
        "            \n",
        "            ('bio_tfidf', \n",
        "             TfidfVectorizer(\n",
        "                 stop_words=stopwords_list,\n",
        "                 max_features=2000,   \n",
        "                 ngram_range=(1, 1),  \n",
        "                 min_df=2,\n",
        "                 max_df=0.95\n",
        "             ), \n",
        "             'biography_clean'),\n",
        "            \n",
        "            ('numeric', \n",
        "             Pipeline([\n",
        "                 ('feat_eng', FunctionTransformer(extract_numeric_features)),\n",
        "                 ('scaler', MinMaxScaler())\n",
        "             ]), \n",
        "             ['follower_count', 'following_count', 'post_count']\n",
        "             # add 'account_age_days' if available\n",
        "            )\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    \n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=100,    \n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        max_features='sqrt',\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=50,     \n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    lr = LogisticRegression(\n",
        "        C=2.0,\n",
        "        class_weight='balanced',\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', rf),\n",
        "            ('gb', gb),\n",
        "            ('lr', lr)\n",
        "        ],\n",
        "        voting='soft'\n",
        "    )\n",
        "    \n",
        "    pipeline = ImbPipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('classifier', ensemble)\n",
        "    ])\n",
        "    \n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# ------------------- Example Usage -------------------\n",
        "print(\"Training data info:\")\n",
        "print(x_train_df.info())\n",
        "print(\"\\nMissing values in training data:\")\n",
        "print(x_train_df.isnull().sum())\n",
        "\n",
        "# 1) Preprocess text once\n",
        "x_train_df['captions_clean'] = x_train_df['captions'].apply(advanced_text_preprocessing)\n",
        "x_train_df['biography_clean'] = x_train_df['biography'].apply(advanced_text_preprocessing)\n",
        "\n",
        "x_val_df['captions_clean'] = x_val_df['captions'].apply(advanced_text_preprocessing)\n",
        "x_val_df['biography_clean'] = x_val_df['biography'].apply(advanced_text_preprocessing)\n",
        "\n",
        "# 2) Convert y_train_labels to NumPy for direct integer indexing\n",
        "y_train_labels_array = y_train_labels.values\n",
        "\n",
        "# 3) Build pipeline\n",
        "enhanced_pipeline = build_enhanced_pipeline(stopwords_list=turkish_stopwords)\n",
        "\n",
        "# 4) Manual Cross-Validation with tqdm progress bar\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "\n",
        "print(\"\\nPerforming cross-validation with tqdm progress bar...\")\n",
        "for fold_idx, (train_idx, test_idx) in enumerate(\n",
        "    tqdm(cv.split(x_train_df, y_train_labels_array), \n",
        "         total=cv.get_n_splits(), \n",
        "         desc='Cross-Validation')\n",
        "):\n",
        "    # Split the data\n",
        "    X_fold_train = x_train_df.iloc[train_idx]\n",
        "    y_fold_train = y_train_labels_array[train_idx]\n",
        "    X_fold_test  = x_train_df.iloc[test_idx]\n",
        "    y_fold_test  = y_train_labels_array[test_idx]\n",
        "    \n",
        "    # Fit on this fold\n",
        "    enhanced_pipeline.fit(X_fold_train, y_fold_train)\n",
        "    \n",
        "    # Predict\n",
        "    preds = enhanced_pipeline.predict(X_fold_test)\n",
        "    \n",
        "    # Compute accuracy\n",
        "    acc = (preds == y_fold_test).mean()\n",
        "    scores.append(acc)\n",
        "\n",
        "print(\"\\nCross-validation scores:\", scores)\n",
        "print(\"Mean CV accuracy: {:.3f} (+/- {:.3f})\".format(\n",
        "    np.mean(scores), np.std(scores) * 2\n",
        "))\n",
        "\n",
        "# 5) Train final model on the entire training set\n",
        "enhanced_pipeline.fit(x_train_df, y_train_labels_array)\n",
        "\n",
        "# 6) Evaluate on Training Data\n",
        "y_train_pred = enhanced_pipeline.predict(x_train_df)\n",
        "print(\"\\nTraining Classification Report:\")\n",
        "print(classification_report(y_train_labels_array, y_train_pred))\n",
        "\n",
        "# 7) Evaluate on Validation Data\n",
        "#    => Implement \"if username in training\" logic here.\n",
        "\n",
        "# First, build a dictionary from training usernames to labels\n",
        "# We rely on the fact that x_train_df still has \"username\" column\n",
        "train_user2label = dict(zip(x_train_df[\"username\"], y_train_labels_array))\n",
        "\n",
        "# Now we define a function that uses the dictionary first:\n",
        "def predict_with_username_lookup(df, pipeline):\n",
        "    \"\"\"\n",
        "    If username is in training (train_user2label), \n",
        "    use that known label.\n",
        "    Otherwise, call pipeline.predict.\n",
        "    \"\"\"\n",
        "    # Identify known vs. unknown usernames\n",
        "    known_mask = df[\"username\"].isin(train_user2label)\n",
        "    \n",
        "    # For the unknown subset, run the pipeline\n",
        "    df_unknown = df[~known_mask].copy()\n",
        "    \n",
        "    # We must drop the columns that are not used by the pipeline \n",
        "    # (e.g., \"username\", \"captions_clean\", \"biography_clean\" are used internally \n",
        "    # by pipeline, so let's keep them. Actually the pipeline transforms \"captions_clean\" and \"biography_clean\".)\n",
        "    # The pipeline expects columns [follower_count, following_count, post_count, \n",
        "    #  captions_clean, biography_clean].\n",
        "    # So let's do NOT drop them. We only drop \"username\" if needed. The pipeline does remainder='drop' anyway.\n",
        "    \n",
        "    X_unknown = df_unknown.drop(columns=[\"username\"])  # pipeline doesn't need \"username\"\n",
        "    \n",
        "    # Predict\n",
        "    y_pred_unknown = pipeline.predict(X_unknown)\n",
        "    \n",
        "    # Rebuild final predictions in the original df order\n",
        "    y_pred_final = []\n",
        "    j = 0  # index for unknown predictions\n",
        "    \n",
        "    for idx in df.index:\n",
        "        if known_mask.loc[idx]:\n",
        "            # known => use label from training\n",
        "            user = df.loc[idx, \"username\"]\n",
        "            y_pred_final.append(train_user2label[user])\n",
        "        else:\n",
        "            # unknown => use pipeline result\n",
        "            y_pred_final.append(y_pred_unknown[j])\n",
        "            j += 1\n",
        "    \n",
        "    return np.array(y_pred_final)\n",
        "\n",
        "# Make predictions on the validation data, using the check\n",
        "# Ensure x_val_df has 'username' so we can do the lookup\n",
        "y_val_pred_custom = predict_with_username_lookup(x_val_df, enhanced_pipeline)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nValidation Classification Report (with username lookup):\")\n",
        "print(classification_report(y_val_labels, y_val_pred_custom))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading test data...\n",
            "Preparing features for 999 test users...\n",
            "Preprocessing test data...\n",
            "Making predictions on test data...\n",
            "Predictions saved to: c:\\Users\\itsmm\\OneDrive\\Desktop\\CS412\\CS412-InstagramInfluencersAnalysis\\data\\output\\prediction-classification-round1.json\n"
          ]
        }
      ],
      "source": [
        "def load_test_data():\n",
        "    \"\"\"Load and prepare test data\"\"\"\n",
        "    # Define file paths\n",
        "    current_notebook_dir = os.getcwd()\n",
        "    repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "    data_dir = os.path.join(repo_dir, 'data')\n",
        "    testing_dir = os.path.join(data_dir, 'testing')\n",
        "    test_data_path = os.path.join(testing_dir, 'test-classification-round1.dat')\n",
        "    \n",
        "    # Read test usernames\n",
        "    test_unames = []\n",
        "    with open(test_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "        for line in fh:\n",
        "            username = line.strip()\n",
        "            if username != \"screenname\":  # Skip screenname\n",
        "                test_unames.append(username)\n",
        "    \n",
        "    return test_unames\n",
        "\n",
        "def prepare_test_features(test_unames, username2posts_test, username2profile_test):\n",
        "    \"\"\"Prepare features for test data\"\"\"\n",
        "    test_data = []\n",
        "    for username in test_unames:\n",
        "        # Get posts and profile data\n",
        "        posts = username2posts_test.get(username, [])\n",
        "        profile = username2profile_test.get(username, {})\n",
        "        \n",
        "        # Prepare text data\n",
        "        captions = \" \".join([post.get('caption', '') for post in posts if post.get('caption')])\n",
        "        biography = profile.get('biography', '')\n",
        "        \n",
        "        # Prepare numeric data\n",
        "        row_data = {\n",
        "            'username': username,\n",
        "            'captions': captions,\n",
        "            'biography': biography,\n",
        "            'follower_count': profile.get('follower_count', 0),\n",
        "            'following_count': profile.get('following_count', 0),\n",
        "            'post_count': profile.get('post_count', 0)\n",
        "        }\n",
        "        test_data.append(row_data)\n",
        "    \n",
        "    return pd.DataFrame(test_data)\n",
        "\n",
        "def save_predictions(predictions, usernames, output_dir):\n",
        "    \"\"\"Save predictions to JSON file\"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Create output dictionary\n",
        "    output = dict(zip(usernames, predictions))\n",
        "    \n",
        "    # Save to file\n",
        "    output_path = os.path.join(output_dir, 'prediction-classification-round1.json')\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(output, f, indent=4)\n",
        "    \n",
        "    print(f\"Predictions saved to: {output_path}\")\n",
        "\n",
        "def main():\n",
        "    \n",
        "    # Load and prepare test data\n",
        "    print(\"\\nLoading test data...\")\n",
        "    test_unames = load_test_data()\n",
        "    \n",
        "    print(f\"Preparing features for {len(test_unames)} test users...\")\n",
        "    x_test_df = prepare_test_features(test_unames, username2posts_test, username2profile_test)\n",
        "    \n",
        "    # Preprocess test text data\n",
        "    print(\"Preprocessing test data...\")\n",
        "    x_test_df['captions_clean'] = x_test_df['captions'].apply(advanced_text_preprocessing)\n",
        "    x_test_df['biography_clean'] = x_test_df['biography'].apply(advanced_text_preprocessing)\n",
        "    \n",
        "    # Make predictions on test data\n",
        "    print(\"Making predictions on test data...\")\n",
        "    test_predictions = predict_with_username_lookup(x_test_df, enhanced_pipeline)\n",
        "    \n",
        "    # Save predictions\n",
        "    output_dir = os.path.join(os.path.dirname(os.getcwd()), 'data', 'output')\n",
        "    save_predictions(test_predictions.tolist(), test_unames, output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khu0eryhNZNN"
      },
      "source": [
        "# Naive Base Classifier\n",
        "\n",
        "### Now we can pass the numerical values to a classifier, Let's try Naive Base!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC7KXsQZL7Kp"
      },
      "source": [
        "# Like Count Prediction\n",
        "\n",
        "\n",
        "Here, we use the average like_count of the user's previous posts to predict each post's like_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "nolpagasSuBq"
      },
      "outputs": [],
      "source": [
        "def predict_like_count(username, current_post=None):\n",
        "  def get_avg_like_count(posts:list):\n",
        "    total = 0.\n",
        "    for post in posts:\n",
        "      if current_post is not None and post[\"id\"] == current_post[\"id\"]:\n",
        "        continue\n",
        "\n",
        "      like_count = post.get(\"like_count\", 0)\n",
        "      if like_count is None:\n",
        "        like_count = 0\n",
        "      total += like_count\n",
        "\n",
        "    if len(posts) == 0:\n",
        "      return 0.\n",
        "\n",
        "    return total / len(posts)\n",
        "\n",
        "  if username in username2posts_train:\n",
        "    return get_avg_like_count(username2posts_train[username])\n",
        "  elif username in username2posts_test:\n",
        "    return get_avg_like_count(username2posts_test[username])\n",
        "  else:\n",
        "    print(f\"No data available for {username}\")\n",
        "    return -1\n",
        "  \n",
        "def log_mse_like_counts(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculate the Log Mean Squared Error (Log MSE) for like counts (log(like_count + 1)).\n",
        "\n",
        "  Parameters:\n",
        "  - y_true: array-like, actual like counts\n",
        "  - y_pred: array-like, predicted like counts\n",
        "\n",
        "  Returns:\n",
        "  - log_mse: float, Log Mean Squared Error\n",
        "  \"\"\"\n",
        "  # Ensure inputs are numpy arrays\n",
        "  y_true = np.array(y_true)\n",
        "  y_pred = np.array(y_pred)\n",
        "\n",
        "  # Log transformation: log(like_count + 1)\n",
        "  log_y_true = np.log1p(y_true)\n",
        "  log_y_pred = np.log1p(y_pred)\n",
        "\n",
        "  # Compute squared errors\n",
        "  squared_errors = (log_y_true - log_y_pred) ** 2\n",
        "\n",
        "  # Return the mean of squared errors\n",
        "  return np.mean(squared_errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2741/2741 [00:00<00:00, 7230.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log MSE Train= 1.1906784633896135\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def enhanced_predict_like_count(username, current_post=None):\n",
        "    \"\"\"Enhanced like count prediction using average likes and time-based adjustments\"\"\"\n",
        "    \n",
        "    # Get base prediction from average method\n",
        "    base_prediction = predict_like_count(username, current_post)\n",
        "    \n",
        "    # If no current post, return base prediction\n",
        "    if not current_post:\n",
        "        return base_prediction\n",
        "    \n",
        "    # Safe get function for numeric values\n",
        "    def safe_get(dict_obj, key, default=0):\n",
        "        value = dict_obj.get(key, default)\n",
        "        return default if value is None else value\n",
        "    \n",
        "    # Extract timestamp if available\n",
        "    adjustment = 1.0\n",
        "    try:\n",
        "        timestamp = safe_get(current_post, 'timestamp', '00:00:00')\n",
        "        hour = int(timestamp.split()[1].split(':')[0])\n",
        "        \n",
        "        # Time of day adjustment\n",
        "        prime_time_hours = {19, 20, 21, 22}  # Evening hours\n",
        "        if hour in prime_time_hours:\n",
        "            adjustment *= 1.1\n",
        "        elif 9 <= hour <= 17:  # Work hours\n",
        "            adjustment *= 0.9\n",
        "        elif 0 <= hour <= 5:   # Late night\n",
        "            adjustment *= 0.8\n",
        "    except:\n",
        "        pass  # If timestamp parsing fails, keep original adjustment\n",
        "    \n",
        "    # Apply adjustment to base prediction\n",
        "    final_prediction = base_prediction * adjustment\n",
        "    \n",
        "    return int(final_prediction)\n",
        "\n",
        "# Modified evaluation code\n",
        "y_like_count_train_true = []\n",
        "y_like_count_train_pred = []\n",
        "\n",
        "for uname, posts in tqdm(username2posts_train.items(), desc=\"Evaluating predictions\"):\n",
        "    for post in posts:\n",
        "        pred_val = enhanced_predict_like_count(uname, post)\n",
        "        true_val = post.get(\"like_count\", 0)\n",
        "        if true_val is None:\n",
        "            true_val = 0\n",
        "            \n",
        "        y_like_count_train_true.append(true_val)\n",
        "        y_like_count_train_pred.append(pred_val)\n",
        "\n",
        "print(f\"Log MSE Train= {log_mse_like_counts(y_like_count_train_true, y_like_count_train_pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1jETdAuXA0H",
        "outputId": "871164d0-74fb-49cd-ea77-b8a3e0793e81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log MSE Train= 1.2271047744059362\n"
          ]
        }
      ],
      "source": [
        "# Train Dataset evaluation\n",
        "\n",
        "y_like_count_train_true = []\n",
        "y_like_count_train_pred = []\n",
        "for uname, posts in username2posts_train.items():\n",
        "  for post in posts:\n",
        "    pred_val = predict_like_count(uname, post)\n",
        "    true_val = post.get(\"like_count\", 0)\n",
        "    if true_val is None:\n",
        "      true_val = 0\n",
        "\n",
        "    y_like_count_train_true.append(true_val)\n",
        "    y_like_count_train_pred.append(pred_val)\n",
        "\n",
        "print(f\"Log MSE Train= {log_mse_like_counts(y_like_count_train_true, y_like_count_train_pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "F02V1wO-WBMV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed data saved to: c:\\Users\\itsmm\\OneDrive\\Desktop\\CS412\\CS412-InstagramInfluencersAnalysis\\data\\output\\prediction-regression-round1.json\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the testing directory\n",
        "testing_dir = os.path.join(data_dir, 'testing')\n",
        "\n",
        "# File path for 'test-regression-round1.jsonl'\n",
        "# Modified output processing\n",
        "# File path for 'test-regression-round1.jsonl'\n",
        "test_dataset_path = os.path.join(testing_dir, 'test-regression-round1.jsonl')\n",
        "\n",
        "# File path for output\n",
        "output_dir = os.path.join(data_dir, 'output')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_file_path = os.path.join(output_dir, 'prediction-regression-round1.json')\n",
        "\n",
        "# Process the Test Dataset\n",
        "output_list = []  # Keep the list structure\n",
        "\n",
        "with open(test_dataset_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "        \n",
        "        # Get prediction\n",
        "        pred_val = predict_like_count(sample[\"username\"])\n",
        "        \n",
        "        # Create simplified output dictionary\n",
        "        output_dict = {\n",
        "            'id': sample['id'],\n",
        "            'like_count': int(pred_val),\n",
        "            'username': sample['username'],\n",
        "            'media_type': sample.get('media_type', ''),\n",
        "            'comments_count': sample.get('comments_count', 0),\n",
        "            'timestamp': sample.get('timestamp', ''),\n",
        "            'media_url': sample.get('media_url', None)\n",
        "        }\n",
        "        output_list.append(output_dict)\n",
        "\n",
        "# Save just the id and like_count to the JSON file\n",
        "predictions_dict = {item['id']: item['like_count'] for item in output_list}\n",
        "\n",
        "# Save to file\n",
        "with open(output_file_path, \"wt\", encoding=\"utf-8\") as of:\n",
        "    json.dump(predictions_dict, of, indent=4)\n",
        "\n",
        "print(f\"\\nProcessed data saved to: {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blI2SqvvvOF8",
        "outputId": "fe1e72e3-3ad0-486d-d054-3e90e8c66669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First 3 predictions:\n",
            "[{'comments_count': 2,\n",
            "  'id': '18144550534306740',\n",
            "  'like_count': 158,\n",
            "  'media_type': 'CAROUSEL_ALBUM',\n",
            "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/397997154_1016992459537522_4925783512176260397_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=7V_eObkFeK4AX-LMtsK&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDEqDhzaTO3ezV-veT6cJFCOcAEyeVzHR6si9n33N6G5A&oe=6551B6B9',\n",
            "  'timestamp': '2023-11-02 15:49:22',\n",
            "  'username': 'kozayarismasi'},\n",
            " {'comments_count': 0,\n",
            "  'id': '17995331788956693',\n",
            "  'like_count': 99,\n",
            "  'media_type': 'VIDEO',\n",
            "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m82/BF4767CB85BDFB8ADCCCA8F15B8C20B5_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmNsaXBzLnVua25vd24tQzMuNzIwLmRhc2hfYmFzZWxpbmVfMV92MSJ9&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1259525061418244_1441854817&_nc_vs=HBksFQIYT2lnX3hwdl9yZWVsc19wZXJtYW5lbnRfcHJvZC9CRjQ3NjdDQjg1QkRGQjhBRENDQ0E4RjE1QjhDMjBCNV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dBRWdfaFZfcDVCYk5HZ0NBQTlzVURvZW5mZ3FicV9FQUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvS3uOiQ0P8%2FFQIoAkMzLBdAJO%2Bdsi0OVhgSZGFzaF9iYXNlbGluZV8xX3YxEQB1AAA%3D&ccb=9-4&oh=00_AfAm22JssMPaUlQe3rpYsFWBhFb5mUgolTCdhV0Xgm4AnA&oe=6556A482&_nc_sid=1d576d&_nc_rid=cd9a998e44',\n",
            "  'timestamp': '2023-08-19 13:46:02',\n",
            "  'username': 'celikbeymobilya'},\n",
            " {'comments_count': 75,\n",
            "  'id': '18302703232191518',\n",
            "  'like_count': 1224,\n",
            "  'media_type': 'VIDEO',\n",
            "  'media_url': None,\n",
            "  'timestamp': '2023-10-02 06:53:33',\n",
            "  'username': 'girisimci_muhendis'}]\n"
          ]
        }
      ],
      "source": [
        "# Print first 3 items\n",
        "print(\"\\nFirst 3 predictions:\")\n",
        "pprint(output_list[:3])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
