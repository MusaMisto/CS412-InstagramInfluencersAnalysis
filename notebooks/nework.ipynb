{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBOlFYpnlr0N"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PM5EnXhLk6BL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgAdz_b8xp8w",
        "outputId": "14cd6ea6-4a61-4b22-f2a6-59088a78e58a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\itsmm\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Turkish StopWords\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "turkish_stopwords = stopwords.words('turkish')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plt9-VkNmlmH"
      },
      "source": [
        "# Influencer Category Classification\n",
        "\n",
        "\n",
        "\n",
        "1.   Read Data\n",
        "2.   Preprocess Data\n",
        "3.   Prepare Model\n",
        "4.   Predict Test Data\n",
        "4.   Save outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8DeBh-b7lrEs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows of the training classification DataFrame:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>taskirancemal</td>\n",
              "      <td>mom and children</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tam_kararinda</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spart4nn</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sosyalyiyiciler</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sonaydizdarahad</td>\n",
              "      <td>mom and children</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           user_id          category\n",
              "0    taskirancemal  mom and children\n",
              "1    tam_kararinda              food\n",
              "2         spart4nn              food\n",
              "3  sosyalyiyiciler              food\n",
              "4  sonaydizdarahad  mom and children"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the training directory\n",
        "training_dir = os.path.join(data_dir, 'training')\n",
        "\n",
        "# File path for 'train-classification.csv'\n",
        "train_classification_path = os.path.join(training_dir, 'train-classification.csv')\n",
        "\n",
        "# Step 2: Load Data Dynamically\n",
        "train_classification_df = pd.read_csv(train_classification_path)\n",
        "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
        "\n",
        "# Step 3: Unify Labels\n",
        "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
        "\n",
        "# Step 4: Create User-to-Category Mapping\n",
        "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\n",
        "\n",
        "# Step 5: Verify Output\n",
        "print(\"First few rows of the training classification DataFrame:\")\n",
        "train_classification_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "SfD7BQ3hE5Jh",
        "outputId": "a67f08dd-ab3e-491f-bfe2-a14da0e961cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>art</th>\n",
              "      <td>191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>entertainment</th>\n",
              "      <td>323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fashion</th>\n",
              "      <td>299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gaming</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>health and lifestyle</th>\n",
              "      <td>503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mom and children</th>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sports</th>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tech</th>\n",
              "      <td>346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>travel</th>\n",
              "      <td>294</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      user_id\n",
              "category                     \n",
              "art                       191\n",
              "entertainment             323\n",
              "fashion                   299\n",
              "food                      511\n",
              "gaming                     13\n",
              "health and lifestyle      503\n",
              "mom and children          149\n",
              "sports                    113\n",
              "tech                      346\n",
              "travel                    294"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# stats about the labels\n",
        "train_classification_df.groupby(\"category\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mhckiEfD2gg_",
        "outputId": "af451a4d-8825-4506-fb70-41a6ef845aaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mom and children'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "username2_category[\"sonaydizdarahad\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GT_IcUM2nGBH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Training Users: 2741\n",
            "Number of Testing Users: 2674\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the training directory\n",
        "training_dir = os.path.join(data_dir, 'training')\n",
        "\n",
        "# File path for 'training-dataset.jsonl.gz'\n",
        "train_data_path = os.path.join(training_dir, 'training-dataset.jsonl.gz')\n",
        "\n",
        "# Step 2: Initialize Dictionaries for Data\n",
        "username2posts_train = dict()\n",
        "username2profile_train = dict()\n",
        "\n",
        "username2posts_test = dict()\n",
        "username2profile_test = dict()\n",
        "\n",
        "# Step 3: Process Data from 'training-dataset.jsonl.gz'\n",
        "with gzip.open(train_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "\n",
        "        profile = sample[\"profile\"]\n",
        "        username = profile.get(\"username\", \"\").strip()  # Handle missing or empty usernames\n",
        "        if not username:\n",
        "            continue  # Skip if username is missing or empty\n",
        "\n",
        "        if username in username2_category:\n",
        "            # Train data info\n",
        "            username2posts_train[username] = sample[\"posts\"]\n",
        "            username2profile_train[username] = profile\n",
        "        else:\n",
        "            # Test data info\n",
        "            username2posts_test[username] = sample[\"posts\"]\n",
        "            username2profile_test[username] = profile\n",
        "\n",
        "# Step 4: Verify Output\n",
        "print(f\"Number of Training Users: {len(username2posts_train)}\")\n",
        "print(f\"Number of Testing Users: {len(username2posts_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "djeF1GQ1oy3v",
        "outputId": "5c2ead24-d7d1-4df8-bec0-bf2152c76832"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>id</th>\n",
              "      <th>full_name</th>\n",
              "      <th>biography</th>\n",
              "      <th>category_name</th>\n",
              "      <th>post_count</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>is_business_account</th>\n",
              "      <th>is_private</th>\n",
              "      <th>...</th>\n",
              "      <th>business_category_name</th>\n",
              "      <th>overall_category_name</th>\n",
              "      <th>category_enum</th>\n",
              "      <th>is_verified_by_mv4b</th>\n",
              "      <th>is_regulated_c18</th>\n",
              "      <th>profile_pic_url</th>\n",
              "      <th>should_show_category</th>\n",
              "      <th>should_show_public_contacts</th>\n",
              "      <th>show_account_transparency_details</th>\n",
              "      <th>profile_picture_base64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>deparmedya</td>\n",
              "      <td>3170700063</td>\n",
              "      <td>Depar Medya</td>\n",
              "      <td>#mediaplanning #mediabuying #sosyalmedya</td>\n",
              "      <td>Local business</td>\n",
              "      <td>None</td>\n",
              "      <td>1167</td>\n",
              "      <td>192</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>LOCAL</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     username          id    full_name  \\\n",
              "0  deparmedya  3170700063  Depar Medya   \n",
              "\n",
              "                                  biography   category_name post_count  \\\n",
              "0  #mediaplanning #mediabuying #sosyalmedya  Local business       None   \n",
              "\n",
              "  follower_count following_count is_business_account is_private  ...  \\\n",
              "0           1167             192                True      False  ...   \n",
              "\n",
              "  business_category_name overall_category_name category_enum  \\\n",
              "0                   None                  None         LOCAL   \n",
              "\n",
              "  is_verified_by_mv4b is_regulated_c18  \\\n",
              "0               False            False   \n",
              "\n",
              "                                     profile_pic_url should_show_category  \\\n",
              "0  https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....                 True   \n",
              "\n",
              "  should_show_public_contacts show_account_transparency_details  \\\n",
              "0                        True                              True   \n",
              "\n",
              "                              profile_picture_base64  \n",
              "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
              "\n",
              "[1 rows x 44 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Profile Dataframe\n",
        "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
        "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)\n",
        "\n",
        "train_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "15tvVohUBHK9",
        "outputId": "15be2533-51b2-41e8-e0f2-26a44933dbae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>id</th>\n",
              "      <th>full_name</th>\n",
              "      <th>biography</th>\n",
              "      <th>category_name</th>\n",
              "      <th>post_count</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>is_business_account</th>\n",
              "      <th>is_private</th>\n",
              "      <th>...</th>\n",
              "      <th>business_category_name</th>\n",
              "      <th>overall_category_name</th>\n",
              "      <th>category_enum</th>\n",
              "      <th>is_verified_by_mv4b</th>\n",
              "      <th>is_regulated_c18</th>\n",
              "      <th>profile_pic_url</th>\n",
              "      <th>should_show_category</th>\n",
              "      <th>should_show_public_contacts</th>\n",
              "      <th>show_account_transparency_details</th>\n",
              "      <th>profile_picture_base64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>beyazyakaliyiz</td>\n",
              "      <td>8634457436</td>\n",
              "      <td>Selam Beyaz Yakalı</td>\n",
              "      <td>Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀</td>\n",
              "      <td>Personal blog</td>\n",
              "      <td>None</td>\n",
              "      <td>1265</td>\n",
              "      <td>665</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>PERSONAL_BLOG</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>https://instagram.fist6-1.fna.fbcdn.net/v/t51....</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         username          id           full_name  \\\n",
              "0  beyazyakaliyiz  8634457436  Selam Beyaz Yakalı   \n",
              "\n",
              "                                     biography  category_name post_count  \\\n",
              "0  Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀  Personal blog       None   \n",
              "\n",
              "  follower_count following_count is_business_account is_private  ...  \\\n",
              "0           1265             665                True      False  ...   \n",
              "\n",
              "  business_category_name overall_category_name  category_enum  \\\n",
              "0                   None                  None  PERSONAL_BLOG   \n",
              "\n",
              "  is_verified_by_mv4b is_regulated_c18  \\\n",
              "0               False            False   \n",
              "\n",
              "                                     profile_pic_url should_show_category  \\\n",
              "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
              "\n",
              "  should_show_public_contacts show_account_transparency_details  \\\n",
              "0                        True                              True   \n",
              "\n",
              "                              profile_picture_base64  \n",
              "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
              "\n",
              "[1 rows x 44 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_classification_df:\n",
            "['user_id', 'category']\n",
            "\n",
            "First few rows of the training classification DataFrame:\n",
            "           user_id          category\n",
            "0    taskirancemal  mom and children\n",
            "1    tam_kararinda              food\n",
            "2         spart4nn              food\n",
            "3  sosyalyiyiciler              food\n",
            "4  sonaydizdarahad  mom and children\n",
            "\n",
            "Label distribution:\n",
            "category\n",
            "food                    511\n",
            "health and lifestyle    503\n",
            "tech                    346\n",
            "entertainment           323\n",
            "fashion                 299\n",
            "travel                  294\n",
            "art                     191\n",
            "mom and children        149\n",
            "sports                  113\n",
            "gaming                   13\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Verify Output\n",
        "print(\"Columns in train_classification_df:\")\n",
        "print(train_classification_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of the training classification DataFrame:\")\n",
        "print(train_classification_df.head())\n",
        "\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(train_classification_df['category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_profile_df:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
            "\n",
            "First few rows of train_profile_df:\n",
            "     username          id    full_name  \\\n",
            "0  deparmedya  3170700063  Depar Medya   \n",
            "\n",
            "                                  biography   category_name post_count  \\\n",
            "0  #mediaplanning #mediabuying #sosyalmedya  Local business       None   \n",
            "\n",
            "  follower_count following_count is_business_account is_private  ...  \\\n",
            "0           1167             192                True      False  ...   \n",
            "\n",
            "  business_category_name overall_category_name category_enum  \\\n",
            "0                   None                  None         LOCAL   \n",
            "\n",
            "  is_verified_by_mv4b is_regulated_c18  \\\n",
            "0               False            False   \n",
            "\n",
            "                                     profile_pic_url should_show_category  \\\n",
            "0  https://instagram.fsaw2-3.fna.fbcdn.net/v/t51....                 True   \n",
            "\n",
            "  should_show_public_contacts show_account_transparency_details  \\\n",
            "0                        True                              True   \n",
            "\n",
            "                              profile_picture_base64  \n",
            "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
            "\n",
            "[1 rows x 44 columns]\n",
            "\n",
            "Columns in test_profile_df:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type', 'fb_profile_biolink', 'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid', 'has_clips', 'hide_like_and_view_counts', 'is_professional_account', 'is_supervision_enabled', 'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled', 'is_joined_recently', 'business_address_json', 'business_contact_method', 'business_email', 'business_phone_number', 'business_category_name', 'overall_category_name', 'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url', 'should_show_category', 'should_show_public_contacts', 'show_account_transparency_details', 'profile_picture_base64']\n",
            "\n",
            "First few rows of test_profile_df:\n",
            "         username          id           full_name  \\\n",
            "0  beyazyakaliyiz  8634457436  Selam Beyaz Yakalı   \n",
            "\n",
            "                                     biography  category_name post_count  \\\n",
            "0  Beyaz yakalıların dünyasına hoşgeldiniz 😀😀😀  Personal blog       None   \n",
            "\n",
            "  follower_count following_count is_business_account is_private  ...  \\\n",
            "0           1265             665                True      False  ...   \n",
            "\n",
            "  business_category_name overall_category_name  category_enum  \\\n",
            "0                   None                  None  PERSONAL_BLOG   \n",
            "\n",
            "  is_verified_by_mv4b is_regulated_c18  \\\n",
            "0               False            False   \n",
            "\n",
            "                                     profile_pic_url should_show_category  \\\n",
            "0  https://instagram.fist6-1.fna.fbcdn.net/v/t51....                 True   \n",
            "\n",
            "  should_show_public_contacts show_account_transparency_details  \\\n",
            "0                        True                              True   \n",
            "\n",
            "                              profile_picture_base64  \n",
            "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
            "\n",
            "[1 rows x 44 columns]\n"
          ]
        }
      ],
      "source": [
        "# Profile Dataframe\n",
        "print(\"Columns in train_profile_df:\")\n",
        "print(train_profile_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of train_profile_df:\")\n",
        "print(train_profile_df.head(1))\n",
        "\n",
        "print(\"\\nColumns in test_profile_df:\")\n",
        "print(test_profile_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of test_profile_df:\")\n",
        "print(test_profile_df.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_profile_df after dropping:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'bio_links', 'is_professional_account', 'business_category_name', 'overall_category_name']\n",
            "\n",
            "Columns in test_profile_df after dropping:\n",
            "['username', 'id', 'full_name', 'biography', 'category_name', 'post_count', 'follower_count', 'following_count', 'is_business_account', 'is_private', 'is_verified', 'bio_links', 'is_professional_account', 'business_category_name', 'overall_category_name']\n"
          ]
        }
      ],
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = [\n",
        "    'highlight_reel_count', 'entities', 'ai_agent_type', 'fb_profile_biolink',\n",
        "    'restricted_by_viewer', 'country_block', 'eimu_id', 'external_url', 'fbid',\n",
        "    'has_clips', 'hide_like_and_view_counts', 'is_supervision_enabled',\n",
        "    'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user',\n",
        "    'is_embeds_disabled', 'is_joined_recently', 'business_address_json',\n",
        "    'business_contact_method', 'business_email', 'business_phone_number',\n",
        "    'category_enum', 'is_verified_by_mv4b', 'is_regulated_c18',\n",
        "    'profile_pic_url', 'should_show_category', 'should_show_public_contacts',\n",
        "    'show_account_transparency_details', 'profile_picture_base64'\n",
        "]\n",
        "\n",
        "# Dropping specified columns from train_profile_df\n",
        "train_profile_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Dropping specified columns from test_profile_df\n",
        "test_profile_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Verify columns in train_profile_df after dropping\n",
        "print(\"Columns in train_profile_df after dropping:\")\n",
        "print(train_profile_df.columns.tolist())\n",
        "\n",
        "# Verify columns in test_profile_df after dropping\n",
        "print(\"\\nColumns in test_profile_df after dropping:\")\n",
        "print(test_profile_df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5UY0eYLsoTr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "Best Params: {'clf__C': 1, 'preprocessor__bio_tfidf__ngram_range': (1, 1), 'preprocessor__captions_tfidf__ngram_range': (1, 2)}\n",
            "Best CV Accuracy: 0.6455259262035492\n",
            "Training Accuracy: 0.9516423357664233\n",
            "\n",
            "Training Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                 art       0.94      0.95      0.94       153\n",
            "       entertainment       0.97      0.91      0.94       258\n",
            "             fashion       0.92      0.97      0.95       239\n",
            "                food       0.98      0.97      0.97       409\n",
            "              gaming       1.00      1.00      1.00        10\n",
            "health and lifestyle       0.97      0.91      0.94       402\n",
            "    mom and children       0.92      0.97      0.94       119\n",
            "              sports       0.98      0.99      0.98        90\n",
            "                tech       0.92      0.97      0.95       277\n",
            "              travel       0.94      0.96      0.95       235\n",
            "\n",
            "            accuracy                           0.95      2192\n",
            "           macro avg       0.95      0.96      0.96      2192\n",
            "        weighted avg       0.95      0.95      0.95      2192\n",
            "\n",
            "Validation Accuracy: 0.644808743169399\n",
            "\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                 art       0.29      0.18      0.23        38\n",
            "       entertainment       0.44      0.40      0.42        65\n",
            "             fashion       0.49      0.55      0.52        60\n",
            "                food       0.89      0.87      0.88       102\n",
            "              gaming       0.50      0.33      0.40         3\n",
            "health and lifestyle       0.75      0.71      0.73       100\n",
            "    mom and children       0.48      0.50      0.49        30\n",
            "              sports       0.65      0.65      0.65        23\n",
            "                tech       0.67      0.83      0.74        69\n",
            "              travel       0.65      0.68      0.66        59\n",
            "\n",
            "            accuracy                           0.64       549\n",
            "           macro avg       0.58      0.57      0.57       549\n",
            "        weighted avg       0.64      0.64      0.64       549\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Preprocessing Function\n",
        "import re\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    text = text.casefold()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Build Corpus and Labels\n",
        "corpus = []\n",
        "train_usernames = []\n",
        "\n",
        "for username, posts in username2posts_train.items():\n",
        "    train_usernames.append(username)\n",
        "    cleaned_captions = []\n",
        "    for post in posts:\n",
        "        post_caption = post.get(\"caption\", \"\")\n",
        "        if post_caption is None:\n",
        "            continue\n",
        "        post_caption = preprocess_text(post_caption)\n",
        "        if post_caption != \"\":\n",
        "            cleaned_captions.append(post_caption)\n",
        "    user_post_captions = \"\\n\".join(cleaned_captions)\n",
        "    corpus.append(user_post_captions)\n",
        "\n",
        "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
        "\n",
        "# Incorporate Metadata\n",
        "records = []\n",
        "for idx, username in enumerate(train_usernames):\n",
        "    profile = username2profile_train.get(username, {})\n",
        "    biography_text = str(profile.get(\"biography\", \"\") or \"\")\n",
        "    follower_count = profile.get(\"follower_count\", 0)\n",
        "    following_count = profile.get(\"following_count\", 0)\n",
        "    post_count = profile.get(\"post_count\", 0) if profile.get(\"post_count\") else 0\n",
        "    row_dict = {\n",
        "        \"username\": username,\n",
        "        \"captions\": corpus[idx],\n",
        "        \"biography\": biography_text,\n",
        "        \"follower_count\": follower_count,\n",
        "        \"following_count\": following_count,\n",
        "        \"post_count\": post_count,\n",
        "        \"label\": y_train[idx]\n",
        "    }\n",
        "    records.append(row_dict)\n",
        "\n",
        "train_full_df = pd.DataFrame(records)\n",
        "\n",
        "# Define Pipeline Components\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Train-Validation Split\n",
        "X = train_full_df.drop(columns=[\"label\"])\n",
        "y = train_full_df[\"label\"]\n",
        "\n",
        "x_train_df, x_val_df, y_train_labels, y_val_labels = train_test_split(\n",
        "    X, \n",
        "    y, \n",
        "    test_size=0.2, \n",
        "    stratify=y, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define ColumnTransformer\n",
        "numeric_features = [\"follower_count\", \"following_count\", \"post_count\"]\n",
        "text_features_caps = \"captions\"\n",
        "text_features_bio = \"biography\"\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"captions_tfidf\", TfidfVectorizer(\n",
        "             stop_words=turkish_stopwords, \n",
        "             max_features=5000\n",
        "         ), text_features_caps),\n",
        "        (\"bio_tfidf\", TfidfVectorizer(\n",
        "             stop_words=turkish_stopwords, \n",
        "             max_features=5000\n",
        "         ), text_features_bio),\n",
        "        (\"numeric_scaler\", MinMaxScaler(), numeric_features)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# Build ImbPipeline (SMOTE + Classifier)\n",
        "pipeline = ImbPipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=42, sampling_strategy=\"auto\")),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        class_weight='balanced',\n",
        "        solver='liblinear',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Define a param grid\n",
        "param_grid = {\n",
        "    \"preprocessor__captions_tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"preprocessor__bio_tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"clf__C\": [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Initialize and fit GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(x_train_df, y_train_labels)\n",
        "\n",
        "print(\"Best Params:\", grid_search.best_params_)\n",
        "print(\"Best CV Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "\n",
        "# **New Section: Evaluate on Training Data**\n",
        "# Predict on the training data\n",
        "y_train_pred = best_pipeline.predict(x_train_df)\n",
        "\n",
        "# Calculate training accuracy\n",
        "train_acc = accuracy_score(y_train_labels, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "\n",
        "# Generate and print the training classification report\n",
        "print(\"\\nTraining Classification Report:\\n\",\n",
        "      classification_report(y_train_labels, y_train_pred, zero_division=0))\n",
        "\n",
        "# **End of New Section**\n",
        "\n",
        "# Predict on the validation data\n",
        "y_val_pred = best_pipeline.predict(x_val_df)\n",
        "\n",
        "# Calculate validation accuracy\n",
        "val_acc = accuracy_score(y_val_labels, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_acc)\n",
        "\n",
        "# Generate and print the validation classification report\n",
        "print(\"\\nClassification Report:\\n\",\n",
        "      classification_report(y_val_labels, y_val_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2192 entries, 2638 to 2667\n",
            "Data columns (total 7 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   captions         2192 non-null   object \n",
            " 1   biography        2192 non-null   object \n",
            " 2   follower_count   2192 non-null   int64  \n",
            " 3   following_count  2192 non-null   int64  \n",
            " 4   post_count       2192 non-null   float64\n",
            " 5   captions_clean   2192 non-null   object \n",
            " 6   biography_clean  2192 non-null   object \n",
            "dtypes: float64(1), int64(2), object(4)\n",
            "memory usage: 137.0+ KB\n",
            "None\n",
            "\n",
            "Missing values in training data:\n",
            "captions           0\n",
            "biography          0\n",
            "follower_count     0\n",
            "following_count    0\n",
            "post_count         0\n",
            "captions_clean     0\n",
            "biography_clean    0\n",
            "dtype: int64\n",
            "\n",
            "Performing cross-validation with tqdm progress bar...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cross-Validation: 100%|██████████| 5/5 [31:01<00:00, 372.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cross-validation scores: [0.6560364464692483, 0.662870159453303, 0.6552511415525114, 0.6643835616438356, 0.6484018264840182]\n",
            "Mean CV accuracy: 0.657 (+/- 0.012)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 222\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean CV accuracy: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m (+/- \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    218\u001b[0m     np\u001b[38;5;241m.\u001b[39mmean(scores), np\u001b[38;5;241m.\u001b[39mstd(scores) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    219\u001b[0m ))\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# 5) Train final model on the entire training set\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m \u001b[43menhanced_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_labels_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# 6) Evaluate on Training Data\u001b[39;00m\n\u001b[0;32m    225\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m enhanced_pipeline\u001b[38;5;241m.\u001b[39mpredict(x_train_df)\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\pipeline.py:526\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    521\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    522\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    523\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    524\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    525\u001b[0m         )\n\u001b[1;32m--> 526\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:349\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    347\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:81\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m     )\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVoting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:36\u001b[0m, in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m---> 36\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:532\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:610\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    603\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    604\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    605\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    606\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    607\u001b[0m         )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 610\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 245\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    250\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    258\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\itsmm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ------------------- Imports -------------------\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# scikit-learn utilities\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# For progress bar in cross-validation\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------- Enhanced Text Preprocessing -------------------\n",
        "def advanced_text_preprocessing(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    text = text.lower()\n",
        "    \n",
        "    # Extract hashtags and mentions\n",
        "    hashtags = re.findall(r'#\\w+', text)\n",
        "    mentions = re.findall(r'@\\w+', text)\n",
        "    \n",
        "    # Densities\n",
        "    text_length = len(text)\n",
        "    hashtag_density = len(hashtags) / (text_length + 1)\n",
        "    mention_density = len(mentions) / (text_length + 1)\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Add density info as extra pseudo-tokens\n",
        "    density_info = f\" hashtag_density_{hashtag_density:.2f} mention_density_{mention_density:.2f}\"\n",
        "    \n",
        "    # Re-append the extracted hashtags and mentions\n",
        "    return f\"{text} {' '.join(hashtags)} {' '.join(mentions)} {density_info}\".strip()\n",
        "\n",
        "# ------------------- Enhanced Numeric Feature Extraction -------------------\n",
        "def extract_numeric_features(X):\n",
        "    # If X is a DataFrame, ensure columns exist\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        cols = ['follower_count', 'following_count', 'post_count']\n",
        "        if 'account_age_days' in X.columns:\n",
        "            cols.append('account_age_days')\n",
        "        \n",
        "        df = X[cols].copy()\n",
        "    else:\n",
        "        # If it's a NumPy array\n",
        "        df = pd.DataFrame(\n",
        "            X, \n",
        "            columns=['follower_count', 'following_count', 'post_count', 'account_age_days'][:X.shape[1]]\n",
        "        )\n",
        "    \n",
        "    df = df.fillna(0)\n",
        "    \n",
        "    df['following_count_safe'] = df['following_count'].replace(0, 1)\n",
        "    df['follower_count_safe']  = df['follower_count'].replace(0, 1)\n",
        "    \n",
        "    df['follower_ratio']   = df['follower_count'] / df['following_count_safe']\n",
        "    df['post_density']     = df['post_count'] / df['follower_count_safe']\n",
        "    df['engagement_score'] = np.log1p(df['follower_count']) * np.log1p(df['post_count'])\n",
        "    \n",
        "    if 'account_age_days' in df.columns:\n",
        "        df['activity_ratio'] = df['post_count'] / (df['account_age_days'] + 1)\n",
        "    else:\n",
        "        df['activity_ratio'] = df['post_count']\n",
        "    \n",
        "    df['follower_growth_rate'] = df['follower_count'] / (df['post_count'] + 1)\n",
        "    df['relative_engagement']  = (df['follower_count'] * df['post_count']) / (df['following_count_safe'])\n",
        "    \n",
        "    # Log transforms\n",
        "    df['log_followers']  = np.log1p(df['follower_count'])\n",
        "    df['log_following']  = np.log1p(df['following_count'])\n",
        "    df['log_posts']      = np.log1p(df['post_count'])\n",
        "    \n",
        "    df.drop(['following_count_safe','follower_count_safe'], axis=1, inplace=True)\n",
        "    \n",
        "    return df.values\n",
        "\n",
        "# ------------------- Build Pipeline -------------------\n",
        "def build_enhanced_pipeline(stopwords_list):\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('captions_tfidf', \n",
        "             TfidfVectorizer(\n",
        "                 stop_words=stopwords_list,\n",
        "                 max_features=3000,   \n",
        "                 ngram_range=(1, 1),  \n",
        "                 min_df=2,\n",
        "                 max_df=0.95\n",
        "             ), \n",
        "             'captions_clean'),\n",
        "            \n",
        "            ('bio_tfidf', \n",
        "             TfidfVectorizer(\n",
        "                 stop_words=stopwords_list,\n",
        "                 max_features=2000,   \n",
        "                 ngram_range=(1, 1),  \n",
        "                 min_df=2,\n",
        "                 max_df=0.95\n",
        "             ), \n",
        "             'biography_clean'),\n",
        "            \n",
        "            ('numeric', \n",
        "             Pipeline([\n",
        "                 ('feat_eng', FunctionTransformer(extract_numeric_features)),\n",
        "                 ('scaler', MinMaxScaler())\n",
        "             ]), \n",
        "             ['follower_count', 'following_count', 'post_count']\n",
        "             # add 'account_age_days' if available\n",
        "            )\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    \n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=100,    \n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        max_features='sqrt',\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=50,     \n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    lr = LogisticRegression(\n",
        "        C=2.0,\n",
        "        class_weight='balanced',\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', rf),\n",
        "            ('gb', gb),\n",
        "            ('lr', lr)\n",
        "        ],\n",
        "        voting='soft'\n",
        "    )\n",
        "    \n",
        "    pipeline = ImbPipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('classifier', ensemble)\n",
        "    ])\n",
        "    \n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# ------------------- Example Usage -------------------\n",
        "print(\"Training data info:\")\n",
        "print(x_train_df.info())\n",
        "print(\"\\nMissing values in training data:\")\n",
        "print(x_train_df.isnull().sum())\n",
        "\n",
        "# 1) Preprocess text once\n",
        "x_train_df['captions_clean'] = x_train_df['captions'].apply(advanced_text_preprocessing)\n",
        "x_train_df['biography_clean'] = x_train_df['biography'].apply(advanced_text_preprocessing)\n",
        "\n",
        "x_val_df['captions_clean'] = x_val_df['captions'].apply(advanced_text_preprocessing)\n",
        "x_val_df['biography_clean'] = x_val_df['biography'].apply(advanced_text_preprocessing)\n",
        "\n",
        "# 2) Convert y_train_labels to NumPy for direct integer indexing\n",
        "y_train_labels_array = y_train_labels.values\n",
        "\n",
        "# 3) Build pipeline\n",
        "enhanced_pipeline = build_enhanced_pipeline(stopwords_list=turkish_stopwords)\n",
        "\n",
        "# 4) Manual Cross-Validation with tqdm progress bar\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "\n",
        "print(\"\\nPerforming cross-validation with tqdm progress bar...\")\n",
        "for fold_idx, (train_idx, test_idx) in enumerate(\n",
        "    tqdm(cv.split(x_train_df, y_train_labels_array), \n",
        "         total=cv.get_n_splits(), \n",
        "         desc='Cross-Validation')\n",
        "):\n",
        "    # Split the data\n",
        "    X_fold_train = x_train_df.iloc[train_idx]\n",
        "    y_fold_train = y_train_labels_array[train_idx]   # now works because it's a NumPy array\n",
        "    X_fold_test  = x_train_df.iloc[test_idx]\n",
        "    y_fold_test  = y_train_labels_array[test_idx]\n",
        "    \n",
        "    # Fit on this fold\n",
        "    enhanced_pipeline.fit(X_fold_train, y_fold_train)\n",
        "    # Predict\n",
        "    preds = enhanced_pipeline.predict(X_fold_test)\n",
        "    # Compute accuracy\n",
        "    acc = (preds == y_fold_test).mean()\n",
        "    scores.append(acc)\n",
        "\n",
        "print(\"\\nCross-validation scores:\", scores)\n",
        "print(\"Mean CV accuracy: {:.3f} (+/- {:.3f})\".format(\n",
        "    np.mean(scores), np.std(scores) * 2\n",
        "))\n",
        "\n",
        "# 5) Train final model on the entire training set\n",
        "enhanced_pipeline.fit(x_train_df, y_train_labels_array)\n",
        "\n",
        "# 6) Evaluate on Training Data\n",
        "y_train_pred = enhanced_pipeline.predict(x_train_df)\n",
        "print(\"\\nTraining Classification Report:\")\n",
        "print(classification_report(y_train_labels_array, y_train_pred))\n",
        "\n",
        "# 7) Evaluate on Validation Data\n",
        "y_val_pred = enhanced_pipeline.predict(x_val_df)\n",
        "print(\"\\nValidation Classification Report:\")\n",
        "print(classification_report(y_val_labels, y_val_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in train_full_df:\n",
            "['username', 'captions', 'biography', 'follower_count', 'following_count', 'post_count', 'label']\n",
            "\n",
            "First few rows of train_full_df:\n",
            "               username                                           captions  \\\n",
            "0            deparmedya  cumhuriyetimizin 100yılı kutlu olsun\\noriflame...   \n",
            "1            kafesfirin  bugün bir fincan köpüklü türk kahvesiyle taçla...   \n",
            "2              vimerang  saygı ve özlemle #atatürk #10kasım #10kasim193...   \n",
            "3     mustafa_yalcinn38  altınoluk çevre şehircilik ve iklim değişikliğ...   \n",
            "4  zorluenergysolutions  güne enerjik bir sohbet ile devam etmek ister ...   \n",
            "\n",
            "                                           biography  follower_count  \\\n",
            "0           #mediaplanning #mediabuying #sosyalmedya            1167   \n",
            "1  📍Söğütözü📍FTZ AVM\\n🛒Ankara macro▲center v...           11997   \n",
            "2       Dijital İletişim Yönetimi🎬info@vimerang.comq            2321   \n",
            "3                            Talas Belediye Başkanı           13647   \n",
            "4  Türkiye’nin 81 ilindeki en yaygın elektrikli ş...            7917   \n",
            "\n",
            "   following_count  post_count                 label  \n",
            "0              192         0.0                  tech  \n",
            "1               17         0.0                  food  \n",
            "2              454         0.0                  tech  \n",
            "3               29         0.0  health and lifestyle  \n",
            "4               11         0.0                  tech  \n"
          ]
        }
      ],
      "source": [
        "# After merging captions and profile data\n",
        "print(\"Columns in train_full_df:\")\n",
        "print(train_full_df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst few rows of train_full_df:\")\n",
        "print(train_full_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features (X) columns:\n",
            "['captions', 'biography', 'follower_count', 'following_count', 'post_count']\n",
            "\n",
            "Labels (y) distribution:\n",
            "label\n",
            "food                    511\n",
            "health and lifestyle    502\n",
            "tech                    346\n",
            "entertainment           323\n",
            "fashion                 299\n",
            "travel                  294\n",
            "art                     191\n",
            "mom and children        149\n",
            "sports                  113\n",
            "gaming                   13\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Features and Labels\n",
        "print(\"Features (X) columns:\")\n",
        "print(X.columns.tolist())\n",
        "\n",
        "print(\"\\nLabels (y) distribution:\")\n",
        "print(y.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set columns:\n",
            "['captions', 'biography', 'follower_count', 'following_count', 'post_count']\n",
            "\n",
            "Validation set columns:\n",
            "['captions', 'biography', 'follower_count', 'following_count', 'post_count']\n",
            "\n",
            "Training set preview:\n",
            "                                               captions  \\\n",
            "2638  bugün sofralarımızın vazgeçilmezi #düşükprotei...   \n",
            "206   dün gece #kanald ekranlarında bizi 90lı yıllar...   \n",
            "2073  ulu önderimiz gazi mustafa kemal atatürkün bed...   \n",
            "354   formula 1 in teknoloji alt yapısı için lenovoy...   \n",
            "1736  yaşasin cumhuriyet\\n10102005\\nsağlıklı ve mutl...   \n",
            "\n",
            "                                              biography  follower_count  \\\n",
            "2638  Sosyal medya hesaplarımızın kullanım kuralları...            2562   \n",
            "206   Her cumartesi saat 20.00’de @kanald ekranların...           66804   \n",
            "2073  Cerrahpaşa Tıp Fakültesi, Nükleer Tıp Anabi...            3297   \n",
            "354   Herkes için daha akıllı teknoloji, Dünyanın v...           59517   \n",
            "1736                                                              24217   \n",
            "\n",
            "      following_count  post_count  \n",
            "2638                3         0.0  \n",
            "206                14         0.0  \n",
            "2073                0         0.0  \n",
            "354               134         0.0  \n",
            "1736             3634         0.0  \n",
            "\n",
            "Validation set preview:\n",
            "                                               captions  \\\n",
            "865   her ortamda her yerde iki kişi bir araya geldi...   \n",
            "2728  bademli profiterol görünce biz sana ısmarlamas...   \n",
            "1316  dünle beraber gitme dün yeni bir günle beraber...   \n",
            "2429  daima senin ışığında senin izinde\\ncumhuriyeti...   \n",
            "2448  avşanın meşhur adakarası üzümü coğrafi işaret ...   \n",
            "\n",
            "                                              biography  follower_count  \\\n",
            "865   𝘽𝙚𝙮𝙠𝙤𝙯 𝘽𝙚𝙡𝙚𝙙𝙞𝙮𝙚𝙨𝙞 𝙆𝙪̈𝙡𝙩𝙪̈𝙧 𝙫𝙚 𝙎𝙤𝙨𝙮𝙖𝙡 𝙄̇𝙨̧𝙡𝙚𝙧 𝙈...           34718   \n",
            "2728                                     İyi zamanlar 🎉          147860   \n",
            "1316  Photographer\\n #photohadrian \\n📸  Landscape ph...            8000   \n",
            "2429                                   #focusonsolution             121   \n",
            "2448  Otelimiz merkezde, denize sıfır 🏖\\nSabah akşam...            1027   \n",
            "\n",
            "      following_count  post_count  \n",
            "865                41         0.0  \n",
            "2728               19         0.0  \n",
            "1316             6833         0.0  \n",
            "2429                6         0.0  \n",
            "2448                7         0.0  \n"
          ]
        }
      ],
      "source": [
        "print(\"Training set columns:\")\n",
        "print(x_train_df.columns.tolist())\n",
        "\n",
        "print(\"\\nValidation set columns:\")\n",
        "print(x_val_df.columns.tolist())\n",
        "\n",
        "print(\"\\nTraining set preview:\")\n",
        "print(x_train_df.head())\n",
        "\n",
        "print(\"\\nValidation set preview:\")\n",
        "print(x_val_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ozhotelstr\n",
            "elleturkiye\n",
            "sozerinsaatorhangazi\n",
            "sanliurfapiazzaavym\n",
            "rusanozden\n",
            "*****\n",
            "['ozhotelstr', 'elleturkiye', 'sozerinsaatorhangazi', 'sanliurfapiazzaavym', 'rusanozden']\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the testing directory\n",
        "testing_dir = os.path.join(data_dir, 'testing')\n",
        "\n",
        "# File path for 'test-classification-round1.dat'\n",
        "test_data_path = os.path.join(testing_dir, 'test-classification-round1.dat')\n",
        "\n",
        "# Step 2: Preview First 5 Lines of the Test File\n",
        "with open(test_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for i, line in enumerate(fh):\n",
        "        print(line.strip())\n",
        "        if i == 4:  # Print only the first 5 lines\n",
        "            break\n",
        "\n",
        "print(\"*****\")\n",
        "\n",
        "# Step 3: Extract Usernames from Test Data\n",
        "test_unames = []\n",
        "with open(test_data_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        test_unames.append(line.strip())\n",
        "\n",
        "# Step 4: Verify Output\n",
        "print(test_unames[:5])  # Display the first 5 usernames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khu0eryhNZNN"
      },
      "source": [
        "# Naive Base Classifier\n",
        "\n",
        "### Now we can pass the numerical values to a classifier, Let's try Naive Base!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC7KXsQZL7Kp"
      },
      "source": [
        "# Like Count Prediction\n",
        "\n",
        "\n",
        "Here, we use the average like_count of the user's previous posts to predict each post's like_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nolpagasSuBq"
      },
      "outputs": [],
      "source": [
        "def predict_like_count(username, current_post=None):\n",
        "  def get_avg_like_count(posts:list):\n",
        "    total = 0.\n",
        "    for post in posts:\n",
        "      if current_post is not None and post[\"id\"] == current_post[\"id\"]:\n",
        "        continue\n",
        "\n",
        "      like_count = post.get(\"like_count\", 0)\n",
        "      if like_count is None:\n",
        "        like_count = 0\n",
        "      total += like_count\n",
        "\n",
        "    if len(posts) == 0:\n",
        "      return 0.\n",
        "\n",
        "    return total / len(posts)\n",
        "\n",
        "  if username in username2posts_train:\n",
        "    return get_avg_like_count(username2posts_train[username])\n",
        "  elif username in username2posts_test:\n",
        "    return get_avg_like_count(username2posts_test[username])\n",
        "  else:\n",
        "    print(f\"No data available for {username}\")\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZNWTjLZ6XAuj"
      },
      "outputs": [],
      "source": [
        "def log_mse_like_counts(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculate the Log Mean Squared Error (Log MSE) for like counts (log(like_count + 1)).\n",
        "\n",
        "  Parameters:\n",
        "  - y_true: array-like, actual like counts\n",
        "  - y_pred: array-like, predicted like counts\n",
        "\n",
        "  Returns:\n",
        "  - log_mse: float, Log Mean Squared Error\n",
        "  \"\"\"\n",
        "  # Ensure inputs are numpy arrays\n",
        "  y_true = np.array(y_true)\n",
        "  y_pred = np.array(y_pred)\n",
        "\n",
        "  # Log transformation: log(like_count + 1)\n",
        "  log_y_true = np.log1p(y_true)\n",
        "  log_y_pred = np.log1p(y_pred)\n",
        "\n",
        "  # Compute squared errors\n",
        "  squared_errors = (log_y_true - log_y_pred) ** 2\n",
        "\n",
        "  # Return the mean of squared errors\n",
        "  return np.mean(squared_errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1jETdAuXA0H",
        "outputId": "871164d0-74fb-49cd-ea77-b8a3e0793e81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log MSE Train= 1.2271047744059362\n"
          ]
        }
      ],
      "source": [
        "#@title Train Dataset evaluation\n",
        "\n",
        "y_like_count_train_true = []\n",
        "y_like_count_train_pred = []\n",
        "for uname, posts in username2posts_train.items():\n",
        "  for post in posts:\n",
        "    pred_val = predict_like_count(uname, post)\n",
        "    true_val = post.get(\"like_count\", 0)\n",
        "    if true_val is None:\n",
        "      true_val = 0\n",
        "\n",
        "    y_like_count_train_true.append(true_val)\n",
        "    y_like_count_train_pred.append(pred_val)\n",
        "\n",
        "print(f\"Log MSE Train= {log_mse_like_counts(y_like_count_train_true, y_like_count_train_pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F02V1wO-WBMV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed data saved to: c:\\Users\\itsmm\\OneDrive\\Desktop\\CS412\\CS412-InstagramInfluencersAnalysis\\data\\output\\test-regression-round1.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Define File Paths Dynamically\n",
        "# Get the current notebook directory\n",
        "current_notebook_dir = os.getcwd()\n",
        "\n",
        "# Get the repo directory (assuming notebooks are inside the \"notebooks\" folder)\n",
        "repo_dir = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
        "\n",
        "# Get the data directory\n",
        "data_dir = os.path.join(repo_dir, 'data')\n",
        "\n",
        "# Get the testing directory\n",
        "testing_dir = os.path.join(data_dir, 'testing')\n",
        "\n",
        "# File path for 'test-regression-round1.jsonl'\n",
        "test_dataset_path = os.path.join(testing_dir, 'test-regression-round1.jsonl')\n",
        "\n",
        "# File path for output\n",
        "output_dir = os.path.join(data_dir, 'output')\n",
        "os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
        "output_file_path = os.path.join(output_dir, 'test-regression-round1.jsonl')\n",
        "\n",
        "# Step 2: Process the Test Dataset\n",
        "to_predict_like_counts_usernames = []\n",
        "output_list = []\n",
        "\n",
        "with open(test_dataset_path, \"rt\", encoding=\"utf-8\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "\n",
        "        # Perform prediction\n",
        "        pred_val = predict_like_count(sample[\"username\"])  # Ensure `predict_like_count` is defined\n",
        "        sample[\"like_count\"] = int(pred_val)\n",
        "        output_list.append(sample)\n",
        "\n",
        "# Step 3: Save the Output to a File\n",
        "with open(output_file_path, \"wt\", encoding=\"utf-8\") as of:\n",
        "    json.dump(output_list, of)\n",
        "\n",
        "# Step 4: Output Verification\n",
        "print(f\"Processed data saved to: {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blI2SqvvvOF8",
        "outputId": "fe1e72e3-3ad0-486d-d054-3e90e8c66669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'caption': 'KOZA 2023 2.si Damla’nın koleksiyonu, Latincede ‘Memento Mori’ '\n",
            "             'olarak bilinen ‘ölümlü olduğunu hatırla’ anlamındaki ifadeden '\n",
            "             'esinleniyor. Koleksiyon, hayatın ve ölümün, para, işçi, kral ve '\n",
            "             'kraliçe kavramları üzerinden yaratıcı görünümlerle bir araya '\n",
            "             'getirilmesini amaçlıyor. Ölüm sembollerinden esinlenen desenler '\n",
            "             'kullanan Damla, “kağıt parçasından ibaret olmak” kavramını '\n",
            "             'vurguluyor. Koleksiyon, yaşamın ve ölümün aynı anda ifade '\n",
            "             'edilmesini hedefliyor; kırmızı ve mavi ışıklarla veya '\n",
            "             'gözlüklerle görülen hologram efekti kullanılarak bu konsept '\n",
            "             'sahneye taşınıyor. Kırmızı renk ölümü, mavi ise yaşamı '\n",
            "             'simgeliyor. Koleksiyon, ofis giyimlerinden esinlenerek '\n",
            "             'kravatlar, gömlekler ve evrak çantaları içeriyor. Klasik sivri '\n",
            "             'burun çizmelerin üzerine spor ayakkabıların üst yüzeyi '\n",
            "             'yerleştirilerek, iş dünyasının koşuşturması ve cenaze '\n",
            "             'temalarının aynı anda ifade edilmesi amaçlanıyor. Para kazanma '\n",
            "             'arzusu, kırmızı zambak desenleri ve büyük mücevher görünümleri '\n",
            "             'ile koleksiyon tamamlanıyor.\\n'\n",
            "             '\\n'\n",
            "             'Tebrikler Damla!\\n'\n",
            "             '\\n'\n",
            "             '#GencModaTasarimcilari #Koza2023 #KozaYarismasi '\n",
            "             '#TasarimYarismasi #Moda #Fashion #ModaTasarımı',\n",
            "  'comments_count': 2,\n",
            "  'id': '18144550534306740',\n",
            "  'like_count': 158,\n",
            "  'media_type': 'CAROUSEL_ALBUM',\n",
            "  'media_url': 'https://scontent-sof1-1.cdninstagram.com/v/t51.29350-15/397997154_1016992459537522_4925783512176260397_n.jpg?_nc_cat=106&ccb=1-7&_nc_sid=c4dd86&_nc_ohc=7V_eObkFeK4AX-LMtsK&_nc_ht=scontent-sof1-1.cdninstagram.com&edm=AL-3X8kEAAAA&oh=00_AfDEqDhzaTO3ezV-veT6cJFCOcAEyeVzHR6si9n33N6G5A&oe=6551B6B9',\n",
            "  'timestamp': '2023-11-02 15:49:22',\n",
            "  'username': 'kozayarismasi'},\n",
            " {'caption': 'Tüm Türkiye ve Avrupa’ya sevkiyatlarımız aralıksız devam ediyor! '\n",
            "             'Aracımız Bursa’dan Ordu’ya müşterimizin ürünleri için yola '\n",
            "             'çıkıyor.\\n'\n",
            "             '\\n'\n",
            "             '👉Tuna Mah. Etibank Cad. No:134 Osmangazi/BURSA\\n'\n",
            "             '\\n'\n",
            "             'www.celikbeymobilya.com sitemizden tüm modelleri '\n",
            "             'inceleyebilirsiniz. \\n'\n",
            "             '\\n'\n",
            "             '#bursa #almanya #fransa',\n",
            "  'comments_count': 0,\n",
            "  'id': '17995331788956693',\n",
            "  'like_count': 99,\n",
            "  'media_type': 'VIDEO',\n",
            "  'media_url': 'https://scontent-sof1-2.cdninstagram.com/o1/v/t16/f1/m82/BF4767CB85BDFB8ADCCCA8F15B8C20B5_video_dashinit.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6InZ0c192b2RfdXJsZ2VuLmNsaXBzLnVua25vd24tQzMuNzIwLmRhc2hfYmFzZWxpbmVfMV92MSJ9&_nc_ht=scontent-sof1-2.cdninstagram.com&_nc_cat=110&vs=1259525061418244_1441854817&_nc_vs=HBksFQIYT2lnX3hwdl9yZWVsc19wZXJtYW5lbnRfcHJvZC9CRjQ3NjdDQjg1QkRGQjhBRENDQ0E4RjE1QjhDMjBCNV92aWRlb19kYXNoaW5pdC5tcDQVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dBRWdfaFZfcDVCYk5HZ0NBQTlzVURvZW5mZ3FicV9FQUFBRhUCAsgBACgAGAAbAYgHdXNlX29pbAExFQAAJvS3uOiQ0P8%2FFQIoAkMzLBdAJO%2Bdsi0OVhgSZGFzaF9iYXNlbGluZV8xX3YxEQB1AAA%3D&ccb=9-4&oh=00_AfAm22JssMPaUlQe3rpYsFWBhFb5mUgolTCdhV0Xgm4AnA&oe=6556A482&_nc_sid=1d576d&_nc_rid=cd9a998e44',\n",
            "  'timestamp': '2023-08-19 13:46:02',\n",
            "  'username': 'celikbeymobilya'},\n",
            " {'caption': '🤩\\n'\n",
            "             '\\n'\n",
            "             '.\\n'\n",
            "             '.\\n'\n",
            "             'Daha Fazlası İçin Beğenmeyi ve Takip Etmeyi Unutmayın\\n'\n",
            "             '.\\n'\n",
            "             'girisimci_muhendis ✅\\n'\n",
            "             '.\\n'\n",
            "             '📣Bu Bilgi Hakkında Ne Düşünüyorsunuz.\\n'\n",
            "             '.\\n'\n",
            "             '✅Görmesini İstediğin Arkadaşını Etiketle.\\n'\n",
            "             '.\\n'\n",
            "             '🔔Gönderi Bildirimlerini Açarak, Bilgileri Anında '\n",
            "             'Öğrenebilirsiniz.\\n'\n",
            "             '\\n'\n",
            "             '.\\n'\n",
            "             '\\n'\n",
            "             'Source: Unknown\\n'\n",
            "             'Dm for Credit or removal\\n'\n",
            "             '.\\n'\n",
            "             '.............................................................\\n'\n",
            "             'All rights and credits reserved to the respective owner(s). If '\n",
            "             'you are the main copyright owner rather than the one mentioned '\n",
            "             'here of this content, contact me to claim credit or content '\n",
            "             'removal',\n",
            "  'comments_count': 75,\n",
            "  'id': '18302703232191518',\n",
            "  'like_count': 1224,\n",
            "  'media_type': 'VIDEO',\n",
            "  'media_url': None,\n",
            "  'timestamp': '2023-10-02 06:53:33',\n",
            "  'username': 'girisimci_muhendis'}]\n"
          ]
        }
      ],
      "source": [
        "# output_list first 3 items\n",
        "pprint(output_list[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYP0TzdoGbpt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
