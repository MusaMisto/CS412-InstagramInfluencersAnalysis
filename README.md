# **Social Media User Classification and Regression**

## **Authors**
- **Musa Misto (29996)**
- **Neda Mohamed (33016)**

---

## **Introduction**
This project focuses on leveraging social media data to:
1. **Classification Task**: Categorize users into predefined labels based on profile information and posts.
2. **Regression Task**: Predict `like_count` from user metadata and content.

The primary objectives include handling imbalanced datasets, extracting meaningful features, and building robust machine learning pipelines for both tasks.

---

## **Dataset Overview**
The dataset consists of two primary components:
1. **User Profiles**: Metadata such as `username`, `biography`, `follower_count`, `post_count`, and `engagement_rate`.
2. **Posts**: Textual content generated by users.

### **Dataset Statistics**
- **Number of Users**:
  - Training: **2741 users**.
  - Testing: **2674 users**.
- **Target Variables**:
  - Classification: Categories such as `food`, `tech`, and `sports`.
  - Regression: Continuous variables like `follower_count` or `engagement_rate`.

---

## **Methodology**

### **1. Data Preprocessing**
- **Text Preprocessing**:
  - Tokenization and removal of Turkish stop words.
  - Vectorization using **TF-IDF** to convert text data into numerical features.
- **Numerical Features**:
  - Normalized attributes like `follower_count` and `post_count` using **MinMaxScaler**.
- **Categorical Features**:
  - Encoded using **LabelEncoder** for classification.

---

### **2. Feature Engineering**
- **Text Features**: Extracted insights from posts and biographies.
- **Numerical Features**: Derived meaningful metrics from user metadata.
- **Derived Features**: Engineered attributes to enhance predictive power.

---

### **3. Handling Data Imbalance**
For the classification task:
- **SMOTE**: Synthetic Minority Oversampling Technique generated synthetic samples for underrepresented categories.
- **SMOTETomek**: Enhanced the dataset by removing overlapping samples.

---

### **4. Model Training**

#### **Classification Task**
Several models were evaluated for user classification:
1. **Baseline Models**:
   - Logistic Regression.
   - Na√Øve Bayes.
2. **Advanced Models**:
   - Random Forest.
   - Gradient Boosting.
   - XGBoost.
   - LightGBM.
3. **Voting Classifier**:
   - Combined predictions from multiple models for improved accuracy.

**Pipeline Setup**:
- Integrated feature preprocessing, data resampling (SMOTE/SMOTETomek), and classification seamlessly.

#### **Regression Task**
The regression models aimed to predict numerical targets like `follower_count`:
1. **Baseline Model**:
   - Linear Regression.
2. **Advanced Models**:
   - Random Forest Regressor.
   - Gradient Boosting Regressor.
   - XGBoost Regressor.
   - LightGBM Regressor.

**Pipeline Setup**:
- Feature scaling, transformation, and model training were combined into a streamlined pipeline.

#### **Hyperparameter Tuning**
- GridSearchCV was used to optimize hyperparameters for all models.

---

## **Results and Analysis**

### **Classification Task**
- **Evaluation Metrics**:
  - Accuracy, Precision, Recall, and F1-Score were used to assess performance.
- **Best Model**: LightGBM and XGBoost achieved the highest scores across all metrics.
- **Imbalance Handling**: SMOTE and SMOTETomek significantly improved recall for minority classes.
- **Validation Accuracy**: 0.683

### **Regression Task**
- **Evaluation Metrics**:
  - Mean Absolute Error (MAE).
  - Root Mean Squared Error (RMSE).
  - \(R^2\)-score.
- **Best Model**: LightGBM Regressor consistently outperformed other models, achieving the lowest RMSE and highest \(R^2\)-score.
- **Insights**:
  - Non-linear models like Gradient Boosting and XGBoost performed better than Linear Regression due to their ability to capture complex patterns in the data.
- **Validation Log MSE**: 0.1359

### **Visualizations**
- **Classification**:
  - Bar charts visualizing category distributions pre- and post-resampling.
  - Confusion matrices to identify common misclassifications.
- **Regression**:
  - Scatter plots comparing actual vs. predicted values.
  - Residual plots to assess model fit and variance.

---

## **Conclusion**

### **Key Insights**
1. **Classification**:
   - Advanced tree-based ensemble models excelled in handling imbalanced datasets.
   - SMOTE/SMOTETomek played a crucial role in improving minority class performance.
2. **Regression**:
   - Non-linear regression models like LightGBM and XGBoost provided superior performance compared to Linear Regression.
   - Proper feature scaling and transformation significantly impacted results.
3. **Feature Engineering**:
   - Text-based features (e.g., TF-IDF vectors) were crucial for both tasks.
   - Metadata attributes enriched predictive power for regression.